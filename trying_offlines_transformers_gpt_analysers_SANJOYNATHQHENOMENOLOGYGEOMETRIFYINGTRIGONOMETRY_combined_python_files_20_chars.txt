File 1 - installed_pypackages_versions_dependency_reports.py:

1: (0)              import subprocess
2: (0)              import pkg_resources
3: (0)              def get_installed_packages():
4: (4)                  installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}
5: (4)                  return installed_packages
6: (0)              def get_package_dependencies(package_name):
7: (4)                  result = subprocess.run(['pip', 'show', package_name], capture_output=True, text=True)
8: (4)                  dependencies = []
9: (4)                  if result.returncode == 0:
10: (8)                     for line in result.stdout.splitlines():
11: (12)                        if line.startswith('Requires'):
12: (16)                            dependencies = line.split(':')[1].strip().split(', ')
13: (16)                            break
14: (4)                 return dependencies
15: (0)             def get_reverse_dependencies(package_name):
16: (4)                 result = subprocess.run(['pipdeptree', '--reverse', '--packages', package_name], capture_output=True, text=True)
17: (4)                 reverse_dependencies = []
18: (4)                 if result.returncode == 0:
19: (8)                     reverse_dependencies = result.stdout.strip().splitlines()
20: (4)                 return reverse_dependencies
21: (0)             def generate_report():
22: (4)                 installed_packages = get_installed_packages()
23: (4)                 report = []
24: (4)                 for package, version in installed_packages.items():
25: (8)                     dependencies = get_package_dependencies(package)
26: (8)                     reverse_dependencies = get_reverse_dependencies(package)
27: (8)                     report.append({
28: (12)                        'Package': package,
29: (12)                        'Version': version,
30: (12)                        'Dependencies': dependencies,
31: (12)                        'Reverse Dependencies': reverse_dependencies
32: (8)                     })
33: (4)                 return report
34: (0)             def write_report_to_file(report, filename="package_report.txt"):
35: (4)                 with open(filename, 'w') as file:
36: (8)                     for package_info in report:
37: (12)                        file.write(f"Package: {package_info['Package']}\n")
38: (12)                        file.write(f"Version: {package_info['Version']}\n")
39: (12)                        file.write(f"Dependencies: {', '.join(package_info['Dependencies']) if package_info['Dependencies'] else 'None'}\n")
40: (12)                        file.write(f"Reverse Dependencies: {', '.join(package_info['Reverse Dependencies']) if package_info['Reverse Dependencies'] else 'None'}\n")
41: (12)                        file.write("\n" + "-"*50 + "\n")
42: (0)             if __name__ == '__main__':
43: (4)                 report = generate_report()
44: (4)                 write_report_to_file(report)
45: (4)                 print("Report generated and saved to 'package_report.txt'.")

----------------------------------------

File 2 - refined_installed_pypackages_versions_dependency_reports.py:

1: (0)              import subprocess
2: (0)              import sys
3: (0)              def get_installed_packages():
4: (4)                  installed_packages = {}
5: (4)                  result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True)
6: (4)                  if result.returncode == 0:
7: (8)                      for line in result.stdout.splitlines():
8: (12)                         package, version = line.split('==')
9: (12)                         installed_packages[package] = version
10: (4)                 return installed_packages
11: (0)             def get_package_dependencies(package_name):
12: (4)                 result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], capture_output=True, text=True)
13: (4)                 dependencies = []
14: (4)                 if result.returncode == 0:
15: (8)                     for line in result.stdout.splitlines():
16: (12)                        if line.startswith('Requires'):
17: (16)                            dependencies = line.split(':')[1].strip().split(', ')
18: (16)                            break
19: (4)                 return dependencies
20: (0)             def generate_report():
21: (4)                 installed_packages = get_installed_packages()
22: (4)                 report = []
23: (4)                 for package, version in installed_packages.items():
24: (8)                     dependencies = get_package_dependencies(package)
25: (8)                     report.append({
26: (12)                        'Package': package,
27: (12)                        'Version': version,
28: (12)                        'Dependencies': dependencies if dependencies else 'None'
29: (8)                     })
30: (4)                 return report
31: (0)             def write_report_to_file(report, filename="package_report.txt"):
32: (4)                 with open(filename, 'w') as file:
33: (8)                     for package_info in report:
34: (12)                        file.write(f"Package: {package_info['Package']}\n")
35: (12)                        file.write(f"Version: {package_info['Version']}\n")
36: (12)                        file.write(f"Dependencies: {', '.join(package_info['Dependencies']) if isinstance(package_info['Dependencies'], list) else package_info['Dependencies']}\n")
37: (12)                        file.write("\n" + "-"*50 + "\n")
38: (0)             if __name__ == '__main__':
39: (4)                 report = generate_report()
40: (4)                 write_report_to_file(report)
41: (4)                 print("Report generated and saved to 'package_report.txt'.")

----------------------------------------

File 3 - new_refined_installed_pypackages_versions_dependency_reports.py:

1: (0)              import subprocess
2: (0)              import sys
3: (0)              def get_installed_packages():
4: (4)                  installed_packages = {}
5: (4)                  result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True)
6: (4)                  if result.returncode == 0:
7: (8)                      for line in result.stdout.splitlines():
8: (12)                         if '==' in line:
9: (16)                             package, version = line.split('==')
10: (16)                            installed_packages[package] = version
11: (4)                 return installed_packages
12: (0)             def get_package_dependencies(package_name):
13: (4)                 result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], capture_output=True, text=True)
14: (4)                 dependencies = []
15: (4)                 if result.returncode == 0:
16: (8)                     for line in result.stdout.splitlines():
17: (12)                        if line.startswith('Requires'):
18: (16)                            dependencies = line.split(':')[1].strip().split(', ')
19: (16)                            break
20: (4)                 return dependencies
21: (0)             def generate_report():
22: (4)                 installed_packages = get_installed_packages()
23: (4)                 report = []
24: (4)                 for package, version in installed_packages.items():
25: (8)                     dependencies = get_package_dependencies(package)
26: (8)                     report.append({
27: (12)                        'Package': package,
28: (12)                        'Version': version,
29: (12)                        'Dependencies': dependencies if dependencies else 'None'
30: (8)                     })
31: (4)                 return report
32: (0)             def write_report_to_file(report, filename="package_report.txt"):
33: (4)                 with open(filename, 'w') as file:
34: (8)                     for package_info in report:
35: (12)                        file.write(f"Package: {package_info['Package']}\n")
36: (12)                        file.write(f"Version: {package_info['Version']}\n")
37: (12)                        file.write(f"Dependencies: {', '.join(package_info['Dependencies']) if isinstance(package_info['Dependencies'], list) else package_info['Dependencies']}\n")
38: (12)                        file.write("\n" + "-"*50 + "\n")
39: (0)             if __name__ == '__main__':
40: (4)                 report = generate_report()
41: (4)                 write_report_to_file(report)
42: (4)                 print("Report generated and saved to 'package_report.txt'.")

----------------------------------------

File 4 - pypdfsgraphcsextractions_fitz_with_pytorches.py:

1: (0)              import fitz  # PyMuPDF
2: (0)              doc = fitz.open("AISC STEEL CHART rev 11 (Draft copy).pdf")
3: (0)              for page_num in range(len(doc)):
4: (4)                  page = doc.load_page(page_num)
5: (4)                  pix = page.get_pixmap()  # Convert page to image
6: (4)                  pix.save(f"page_{page_num}.png")

----------------------------------------

File 5 - trying___pdfparsetreesgenerators.py:

1: (0)              import fitz  # PyMuPDF
2: (0)              import nltk
3: (0)              from nltk.tokenize import sent_tokenize
4: (0)              from nltk import pos_tag, word_tokenize
5: (0)              from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
6: (0)              from transformers import pipeline
7: (0)              import pdfplumber
8: (0)              import os
9: (0)              from tkinter import Tk, filedialog, messagebox
10: (0)             model_name = "bert-base-uncased"  # Placeholder, choose an appropriate model for parsing
11: (0)             tokenizer = AutoTokenizer.from_pretrained(model_name)
12: (0)             model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
13: (0)             parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
14: (0)             def extract_text_from_pdf(pdf_path):
15: (4)                 with pdfplumber.open(pdf_path) as pdf:
16: (8)                     text = ""
17: (8)                     for page in pdf.pages:
18: (12)                        text += page.extract_text() + "\n"
19: (4)                 return text
20: (0)             def extract_text_from_txt(txt_path):
21: (4)                 with open(txt_path, "r", encoding="utf-8") as file:
22: (8)                     return file.read()
23: (0)             def create_pdf_with_parse_trees(sentences, output_pdf_path):
24: (4)                 doc = fitz.open()  # Create a new PDF document
25: (4)                 for sentence in sentences:
26: (8)                     page = doc.new_page()  # Add a new page for each sentence
27: (8)                     page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text
28: (8)                     parse_tree = generate_parse_tree(sentence)
29: (8)                     page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree
30: (4)                 doc.save(output_pdf_path)  # Save the PDF
31: (0)             def generate_parse_tree(sentence):
32: (4)                 result = parse_pipeline(sentence)  # Generate parse tree or structure
33: (4)                 return result[0]['generated_text']  # Placeholder, adapt based on your model's output
34: (0)             def main():
35: (4)                 root = Tk()
36: (4)                 root.withdraw()
37: (4)                 file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])
38: (4)                 if not file_path:
39: (8)                     messagebox.showerror("Error", "No file selected. Exiting.")
40: (8)                     return
41: (4)                 if file_path.endswith('.pdf'):
42: (8)                     text = extract_text_from_pdf(file_path)
43: (4)                 else:
44: (8)                     text = extract_text_from_txt(file_path)
45: (4)                 sentences = sent_tokenize(text)
46: (4)                 output_pdf_path = "output_with_parse_trees.pdf"
47: (4)                 create_pdf_with_parse_trees(sentences, output_pdf_path)
48: (4)                 messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")
49: (0)             if __name__ == "__main__":
50: (4)                 main()

----------------------------------------

File 6 - trying___pretrained_pdfparsetreesgenerators.py:

1: (0)              import fitz  # PyMuPDF
2: (0)              import nltk
3: (0)              from nltk.tokenize import sent_tokenize
4: (0)              from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
5: (0)              from transformers import pipeline
6: (0)              import pdfplumber
7: (0)              import os
8: (0)              from tkinter import Tk, filedialog, messagebox
9: (0)              model_name = "t5-small"  # Change this to any seq2seq model like "t5-base", "t5-large"
10: (0)             tokenizer = AutoTokenizer.from_pretrained(model_name)
11: (0)             model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
12: (0)             parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
13: (0)             def extract_text_from_pdf(pdf_path):
14: (4)                 with pdfplumber.open(pdf_path) as pdf:
15: (8)                     text = ""
16: (8)                     for page in pdf.pages:
17: (12)                        text += page.extract_text() + "\n"
18: (4)                 return text
19: (0)             def extract_text_from_txt(txt_path):
20: (4)                 with open(txt_path, "r", encoding="utf-8") as file:
21: (8)                     return file.read()
22: (0)             parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
23: (0)             def generate_parse_tree(sentence):
24: (4)                 result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
25: (4)                 return result[0]['generated_text']  # Adapt based on your model's output    
26: (0)             def create_pdf_with_parse_trees(sentences, output_pdf_path):
27: (4)                 doc = fitz.open()  # Create a new PDF document
28: (4)                 for sentence in sentences:
29: (8)                     page = doc.new_page()  # Add a new page for each sentence
30: (8)                     page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text
31: (8)                     parse_tree = generate_parse_tree(sentence)
32: (8)                     page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree
33: (4)                 doc.save(output_pdf_path)  # Save the PDF
34: (0)             def main():
35: (4)                 root = Tk()
36: (4)                 root.withdraw()
37: (4)                 file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])
38: (4)                 if not file_path:
39: (8)                     messagebox.showerror("Error", "No file selected. Exiting.")
40: (8)                     return
41: (4)                 if file_path.endswith('.pdf'):
42: (8)                     text = extract_text_from_pdf(file_path)
43: (4)                 else:
44: (8)                     text = extract_text_from_txt(file_path)
45: (4)                 sentences = sent_tokenize(text)
46: (4)                 output_pdf_path = "output_with_parse_trees.pdf"
47: (4)                 create_pdf_with_parse_trees(sentences, output_pdf_path)
48: (4)                 messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")
49: (0)             if __name__ == "__main__":
50: (4)                 main()

----------------------------------------

File 7 - new_trying___pretrained_pdfparsetreesgenerators.py:

1: (0)              import fitz  # PyMuPDF
2: (0)              import nltk
3: (0)              from nltk.tokenize import sent_tokenize
4: (0)              from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
5: (0)              from transformers import pipeline
6: (0)              import pdfplumber
7: (0)              import os
8: (0)              from tkinter import Tk, filedialog, messagebox
9: (0)              import torch  # Ensure latest PyTorch version is used
10: (0)             model_name = "t5-small"  # Change this to any seq2seq model like "t5-base", "t5-large"
11: (0)             tokenizer = AutoTokenizer.from_pretrained(model_name)
12: (0)             model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
13: (0)             device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
14: (0)             model.to(device)
15: (0)             parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=device.index if device.type == 'cuda' else -1)
16: (0)             def extract_text_from_pdf(pdf_path):
17: (4)                 with pdfplumber.open(pdf_path) as pdf:
18: (8)                     text = ""
19: (8)                     for page in pdf.pages:
20: (12)                        text += page.extract_text() + "\n"
21: (4)                 return text
22: (0)             def extract_text_from_txt(txt_path):
23: (4)                 with open(txt_path, "r", encoding="utf-8") as file:
24: (8)                     return file.read()
25: (0)             def generate_parse_tree(sentence):
26: (4)                 result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
27: (4)                 return result[0]['generated_text']  # Adapt based on your model's output
28: (0)             def create_pdf_with_parse_trees(sentences, output_pdf_path):
29: (4)                 doc = fitz.open()  # Create a new PDF document
30: (4)                 for sentence in sentences:
31: (8)                     page = doc.new_page()  # Add a new page for each sentence
32: (8)                     page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text
33: (8)                     parse_tree = generate_parse_tree(sentence)
34: (8)                     page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree
35: (4)                 doc.save(output_pdf_path)  # Save the PDF
36: (0)             def main():
37: (4)                 root = Tk()
38: (4)                 root.withdraw()
39: (4)                 file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])
40: (4)                 if not file_path:
41: (8)                     messagebox.showerror("Error", "No file selected. Exiting.")
42: (8)                     return
43: (4)                 if file_path.endswith('.pdf'):
44: (8)                     text = extract_text_from_pdf(file_path)
45: (4)                 else:
46: (8)                     text = extract_text_from_txt(file_path)
47: (4)                 sentences = sent_tokenize(text)
48: (4)                 output_pdf_path = "output_with_parse_trees.pdf"
49: (4)                 create_pdf_with_parse_trees(sentences, output_pdf_path)
50: (4)                 messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")
51: (0)             if __name__ == "__main__":
52: (4)                 main()

----------------------------------------

File 8 - withprogressbars_logs_new_trying___pretrained_pdfparsetreesgenerators.py:

1: (0)              import fitz  # PyMuPDF
2: (0)              import nltk
3: (0)              from nltk.tokenize import sent_tokenize
4: (0)              from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
5: (0)              from transformers import pipeline
6: (0)              import pdfplumber
7: (0)              import os
8: (0)              from tkinter import Tk, filedialog, messagebox
9: (0)              import torch
10: (0)             from tqdm import tqdm  # For the progress bar
11: (0)             import logging
12: (0)             logging.basicConfig(filename="process_log.log", level=logging.INFO,
13: (20)                                format="%(asctime)s - %(levelname)s - %(message)s")
14: (0)             model_name = "t5-small"  # Change this to any seq2seq model like "t5-base", "t5-large"
15: (0)             try:
16: (4)                 tokenizer = AutoTokenizer.from_pretrained(model_name)
17: (4)                 model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
18: (4)                 logging.info("Model and tokenizer loaded successfully.")
19: (0)             except Exception as e:
20: (4)                 logging.error(f"Error loading model and tokenizer: {e}")
21: (4)                 raise
22: (0)             device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
23: (0)             model.to(device)
24: (0)             parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=device.index if device.type == 'cuda' else -1)
25: (0)             def extract_text_from_pdf(pdf_path):
26: (4)                 try:
27: (8)                     with pdfplumber.open(pdf_path) as pdf:
28: (12)                        text = ""
29: (12)                        for page in pdf.pages:
30: (16)                            text += page.extract_text() + "\n"
31: (8)                     logging.info(f"Text extracted from PDF: {pdf_path}")
32: (8)                     return text
33: (4)                 except Exception as e:
34: (8)                     logging.error(f"Error extracting text from PDF {pdf_path}: {e}")
35: (8)                     raise
36: (0)             def extract_text_from_txt(txt_path):
37: (4)                 try:
38: (8)                     with open(txt_path, "r", encoding="utf-8") as file:
39: (12)                        text = file.read()
40: (8)                     logging.info(f"Text extracted from TXT: {txt_path}")
41: (8)                     return text
42: (4)                 except Exception as e:
43: (8)                     logging.error(f"Error extracting text from TXT {txt_path}: {e}")
44: (8)                     raise
45: (0)             def generate_parse_tree(sentence):
46: (4)                 try:
47: (8)                     result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
48: (8)                     return result[0]['generated_text']
49: (4)                 except Exception as e:
50: (8)                     logging.error(f"Error generating parse tree for sentence: {sentence} - {e}")
51: (8)                     raise
52: (0)             def create_pdf_with_parse_trees(sentences, output_pdf_path):
53: (4)                 try:
54: (8)                     doc = fitz.open()  # Create a new PDF document
55: (8)                     for idx, sentence in enumerate(sentences):
56: (12)                        page = doc.new_page()  # Add a new page for each sentence
57: (12)                        page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text
58: (12)                        parse_tree = generate_parse_tree(sentence)
59: (12)                        page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree
60: (12)                        progress_bar.update(1)
61: (8)                     doc.save(output_pdf_path)  # Save the PDF
62: (8)                     logging.info(f"PDF created successfully at: {output_pdf_path}")
63: (4)                 except Exception as e:
64: (8)                     logging.error(f"Error creating PDF with parse trees: {e}")
65: (8)                     raise
66: (0)             def main():
67: (4)                 try:
68: (8)                     root = Tk()
69: (8)                     root.withdraw()
70: (8)                     file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])
71: (8)                     if not file_path:
72: (12)                        messagebox.showerror("Error", "No file selected. Exiting.")
73: (12)                        logging.warning("No file selected by the user.")
74: (12)                        return
75: (8)                     if file_path.endswith('.pdf'):
76: (12)                        text = extract_text_from_pdf(file_path)
77: (8)                     else:
78: (12)                        text = extract_text_from_txt(file_path)
79: (8)                     sentences = sent_tokenize(text)
80: (8)                     total_sentences = len(sentences)
81: (8)                     global progress_bar
82: (8)                     progress_bar = tqdm(total=total_sentences, desc="Processing Sentences", unit="sentence")
83: (8)                     output_pdf_path = "output_with_parse_trees.pdf"
84: (8)                     create_pdf_with_parse_trees(sentences, output_pdf_path)
85: (8)                     progress_bar.close()  # Close the progress bar once done
86: (8)                     messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")
87: (8)                     logging.info(f"PDF generated successfully: {output_pdf_path}")
88: (4)                 except Exception as e:
89: (8)                     logging.error(f"An error occurred in the main function: {e}")
90: (8)                     messagebox.showerror("Error", f"An error occurred: {e}")
91: (0)             if __name__ == "__main__":
92: (4)                 main()

----------------------------------------

File 9 - 003_withprogressbars_logs_new_trying___pretrained_pdfparsetreesgenerators.py:

1: (0)              import fitz  # PyMuPDF
2: (0)              import nltk
3: (0)              from nltk.tokenize import sent_tokenize
4: (0)              from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
5: (0)              from transformers import pipeline
6: (0)              import pdfplumber
7: (0)              import os
8: (0)              from tkinter import Tk, filedialog, messagebox
9: (0)              import torch
10: (0)             from tqdm import tqdm  # For the progress bar
11: (0)             import logging
12: (0)             logging.basicConfig(filename="process_log.log", level=logging.INFO,
13: (20)                                format="%(asctime)s - %(levelname)s - %(message)s")
14: (0)             model_name = "t5-small"  # Change this to any seq2seq model like "t5-base", "t5-large"
15: (0)             try:
16: (4)                 tokenizer = AutoTokenizer.from_pretrained(model_name)
17: (4)                 model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
18: (4)                 logging.info("Model and tokenizer loaded successfully.")
19: (0)             except Exception as e:
20: (4)                 logging.error(f"Error loading model and tokenizer: {e}")
21: (4)                 raise
22: (0)             device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
23: (0)             model.to(device)
24: (0)             parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=device.index if device.type == 'cuda' else -1)
25: (0)             def extract_text_from_pdf(pdf_path):
26: (4)                 try:
27: (8)                     with pdfplumber.open(pdf_path) as pdf:
28: (12)                        text = ""
29: (12)                        for page in pdf.pages:
30: (16)                            text += page.extract_text() + "\n"
31: (8)                     logging.info(f"Text extracted from PDF: {pdf_path}")
32: (8)                     return text
33: (4)                 except Exception as e:
34: (8)                     logging.error(f"Error extracting text from PDF {pdf_path}: {e}")
35: (8)                     raise
36: (0)             def extract_text_from_txt(txt_path):
37: (4)                 try:
38: (8)                     with open(txt_path, "r", encoding="utf-8") as file:
39: (12)                        text = file.read()
40: (8)                     logging.info(f"Text extracted from TXT: {txt_path}")
41: (8)                     return text
42: (4)                 except Exception as e:
43: (8)                     logging.error(f"Error extracting text from TXT {txt_path}: {e}")
44: (8)                     raise
45: (0)             def generate_parse_tree(sentence):
46: (4)                 try:
47: (8)                     result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
48: (8)                     return result[0]['generated_text']
49: (4)                 except Exception as e:
50: (8)                     logging.error(f"Error generating parse tree for sentence: {sentence} - {e}")
51: (8)                     raise
52: (0)             def create_pdf_with_parse_trees(sentences, output_pdf_path):
53: (4)                 try:
54: (8)                     doc = fitz.open()  # Create a new PDF document
55: (8)                     for idx, sentence in enumerate(sentences):
56: (12)                        page = doc.new_page()  # Add a new page for each sentence
57: (12)                        page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text
58: (12)                        parse_tree = generate_parse_tree(sentence)
59: (12)                        page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree
60: (12)                        progress_bar.update(1)  # Update progress bar here
61: (8)                     doc.save(output_pdf_path)  # Save the PDF
62: (8)                     logging.info(f"PDF created successfully at: {output_pdf_path}")
63: (4)                 except Exception as e:
64: (8)                     logging.error(f"Error creating PDF with parse trees: {e}")
65: (8)                     raise
66: (0)             def main():
67: (4)                 try:
68: (8)                     root = Tk()
69: (8)                     root.withdraw()
70: (8)                     file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])
71: (8)                     if not file_path:
72: (12)                        messagebox.showerror("Error", "No file selected. Exiting.")
73: (12)                        logging.warning("No file selected by the user.")
74: (12)                        return
75: (8)                     if file_path.endswith('.pdf'):
76: (12)                        text = extract_text_from_pdf(file_path)
77: (8)                     else:
78: (12)                        text = extract_text_from_txt(file_path)
79: (8)                     sentences = sent_tokenize(text)
80: (8)                     total_sentences = len(sentences)
81: (8)                     global progress_bar
82: (8)                     progress_bar = tqdm(total=total_sentences, desc="Processing Sentences", unit="sentence")
83: (8)                     output_pdf_path = "output_with_parse_trees.pdf"
84: (8)                     create_pdf_with_parse_trees(sentences, output_pdf_path)
85: (8)                     progress_bar.close()  # Close the progress bar once done
86: (8)                     messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")
87: (8)                     logging.info(f"PDF generated successfully: {output_pdf_path}")
88: (4)                 except Exception as e:
89: (8)                     logging.error(f"An error occurred in the main function: {e}")
90: (8)                     messagebox.showerror("Error", f"An error occurred: {e}")
91: (0)             if __name__ == "__main__":
92: (4)                 main()

----------------------------------------

File 10 - SANJOYNATHQHENOMENOLOGYGEOMETRIFYINGTRIGONOMETRYCOMBINER_aligner_20_characters_for_pythons_codes.py:

1: (0)              import os
2: (0)              from datetime import datetime
3: (0)              def get_file_info(root_folder):
4: (4)                  file_info_list = []
5: (4)                  for root, dirs, files in os.walk(root_folder):
6: (8)                      for file in files:
7: (12)                         try:
8: (16)                             if file.endswith('.py'):
9: (20)                                 file_path = os.path.join(root, file)
10: (20)                                creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
11: (20)                                modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))
12: (20)                                file_extension = os.path.splitext(file)[1].lower()
13: (20)                                file_info_list.append([file, file_path, creation_time, modified_time, file_extension, root])
14: (12)                        except Exception as e:
15: (16)                            print(f"Error processing file {file}: {e}")
16: (4)                 file_info_list.sort(key=lambda x: (x[2], x[3], len(x[0]), x[4]))  # Sort by creation, modification time, name length, extension
17: (4)                 return file_info_list
18: (0)             def process_file(file_info_list):
19: (4)                 combined_output = []
20: (4)                 for idx, (file_name, file_path, creation_time, modified_time, file_extension, root) in enumerate(file_info_list):
21: (8)                     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
22: (12)                        content = f.read()
23: (12)                        content = "\n".join([line for line in content.split('\n') if line.strip() and not line.strip().startswith("#")])
24: (12)                        content = content.replace('\t', '    ')
25: (12)                        processed_lines = []
26: (12)                        for i, line in enumerate(content.split('\n')):
27: (16)                            leading_spaces = len(line) - len(line.lstrip(' '))
28: (16)                            line_number_str = f"{i+1}: ({leading_spaces})"
29: (16)                            padding = ' ' * (20 - len(line_number_str))
30: (16)                            processed_line = f"{line_number_str}{padding}{line}"
31: (16)                            processed_lines.append(processed_line)
32: (12)                        content_with_line_numbers = "\n".join(processed_lines)
33: (12)                        combined_output.append(f"File {idx + 1} - {file_name}:\n")
34: (12)                        combined_output.append(content_with_line_numbers)
35: (12)                        combined_output.append("\n" + "-"*40 + "\n")
36: (4)                 return combined_output
37: (0)             root_folder_path = '.'  # Set this to the desired folder
38: (0)             file_info_list = get_file_info(root_folder_path)
39: (0)             combined_output = process_file(file_info_list)
40: (0)             output_file = 'SANJOYNATHQHENOMENOLOGYGEOMETRIFYINGTRIGONOMETRY_combined_python_files_20_chars.txt'
41: (0)             with open(output_file, 'w', encoding='utf-8') as logfile:
42: (4)                 logfile.write("\n".join(combined_output))
43: (0)             print(f"Processed file info logged to {output_file}")

----------------------------------------
