File 1 - . \__init__.py:

1: (0)              from .blob import Blobber, Sentence, TextBlob, Word, WordList
2: (0)              __all__ = [
3: (4)                  "TextBlob",
4: (4)                  "Word",
5: (4)                  "Sentence",
6: (4)                  "Blobber",
7: (4)                  "WordList",
8: (0)              ]

----------------------------------------

File 2 - . \base.py:

1: (0)              """Abstract base classes for models (taggers, noun phrase extractors, etc.)
2: (0)              which define the interface for descendant classes.
3: (0)              .. versionchanged:: 0.7.0
4: (4)                  All base classes are defined in the same module, ``textblob.base``.
5: (0)              """
6: (0)              from abc import ABCMeta, abstractmethod
7: (0)              import nltk
8: (0)              ##### POS TAGGERS #####
9: (0)              class BaseTagger(metaclass=ABCMeta):
10: (4)                 """Abstract tagger class from which all taggers
11: (4)                 inherit from. All descendants must implement a
12: (4)                 ``tag()`` method.
13: (4)                 """
14: (4)                 @abstractmethod
15: (4)                 def tag(self, text, tokenize=True):
16: (8)                     """Return a list of tuples of the form (word, tag)
17: (8)                     for a given set of text or BaseBlob instance.
18: (8)                     """
19: (8)                     return
20: (0)             ##### NOUN PHRASE EXTRACTORS #####
21: (0)             class BaseNPExtractor(metaclass=ABCMeta):
22: (4)                 """Abstract base class from which all NPExtractor classes inherit.
23: (4)                 Descendant classes must implement an ``extract(text)`` method
24: (4)                 that returns a list of noun phrases as strings.
25: (4)                 """
26: (4)                 @abstractmethod
27: (4)                 def extract(self, text):
28: (8)                     """Return a list of noun phrases (strings) for a body of text."""
29: (8)                     return
30: (0)             ##### TOKENIZERS #####
31: (0)             class BaseTokenizer(nltk.tokenize.api.TokenizerI, metaclass=ABCMeta):
32: (4)                 """Abstract base class from which all Tokenizer classes inherit.
33: (4)                 Descendant classes must implement a ``tokenize(text)`` method
34: (4)                 that returns a list of noun phrases as strings.
35: (4)                 """
36: (4)                 @abstractmethod
37: (4)                 def tokenize(self, text):
38: (8)                     """Return a list of tokens (strings) for a body of text.
39: (8)                     :rtype: list
40: (8)                     """
41: (8)                     return
42: (4)                 def itokenize(self, text, *args, **kwargs):
43: (8)                     """Return a generator that generates tokens "on-demand".
44: (8)                     .. versionadded:: 0.6.0
45: (8)                     :rtype: generator
46: (8)                     """
47: (8)                     return (t for t in self.tokenize(text, *args, **kwargs))
48: (0)             ##### SENTIMENT ANALYZERS ####
49: (0)             DISCRETE = "ds"
50: (0)             CONTINUOUS = "co"
51: (0)             class BaseSentimentAnalyzer(metaclass=ABCMeta):
52: (4)                 """Abstract base class from which all sentiment analyzers inherit.
53: (4)                 Should implement an ``analyze(text)`` method which returns either the
54: (4)                 results of analysis.
55: (4)                 """
56: (4)                 kind = DISCRETE
57: (4)                 def __init__(self):
58: (8)                     self._trained = False
59: (4)                 def train(self):
60: (8)                     # Train me
61: (8)                     self._trained = True
62: (4)                 @abstractmethod
63: (4)                 def analyze(self, text):
64: (8)                     """Return the result of of analysis. Typically returns either a
65: (8)                     tuple, float, or dictionary.
66: (8)                     """
67: (8)                     # Lazily train the classifier
68: (8)                     if not self._trained:
69: (12)                        self.train()
70: (8)                     # Analyze text
71: (8)                     return None
72: (0)             ##### PARSERS #####
73: (0)             class BaseParser(metaclass=ABCMeta):
74: (4)                 """Abstract parser class from which all parsers inherit from. All
75: (4)                 descendants must implement a ``parse()`` method.
76: (4)                 """
77: (4)                 @abstractmethod
78: (4)                 def parse(self, text):
79: (8)                     """Parses the text."""
80: (8)                     return

----------------------------------------

File 3 - . \blob.py:

1: (0)              """Wrappers for various units of text, including the main
2: (0)              :class:`TextBlob <textblob.blob.TextBlob>`, :class:`Word <textblob.blob.Word>`,
3: (0)              and :class:`WordList <textblob.blob.WordList>` classes.
4: (0)              Example usage: ::
5: (4)                  >>> from textblob import TextBlob
6: (4)                  >>> b = TextBlob("Simple is better than complex.")
7: (4)                  >>> b.tags
8: (4)                  [(u'Simple', u'NN'), (u'is', u'VBZ'), (u'better', u'JJR'), (u'than', u'IN'), (u'complex', u'NN')]
9: (4)                  >>> b.noun_phrases
10: (4)                 WordList([u'simple'])
11: (4)                 >>> b.words
12: (4)                 WordList([u'Simple', u'is', u'better', u'than', u'complex'])
13: (4)                 >>> b.sentiment
14: (4)                 (0.06666666666666667, 0.41904761904761906)
15: (4)                 >>> b.words[0].synsets()[0]
16: (4)                 Synset('simple.n.01')
17: (0)             .. versionchanged:: 0.8.0
18: (4)                 These classes are now imported from ``textblob`` rather than ``text.blob``.
19: (0)             """  # noqa: E501
20: (0)             import json
21: (0)             import sys
22: (0)             from collections import defaultdict
23: (0)             import nltk
24: (0)             from textblob.base import (
25: (4)                 BaseNPExtractor,
26: (4)                 BaseParser,
27: (4)                 BaseSentimentAnalyzer,
28: (4)                 BaseTagger,
29: (4)                 BaseTokenizer,
30: (0)             )
31: (0)             from textblob.decorators import cached_property, requires_nltk_corpus
32: (0)             from textblob.en import suggest
33: (0)             from textblob.inflect import pluralize as _pluralize
34: (0)             from textblob.inflect import singularize as _singularize
35: (0)             from textblob.mixins import BlobComparableMixin, StringlikeMixin
36: (0)             from textblob.np_extractors import FastNPExtractor
37: (0)             from textblob.parsers import PatternParser
38: (0)             from textblob.sentiments import PatternAnalyzer
39: (0)             from textblob.taggers import NLTKTagger
40: (0)             from textblob.tokenizers import WordTokenizer, sent_tokenize, word_tokenize
41: (0)             from textblob.utils import PUNCTUATION_REGEX, lowerstrip
42: (0)             # Wordnet interface
43: (0)             # NOTE: textblob.wordnet is not imported so that the wordnet corpus can be lazy-loaded
44: (0)             _wordnet = nltk.corpus.wordnet
45: (0)             basestring = (str, bytes)
46: (0)             def _penn_to_wordnet(tag):
47: (4)                 """Converts a Penn corpus tag into a Wordnet tag."""
48: (4)                 if tag in ("NN", "NNS", "NNP", "NNPS"):
49: (8)                     return _wordnet.NOUN
50: (4)                 if tag in ("JJ", "JJR", "JJS"):
51: (8)                     return _wordnet.ADJ
52: (4)                 if tag in ("VB", "VBD", "VBG", "VBN", "VBP", "VBZ"):
53: (8)                     return _wordnet.VERB
54: (4)                 if tag in ("RB", "RBR", "RBS"):
55: (8)                     return _wordnet.ADV
56: (4)                 return None
57: (0)             class Word(str):
58: (4)                 """A simple word representation. Includes methods for inflection,
59: (4)                 translation, and WordNet integration.
60: (4)                 """
61: (4)                 def __new__(cls, string, pos_tag=None):
62: (8)                     """Return a new instance of the class. It is necessary to override
63: (8)                     this method in order to handle the extra pos_tag argument in the
64: (8)                     constructor.
65: (8)                     """
66: (8)                     return super().__new__(cls, string)
67: (4)                 def __init__(self, string, pos_tag=None):
68: (8)                     self.string = string
69: (8)                     self.pos_tag = pos_tag
70: (4)                 def __repr__(self):
71: (8)                     return repr(self.string)
72: (4)                 def __str__(self):
73: (8)                     return self.string
74: (4)                 def singularize(self):
75: (8)                     """Return the singular version of the word as a string."""
76: (8)                     return Word(_singularize(self.string))
77: (4)                 def pluralize(self):
78: (8)                     """Return the plural version of the word as a string."""
79: (8)                     return Word(_pluralize(self.string))
80: (4)                 def spellcheck(self):
81: (8)                     """Return a list of (word, confidence) tuples of spelling corrections.
82: (8)                     Based on: Peter Norvig, "How to Write a Spelling Corrector"
83: (8)                     (http://norvig.com/spell-correct.html) as implemented in the pattern
84: (8)                     library.
85: (8)                     .. versionadded:: 0.6.0
86: (8)                     """
87: (8)                     return suggest(self.string)
88: (4)                 def correct(self):
89: (8)                     """Correct the spelling of the word. Returns the word with the highest
90: (8)                     confidence using the spelling corrector.
91: (8)                     .. versionadded:: 0.6.0
92: (8)                     """
93: (8)                     return Word(self.spellcheck()[0][0])
94: (4)                 @cached_property
95: (4)                 @requires_nltk_corpus
96: (4)                 def lemma(self):
97: (8)                     """Return the lemma of this word using Wordnet's morphy function."""
98: (8)                     return self.lemmatize(pos=self.pos_tag)
99: (4)                 @requires_nltk_corpus
100: (4)                def lemmatize(self, pos=None):
101: (8)                    """Return the lemma for a word using WordNet's morphy function.
102: (8)                    :param pos: Part of speech to filter upon. If `None`, defaults to
103: (12)                       ``_wordnet.NOUN``.
104: (8)                    .. versionadded:: 0.8.1
105: (8)                    """
106: (8)                    if pos is None:
107: (12)                       tag = _wordnet.NOUN
108: (8)                    elif pos in _wordnet._FILEMAP.keys():
109: (12)                       tag = pos
110: (8)                    else:
111: (12)                       tag = _penn_to_wordnet(pos)
112: (8)                    lemmatizer = nltk.stem.WordNetLemmatizer()
113: (8)                    return lemmatizer.lemmatize(self.string, tag)
114: (4)                PorterStemmer = nltk.stem.porter.PorterStemmer()
115: (4)                LancasterStemmer = nltk.stem.lancaster.LancasterStemmer()
116: (4)                SnowballStemmer = nltk.stem.snowball.SnowballStemmer("english")
117: (4)                # added 'stemmer' on lines of lemmatizer
118: (4)                # based on nltk
119: (4)                def stem(self, stemmer=PorterStemmer):
120: (8)                    """Stem a word using various NLTK stemmers. (Default: Porter Stemmer)
121: (8)                    .. versionadded:: 0.12.0
122: (8)                    """
123: (8)                    return stemmer.stem(self.string)
124: (4)                @cached_property
125: (4)                def synsets(self):
126: (8)                    """The list of Synset objects for this Word.
127: (8)                    :rtype: list of Synsets
128: (8)                    .. versionadded:: 0.7.0
129: (8)                    """
130: (8)                    return self.get_synsets(pos=None)
131: (4)                @cached_property
132: (4)                def definitions(self):
133: (8)                    """The list of definitions for this word. Each definition corresponds
134: (8)                    to a synset.
135: (8)                    .. versionadded:: 0.7.0
136: (8)                    """
137: (8)                    return self.define(pos=None)
138: (4)                def get_synsets(self, pos=None):
139: (8)                    """Return a list of Synset objects for this word.
140: (8)                    :param pos: A part-of-speech tag to filter upon. If ``None``, all
141: (12)                       synsets for all parts of speech will be loaded.
142: (8)                    :rtype: list of Synsets
143: (8)                    .. versionadded:: 0.7.0
144: (8)                    """
145: (8)                    return _wordnet.synsets(self.string, pos)
146: (4)                def define(self, pos=None):
147: (8)                    """Return a list of definitions for this word. Each definition
148: (8)                    corresponds to a synset for this word.
149: (8)                    :param pos: A part-of-speech tag to filter upon. If ``None``, definitions
150: (12)                       for all parts of speech will be loaded.
151: (8)                    :rtype: List of strings
152: (8)                    .. versionadded:: 0.7.0
153: (8)                    """
154: (8)                    return [syn.definition() for syn in self.get_synsets(pos=pos)]
155: (0)            class WordList(list):
156: (4)                """A list-like collection of words."""
157: (4)                def __init__(self, collection):
158: (8)                    """Initialize a WordList. Takes a collection of strings as
159: (8)                    its only argument.
160: (8)                    """
161: (8)                    super().__init__([Word(w) for w in collection])
162: (4)                def __str__(self):
163: (8)                    """Returns a string representation for printing."""
164: (8)                    return super().__repr__()
165: (4)                def __repr__(self):
166: (8)                    """Returns a string representation for debugging."""
167: (8)                    class_name = self.__class__.__name__
168: (8)                    return f"{class_name}({super().__repr__()})"
169: (4)                def __getitem__(self, key):
170: (8)                    """Returns a string at the given index."""
171: (8)                    item = super().__getitem__(key)
172: (8)                    if isinstance(key, slice):
173: (12)                       return self.__class__(item)
174: (8)                    else:
175: (12)                       return item
176: (4)                def __getslice__(self, i, j):
177: (8)                    # This is included for Python 2.* compatibility
178: (8)                    return self.__class__(super().__getslice__(i, j))
179: (4)                def __setitem__(self, index, obj):
180: (8)                    """Places object at given index, replacing existing item. If the object
181: (8)                    is a string, inserts a :class:`Word <Word>` object.
182: (8)                    """
183: (8)                    if isinstance(obj, basestring):
184: (12)                       super().__setitem__(index, Word(obj))
185: (8)                    else:
186: (12)                       super().__setitem__(index, obj)
187: (4)                def count(self, strg, case_sensitive=False, *args, **kwargs):
188: (8)                    """Get the count of a word or phrase `s` within this WordList.
189: (8)                    :param strg: The string to count.
190: (8)                    :param case_sensitive: A boolean, whether or not the search is case-sensitive.
191: (8)                    """
192: (8)                    if not case_sensitive:
193: (12)                       return [word.lower() for word in self].count(strg.lower(), *args, **kwargs)
194: (8)                    return super().count(strg, *args, **kwargs)
195: (4)                def append(self, obj):
196: (8)                    """Append an object to end. If the object is a string, appends a
197: (8)                    :class:`Word <Word>` object.
198: (8)                    """
199: (8)                    if isinstance(obj, basestring):
200: (12)                       super().append(Word(obj))
201: (8)                    else:
202: (12)                       super().append(obj)
203: (4)                def extend(self, iterable):
204: (8)                    """Extend WordList by appending elements from ``iterable``. If an element
205: (8)                    is a string, appends a :class:`Word <Word>` object.
206: (8)                    """
207: (8)                    for e in iterable:
208: (12)                       self.append(e)
209: (4)                def upper(self):
210: (8)                    """Return a new WordList with each word upper-cased."""
211: (8)                    return self.__class__([word.upper() for word in self])
212: (4)                def lower(self):
213: (8)                    """Return a new WordList with each word lower-cased."""
214: (8)                    return self.__class__([word.lower() for word in self])
215: (4)                def singularize(self):
216: (8)                    """Return the single version of each word in this WordList."""
217: (8)                    return self.__class__([word.singularize() for word in self])
218: (4)                def pluralize(self):
219: (8)                    """Return the plural version of each word in this WordList."""
220: (8)                    return self.__class__([word.pluralize() for word in self])
221: (4)                def lemmatize(self):
222: (8)                    """Return the lemma of each word in this WordList."""
223: (8)                    return self.__class__([word.lemmatize() for word in self])
224: (4)                def stem(self, *args, **kwargs):
225: (8)                    """Return the stem for each word in this WordList."""
226: (8)                    return self.__class__([word.stem(*args, **kwargs) for word in self])
227: (0)            def _validated_param(obj, name, base_class, default, base_class_name=None):
228: (4)                """Validates a parameter passed to __init__. Makes sure that obj is
229: (4)                the correct class. Return obj if it's not None or falls back to default
230: (4)                :param obj: The object passed in.
231: (4)                :param name: The name of the parameter.
232: (4)                :param base_class: The class that obj must inherit from.
233: (4)                :param default: The default object to fall back upon if obj is None.
234: (4)                """
235: (4)                base_class_name = base_class_name if base_class_name else base_class.__name__
236: (4)                if obj is not None and not isinstance(obj, base_class):
237: (8)                    raise ValueError(f"{name} must be an instance of {base_class_name}")
238: (4)                return obj or default
239: (0)            def _initialize_models(
240: (4)                obj, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier
241: (0)            ):
242: (4)                """Common initialization between BaseBlob and Blobber classes."""
243: (4)                # tokenizer may be a textblob or an NLTK tokenizer
244: (4)                obj.tokenizer = _validated_param(
245: (8)                    tokenizer,
246: (8)                    "tokenizer",
247: (8)                    base_class=(BaseTokenizer, nltk.tokenize.api.TokenizerI),
248: (8)                    default=BaseBlob.tokenizer,
249: (8)                    base_class_name="BaseTokenizer",
250: (4)                )
251: (4)                obj.np_extractor = _validated_param(
252: (8)                    np_extractor,
253: (8)                    "np_extractor",
254: (8)                    base_class=BaseNPExtractor,
255: (8)                    default=BaseBlob.np_extractor,
256: (4)                )
257: (4)                obj.pos_tagger = _validated_param(
258: (8)                    pos_tagger, "pos_tagger", BaseTagger, BaseBlob.pos_tagger
259: (4)                )
260: (4)                obj.analyzer = _validated_param(
261: (8)                    analyzer, "analyzer", BaseSentimentAnalyzer, BaseBlob.analyzer
262: (4)                )
263: (4)                obj.parser = _validated_param(parser, "parser", BaseParser, BaseBlob.parser)
264: (4)                obj.classifier = classifier
265: (0)            class BaseBlob(StringlikeMixin, BlobComparableMixin):
266: (4)                """An abstract base class that all textblob classes will inherit from.
267: (4)                Includes words, POS tag, NP, and word count properties. Also includes
268: (4)                basic dunder and string methods for making objects like Python strings.
269: (4)                :param text: A string.
270: (4)                :param tokenizer: (optional) A tokenizer instance. If ``None``,
271: (8)                    defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.
272: (4)                :param np_extractor: (optional) An NPExtractor instance. If ``None``,
273: (8)                    defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.
274: (4)                :param pos_tagger: (optional) A Tagger instance. If ``None``,
275: (8)                    defaults to :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.
276: (4)                :param analyzer: (optional) A sentiment analyzer. If ``None``,
277: (8)                    defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
278: (4)                :param parser: A parser. If ``None``, defaults to
279: (8)                    :class:`PatternParser <textblob.en.parsers.PatternParser>`.
280: (4)                :param classifier: A classifier.
281: (4)                .. versionchanged:: 0.6.0
282: (8)                    ``clean_html`` parameter deprecated, as it was in NLTK.
283: (4)                """  # noqa: E501
284: (4)                np_extractor = FastNPExtractor()
285: (4)                pos_tagger = NLTKTagger()
286: (4)                tokenizer = WordTokenizer()
287: (4)                analyzer = PatternAnalyzer()
288: (4)                parser = PatternParser()
289: (4)                def __init__(
290: (8)                    self,
291: (8)                    text,
292: (8)                    tokenizer=None,
293: (8)                    pos_tagger=None,
294: (8)                    np_extractor=None,
295: (8)                    analyzer=None,
296: (8)                    parser=None,
297: (8)                    classifier=None,
298: (8)                    clean_html=False,
299: (4)                ):
300: (8)                    if not isinstance(text, basestring):
301: (12)                       raise TypeError(
302: (16)                           "The `text` argument passed to `__init__(text)` "
303: (16)                           f"must be a string, not {type(text)}"
304: (12)                       )
305: (8)                    if clean_html:
306: (12)                       raise NotImplementedError(
307: (16)                           "clean_html has been deprecated. "
308: (16)                           "To remove HTML markup, use BeautifulSoup's "
309: (16)                           "get_text() function"
310: (12)                       )
311: (8)                    self.raw = self.string = text
312: (8)                    self.stripped = lowerstrip(self.raw, all=True)
313: (8)                    _initialize_models(
314: (12)                       self, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier
315: (8)                    )
316: (4)                @cached_property
317: (4)                def words(self):
318: (8)                    """Return a list of word tokens. This excludes punctuation characters.
319: (8)                    If you want to include punctuation characters, access the ``tokens``
320: (8)                    property.
321: (8)                    :returns: A :class:`WordList <WordList>` of word tokens.
322: (8)                    """
323: (8)                    return WordList(word_tokenize(self.raw, include_punc=False))
324: (4)                @cached_property
325: (4)                def tokens(self):
326: (8)                    """Return a list of tokens, using this blob's tokenizer object
327: (8)                    (defaults to :class:`WordTokenizer <textblob.tokenizers.WordTokenizer>`).
328: (8)                    """
329: (8)                    return WordList(self.tokenizer.tokenize(self.raw))
330: (4)                def tokenize(self, tokenizer=None):
331: (8)                    """Return a list of tokens, using ``tokenizer``.
332: (8)                    :param tokenizer: (optional) A tokenizer object. If None, defaults to
333: (12)                       this blob's default tokenizer.
334: (8)                    """
335: (8)                    t = tokenizer if tokenizer is not None else self.tokenizer
336: (8)                    return WordList(t.tokenize(self.raw))
337: (4)                def parse(self, parser=None):
338: (8)                    """Parse the text.
339: (8)                    :param parser: (optional) A parser instance. If ``None``, defaults to
340: (12)                       this blob's default parser.
341: (8)                    .. versionadded:: 0.6.0
342: (8)                    """
343: (8)                    p = parser if parser is not None else self.parser
344: (8)                    return p.parse(self.raw)
345: (4)                def classify(self):
346: (8)                    """Classify the blob using the blob's ``classifier``."""
347: (8)                    if self.classifier is None:
348: (12)                       raise NameError("This blob has no classifier. Train one first!")
349: (8)                    return self.classifier.classify(self.raw)
350: (4)                @cached_property
351: (4)                def sentiment(self):
352: (8)                    """Return a tuple of form (polarity, subjectivity ) where polarity
353: (8)                    is a float within the range [-1.0, 1.0] and subjectivity is a float
354: (8)                    within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is
355: (8)                    very subjective.
356: (8)                    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)``
357: (8)                    """
358: (8)                    return self.analyzer.analyze(self.raw)
359: (4)                @cached_property
360: (4)                def sentiment_assessments(self):
361: (8)                    """Return a tuple of form (polarity, subjectivity, assessments ) where
362: (8)                    polarity is a float within the range [-1.0, 1.0], subjectivity is a
363: (8)                    float within the range [0.0, 1.0] where 0.0 is very objective and 1.0
364: (8)                    is very subjective, and assessments is a list of polarity and
365: (8)                    subjectivity scores for the assessed tokens.
366: (8)                    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity,
367: (8)                    assessments)``
368: (8)                    """
369: (8)                    return self.analyzer.analyze(self.raw, keep_assessments=True)
370: (4)                @cached_property
371: (4)                def polarity(self):
372: (8)                    """Return the polarity score as a float within the range [-1.0, 1.0]
373: (8)                    :rtype: float
374: (8)                    """
375: (8)                    return PatternAnalyzer().analyze(self.raw)[0]
376: (4)                @cached_property
377: (4)                def subjectivity(self):
378: (8)                    """Return the subjectivity score as a float within the range [0.0, 1.0]
379: (8)                    where 0.0 is very objective and 1.0 is very subjective.
380: (8)                    :rtype: float
381: (8)                    """
382: (8)                    return PatternAnalyzer().analyze(self.raw)[1]
383: (4)                @cached_property
384: (4)                def noun_phrases(self):
385: (8)                    """Returns a list of noun phrases for this blob."""
386: (8)                    return WordList(
387: (12)                       [
388: (16)                           phrase.strip().lower()
389: (16)                           for phrase in self.np_extractor.extract(self.raw)
390: (16)                           if len(phrase) > 1
391: (12)                       ]
392: (8)                    )
393: (4)                @cached_property
394: (4)                def pos_tags(self):
395: (8)                    """Returns an list of tuples of the form (word, POS tag).
396: (8)                    Example:
397: (8)                    ::
398: (12)                       [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'),
399: (20)                               ('Thursday', 'NNP'), ('morning', 'NN')]
400: (8)                    :rtype: list of tuples
401: (8)                    """
402: (8)                    if isinstance(self, TextBlob):
403: (12)                       return [
404: (16)                           val
405: (16)                           for sublist in [s.pos_tags for s in self.sentences]
406: (16)                           for val in sublist
407: (12)                       ]
408: (8)                    else:
409: (12)                       return [
410: (16)                           (Word(str(word), pos_tag=t), str(t))
411: (16)                           for word, t in self.pos_tagger.tag(self)
412: (16)                           if not PUNCTUATION_REGEX.match(str(t))
413: (12)                       ]
414: (4)                tags = pos_tags
415: (4)                @cached_property
416: (4)                def word_counts(self):
417: (8)                    """Dictionary of word frequencies in this text."""
418: (8)                    counts = defaultdict(int)
419: (8)                    stripped_words = [lowerstrip(word) for word in self.words]
420: (8)                    for word in stripped_words:
421: (12)                       counts[word] += 1
422: (8)                    return counts
423: (4)                @cached_property
424: (4)                def np_counts(self):
425: (8)                    """Dictionary of noun phrase frequencies in this text."""
426: (8)                    counts = defaultdict(int)
427: (8)                    for phrase in self.noun_phrases:
428: (12)                       counts[phrase] += 1
429: (8)                    return counts
430: (4)                def ngrams(self, n=3):
431: (8)                    """Return a list of n-grams (tuples of n successive words) for this
432: (8)                    blob.
433: (8)                    :rtype: List of :class:`WordLists <WordList>`
434: (8)                    """
435: (8)                    if n <= 0:
436: (12)                       return []
437: (8)                    grams = [
438: (12)                       WordList(self.words[i : i + n]) for i in range(len(self.words) - n + 1)
439: (8)                    ]
440: (8)                    return grams
441: (4)                def correct(self):
442: (8)                    """Attempt to correct the spelling of a blob.
443: (8)                    .. versionadded:: 0.6.0
444: (8)                    :rtype: :class:`BaseBlob <BaseBlob>`
445: (8)                    """
446: (8)                    # regex matches: word or punctuation or whitespace
447: (8)                    tokens = nltk.tokenize.regexp_tokenize(self.raw, r"\w+|[^\w\s]|\s")
448: (8)                    corrected = (Word(w).correct() for w in tokens)
449: (8)                    ret = "".join(corrected)
450: (8)                    return self.__class__(ret)
451: (4)                def _cmpkey(self):
452: (8)                    """Key used by ComparableMixin to implement all rich comparison
453: (8)                    operators.
454: (8)                    """
455: (8)                    return self.raw
456: (4)                def _strkey(self):
457: (8)                    """Key used by StringlikeMixin to implement string methods."""
458: (8)                    return self.raw
459: (4)                def __hash__(self):
460: (8)                    return hash(self._cmpkey())
461: (4)                def __add__(self, other):
462: (8)                    """Concatenates two text objects the same way Python strings are
463: (8)                    concatenated.
464: (8)                    Arguments:
465: (8)                    - `other`: a string or a text object
466: (8)                    """
467: (8)                    if isinstance(other, basestring):
468: (12)                       return self.__class__(self.raw + other)
469: (8)                    elif isinstance(other, BaseBlob):
470: (12)                       return self.__class__(self.raw + other.raw)
471: (8)                    else:
472: (12)                       raise TypeError(
473: (16)                           f"Operands must be either strings or {self.__class__.__name__} objects"
474: (12)                       )
475: (4)                def split(self, sep=None, maxsplit=sys.maxsize):
476: (8)                    """Behaves like the built-in str.split() except returns a
477: (8)                    WordList.
478: (8)                    :rtype: :class:`WordList <WordList>`
479: (8)                    """
480: (8)                    return WordList(self._strkey().split(sep, maxsplit))
481: (0)            class TextBlob(BaseBlob):
482: (4)                """A general text block, meant for larger bodies of text (esp. those
483: (4)                containing sentences). Inherits from :class:`BaseBlob <BaseBlob>`.
484: (4)                :param str text: A string.
485: (4)                :param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to
486: (8)                    :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.
487: (4)                :param np_extractor: (optional) An NPExtractor instance. If ``None``,
488: (8)                    defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.
489: (4)                :param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to
490: (8)                    :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.
491: (4)                :param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to
492: (8)                    :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
493: (4)                :param classifier: (optional) A classifier.
494: (4)                """  # noqa: E501
495: (4)                @cached_property
496: (4)                def sentences(self):
497: (8)                    """Return list of :class:`Sentence <Sentence>` objects."""
498: (8)                    return self._create_sentence_objects()
499: (4)                @cached_property
500: (4)                def words(self):
501: (8)                    """Return a list of word tokens. This excludes punctuation characters.
502: (8)                    If you want to include punctuation characters, access the ``tokens``
503: (8)                    property.
504: (8)                    :returns: A :class:`WordList <WordList>` of word tokens.
505: (8)                    """
506: (8)                    return WordList(word_tokenize(self.raw, include_punc=False))
507: (4)                @property
508: (4)                def raw_sentences(self):
509: (8)                    """List of strings, the raw sentences in the blob."""
510: (8)                    return [sentence.raw for sentence in self.sentences]
511: (4)                @property
512: (4)                def serialized(self):
513: (8)                    """Returns a list of each sentence's dict representation."""
514: (8)                    return [sentence.dict for sentence in self.sentences]
515: (4)                def to_json(self, *args, **kwargs):
516: (8)                    """Return a json representation (str) of this blob.
517: (8)                    Takes the same arguments as json.dumps.
518: (8)                    .. versionadded:: 0.5.1
519: (8)                    """
520: (8)                    return json.dumps(self.serialized, *args, **kwargs)
521: (4)                @property
522: (4)                def json(self):
523: (8)                    """The json representation of this blob.
524: (8)                    .. versionchanged:: 0.5.1
525: (12)                       Made ``json`` a property instead of a method to restore backwards
526: (12)                       compatibility that was broken after version 0.4.0.
527: (8)                    """
528: (8)                    return self.to_json()
529: (4)                def _create_sentence_objects(self):
530: (8)                    """Returns a list of Sentence objects from the raw text."""
531: (8)                    sentence_objects = []
532: (8)                    sentences = sent_tokenize(self.raw)
533: (8)                    char_index = 0  # Keeps track of character index within the blob
534: (8)                    for sent in sentences:
535: (12)                       # Compute the start and end indices of the sentence
536: (12)                       # within the blob
537: (12)                       start_index = self.raw.index(sent, char_index)
538: (12)                       char_index += len(sent)
539: (12)                       end_index = start_index + len(sent)
540: (12)                       # Sentences share the same models as their parent blob
541: (12)                       s = Sentence(
542: (16)                           sent,
543: (16)                           start_index=start_index,
544: (16)                           end_index=end_index,
545: (16)                           tokenizer=self.tokenizer,
546: (16)                           np_extractor=self.np_extractor,
547: (16)                           pos_tagger=self.pos_tagger,
548: (16)                           analyzer=self.analyzer,
549: (16)                           parser=self.parser,
550: (16)                           classifier=self.classifier,
551: (12)                       )
552: (12)                       sentence_objects.append(s)
553: (8)                    return sentence_objects
554: (0)            class Sentence(BaseBlob):
555: (4)                """A sentence within a TextBlob. Inherits from :class:`BaseBlob <BaseBlob>`.
556: (4)                :param sentence: A string, the raw sentence.
557: (4)                :param start_index: An int, the index where this sentence begins
558: (24)                                   in a TextBlob. If not given, defaults to 0.
559: (4)                :param end_index: An int, the index where this sentence ends in
560: (24)                                   a TextBlob. If not given, defaults to the
561: (24)                                   length of the sentence - 1.
562: (4)                """
563: (4)                def __init__(self, sentence, start_index=0, end_index=None, *args, **kwargs):
564: (8)                    super().__init__(sentence, *args, **kwargs)
565: (8)                    #: The start index within a TextBlob
566: (8)                    self.start = self.start_index = start_index
567: (8)                    #: The end index within a textBlob
568: (8)                    self.end = self.end_index = end_index or len(sentence) - 1
569: (4)                @property
570: (4)                def dict(self):
571: (8)                    """The dict representation of this sentence."""
572: (8)                    return {
573: (12)                       "raw": self.raw,
574: (12)                       "start_index": self.start_index,
575: (12)                       "end_index": self.end_index,
576: (12)                       "stripped": self.stripped,
577: (12)                       "noun_phrases": self.noun_phrases,
578: (12)                       "polarity": self.polarity,
579: (12)                       "subjectivity": self.subjectivity,
580: (8)                    }
581: (0)            class Blobber:
582: (4)                """A factory for TextBlobs that all share the same tagger,
583: (4)                tokenizer, parser, classifier, and np_extractor.
584: (4)                Usage:
585: (8)                    >>> from textblob import Blobber
586: (8)                    >>> from textblob.taggers import NLTKTagger
587: (8)                    >>> from textblob.tokenizers import SentenceTokenizer
588: (8)                    >>> tb = Blobber(pos_tagger=NLTKTagger(), tokenizer=SentenceTokenizer())
589: (8)                    >>> blob1 = tb("This is one blob.")
590: (8)                    >>> blob2 = tb("This blob has the same tagger and tokenizer.")
591: (8)                    >>> blob1.pos_tagger is blob2.pos_tagger
592: (8)                    True
593: (4)                :param tokenizer: (optional) A tokenizer instance. If ``None``,
594: (8)                    defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.
595: (4)                :param np_extractor: (optional) An NPExtractor instance. If ``None``,
596: (8)                    defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.
597: (4)                :param pos_tagger: (optional) A Tagger instance. If ``None``,
598: (8)                    defaults to :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.
599: (4)                :param analyzer: (optional) A sentiment analyzer. If ``None``,
600: (8)                    defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
601: (4)                :param parser: A parser. If ``None``, defaults to
602: (8)                    :class:`PatternParser <textblob.en.parsers.PatternParser>`.
603: (4)                :param classifier: A classifier.
604: (4)                .. versionadded:: 0.4.0
605: (4)                """  # noqa: E501
606: (4)                np_extractor = FastNPExtractor()
607: (4)                pos_tagger = NLTKTagger()
608: (4)                tokenizer = WordTokenizer()
609: (4)                analyzer = PatternAnalyzer()
610: (4)                parser = PatternParser()
611: (4)                def __init__(
612: (8)                    self,
613: (8)                    tokenizer=None,
614: (8)                    pos_tagger=None,
615: (8)                    np_extractor=None,
616: (8)                    analyzer=None,
617: (8)                    parser=None,
618: (8)                    classifier=None,
619: (4)                ):
620: (8)                    _initialize_models(
621: (12)                       self, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier
622: (8)                    )
623: (4)                def __call__(self, text):
624: (8)                    """Return a new TextBlob object with this Blobber's ``np_extractor``,
625: (8)                    ``pos_tagger``, ``tokenizer``, ``analyzer``, and ``classifier``.
626: (8)                    :returns: A new :class:`TextBlob <TextBlob>`.
627: (8)                    """
628: (8)                    return TextBlob(
629: (12)                       text,
630: (12)                       tokenizer=self.tokenizer,
631: (12)                       pos_tagger=self.pos_tagger,
632: (12)                       np_extractor=self.np_extractor,
633: (12)                       analyzer=self.analyzer,
634: (12)                       parser=self.parser,
635: (12)                       classifier=self.classifier,
636: (8)                    )
637: (4)                def __repr__(self):
638: (8)                    classifier_name = (
639: (12)                       self.classifier.__class__.__name__ + "()" if self.classifier else "None"
640: (8)                    )
641: (8)                    return (
642: (12)                       "Blobber(tokenizer={}(), pos_tagger={}(), "
643: (12)                       "np_extractor={}(), analyzer={}(), parser={}(), classifier={})"
644: (8)                    ).format(
645: (12)                       self.tokenizer.__class__.__name__,
646: (12)                       self.pos_tagger.__class__.__name__,
647: (12)                       self.np_extractor.__class__.__name__,
648: (12)                       self.analyzer.__class__.__name__,
649: (12)                       self.parser.__class__.__name__,
650: (12)                       classifier_name,
651: (8)                    )
652: (4)                __str__ = __repr__

----------------------------------------

File 4 - . \_text.py:

1: (0)              """This file is adapted from the pattern library.
2: (0)              URL: http://www.clips.ua.ac.be/pages/pattern-web
3: (0)              Licence: BSD
4: (0)              """
5: (0)              import codecs
6: (0)              import os
7: (0)              import re
8: (0)              import string
9: (0)              import types
10: (0)             from itertools import chain
11: (0)             from xml.etree import ElementTree
12: (0)             basestring = (str, bytes)
13: (0)             try:
14: (4)                 MODULE = os.path.dirname(os.path.abspath(__file__))
15: (0)             except:
16: (4)                 MODULE = ""
17: (0)             SLASH, WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA = (
18: (4)                 "&slash;",
19: (4)                 "word",
20: (4)                 "part-of-speech",
21: (4)                 "chunk",
22: (4)                 "preposition",
23: (4)                 "relation",
24: (4)                 "anchor",
25: (4)                 "lemma",
26: (0)             )
27: (0)             # String functions
28: (0)             def decode_string(v, encoding="utf-8"):
29: (4)                 """Returns the given value as a Unicode string (if possible)."""
30: (4)                 if isinstance(encoding, basestring):
31: (8)                     encoding = ((encoding,),) + (("windows-1252",), ("utf-8", "ignore"))
32: (4)                 if isinstance(v, bytes):
33: (8)                     for e in encoding:
34: (12)                        try:
35: (16)                            return v.decode(*e)
36: (12)                        except:
37: (16)                            pass
38: (8)                     return v
39: (4)                 return str(v)
40: (0)             def encode_string(v, encoding="utf-8"):
41: (4)                 """Returns the given value as a Python byte string (if possible)."""
42: (4)                 if isinstance(encoding, basestring):
43: (8)                     encoding = ((encoding,),) + (("windows-1252",), ("utf-8", "ignore"))
44: (4)                 if isinstance(v, str):
45: (8)                     for e in encoding:
46: (12)                        try:
47: (16)                            return v.encode(*e)
48: (12)                        except:
49: (16)                            pass
50: (8)                     return v
51: (4)                 return str(v)
52: (0)             decode_utf8 = decode_string
53: (0)             encode_utf8 = encode_string
54: (0)             def isnumeric(strg):
55: (4)                 try:
56: (8)                     float(strg)
57: (4)                 except ValueError:
58: (8)                     return False
59: (4)                 return True
60: (0)             # --- LAZY DICTIONARY -------------------------------------------------------------------------------
61: (0)             # A lazy dictionary is empty until one of its methods is called.
62: (0)             # This way many instances (e.g., lexicons) can be created without using memory until used.
63: (0)             class lazydict(dict):
64: (4)                 def load(self):
65: (8)                     # Must be overridden in a subclass.
66: (8)                     # Must load data with dict.__setitem__(self, k, v) instead of lazydict[k] = v.
67: (8)                     pass
68: (4)                 def _lazy(self, method, *args):
69: (8)                     """If the dictionary is empty, calls lazydict.load().
70: (8)                     Replaces lazydict.method() with dict.method() and calls it.
71: (8)                     """
72: (8)                     if dict.__len__(self) == 0:
73: (12)                        self.load()
74: (12)                        setattr(self, method, types.MethodType(getattr(dict, method), self))
75: (8)                     return getattr(dict, method)(self, *args)
76: (4)                 def __repr__(self):
77: (8)                     return self._lazy("__repr__")
78: (4)                 def __len__(self):
79: (8)                     return self._lazy("__len__")
80: (4)                 def __iter__(self):
81: (8)                     return self._lazy("__iter__")
82: (4)                 def __contains__(self, *args):
83: (8)                     return self._lazy("__contains__", *args)
84: (4)                 def __getitem__(self, *args):
85: (8)                     return self._lazy("__getitem__", *args)
86: (4)                 def __setitem__(self, *args):
87: (8)                     return self._lazy("__setitem__", *args)
88: (4)                 def setdefault(self, *args):
89: (8)                     return self._lazy("setdefault", *args)
90: (4)                 def get(self, *args, **kwargs):
91: (8)                     return self._lazy("get", *args)
92: (4)                 def items(self):
93: (8)                     return self._lazy("items")
94: (4)                 def keys(self):
95: (8)                     return self._lazy("keys")
96: (4)                 def values(self):
97: (8)                     return self._lazy("values")
98: (4)                 def update(self, *args):
99: (8)                     return self._lazy("update", *args)
100: (4)                def pop(self, *args):
101: (8)                    return self._lazy("pop", *args)
102: (4)                def popitem(self, *args):
103: (8)                    return self._lazy("popitem", *args)
104: (0)            class lazylist(list):
105: (4)                def load(self):
106: (8)                    # Must be overridden in a subclass.
107: (8)                    # Must load data with list.append(self, v) instead of lazylist.append(v).
108: (8)                    pass
109: (4)                def _lazy(self, method, *args):
110: (8)                    """If the list is empty, calls lazylist.load().
111: (8)                    Replaces lazylist.method() with list.method() and calls it.
112: (8)                    """
113: (8)                    if list.__len__(self) == 0:
114: (12)                       self.load()
115: (12)                       setattr(self, method, types.MethodType(getattr(list, method), self))
116: (8)                    return getattr(list, method)(self, *args)
117: (4)                def __repr__(self):
118: (8)                    return self._lazy("__repr__")
119: (4)                def __len__(self):
120: (8)                    return self._lazy("__len__")
121: (4)                def __iter__(self):
122: (8)                    return self._lazy("__iter__")
123: (4)                def __contains__(self, *args):
124: (8)                    return self._lazy("__contains__", *args)
125: (4)                def insert(self, *args):
126: (8)                    return self._lazy("insert", *args)
127: (4)                def append(self, *args):
128: (8)                    return self._lazy("append", *args)
129: (4)                def extend(self, *args):
130: (8)                    return self._lazy("extend", *args)
131: (4)                def remove(self, *args):
132: (8)                    return self._lazy("remove", *args)
133: (4)                def pop(self, *args):
134: (8)                    return self._lazy("pop", *args)
135: (0)            # --- UNIVERSAL TAGSET ------------------------------------------------------------------------------
136: (0)            # The default part-of-speech tagset used in Pattern is Penn Treebank II.
137: (0)            # However, not all languages are well-suited to Penn Treebank (which was developed for English).
138: (0)            # As more languages are implemented, this is becoming more problematic.
139: (0)            #
140: (0)            # A universal tagset is proposed by Slav Petrov (2012):
141: (0)            # http://www.petrovi.de/data/lrec.pdf
142: (0)            #
143: (0)            # Subclasses of Parser should start implementing
144: (0)            # Parser.parse(tagset=UNIVERSAL) with a simplified tagset.
145: (0)            # The names of the constants correspond to Petrov's naming scheme, while
146: (0)            # the value of the constants correspond to Penn Treebank.
147: (0)            UNIVERSAL = "universal"
148: (0)            NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X = (
149: (4)                "NN",
150: (4)                "VB",
151: (4)                "JJ",
152: (4)                "RB",
153: (4)                "PR",
154: (4)                "DT",
155: (4)                "PP",
156: (4)                "PP",
157: (4)                "NO",
158: (4)                "CJ",
159: (4)                "UH",
160: (4)                "PT",
161: (4)                ".",
162: (4)                "X",
163: (0)            )
164: (0)            def penntreebank2universal(token, tag):
165: (4)                """Returns a (token, tag)-tuple with a simplified universal part-of-speech tag."""
166: (4)                if tag.startswith(("NNP-", "NNPS-")):
167: (8)                    return (token, "{}-{}".format(NOUN, tag.split("-")[-1]))
168: (4)                if tag in ("NN", "NNS", "NNP", "NNPS", "NP"):
169: (8)                    return (token, NOUN)
170: (4)                if tag in ("MD", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ"):
171: (8)                    return (token, VERB)
172: (4)                if tag in ("JJ", "JJR", "JJS"):
173: (8)                    return (token, ADJ)
174: (4)                if tag in ("RB", "RBR", "RBS", "WRB"):
175: (8)                    return (token, ADV)
176: (4)                if tag in ("PRP", "PRP$", "WP", "WP$"):
177: (8)                    return (token, PRON)
178: (4)                if tag in ("DT", "PDT", "WDT", "EX"):
179: (8)                    return (token, DET)
180: (4)                if tag in ("IN",):
181: (8)                    return (token, PREP)
182: (4)                if tag in ("CD",):
183: (8)                    return (token, NUM)
184: (4)                if tag in ("CC",):
185: (8)                    return (token, CONJ)
186: (4)                if tag in ("UH",):
187: (8)                    return (token, INTJ)
188: (4)                if tag in ("POS", "RP", "TO"):
189: (8)                    return (token, PRT)
190: (4)                if tag in ("SYM", "LS", ".", "!", "?", ",", ":", "(", ")", '"', "#", "$"):
191: (8)                    return (token, PUNC)
192: (4)                return (token, X)
193: (0)            # --- TOKENIZER -------------------------------------------------------------------------------------
194: (0)            TOKEN = re.compile(r"(\S+)\s")
195: (0)            # Handle common punctuation marks.
196: (0)            PUNCTUATION = punctuation = ".,;:!?()[]{}`''\"@#$^&*+-|=~_"
197: (0)            # Handle common abbreviations.
198: (0)            ABBREVIATIONS = abbreviations = set(
199: (4)                (
200: (8)                    "a.",
201: (8)                    "adj.",
202: (8)                    "adv.",
203: (8)                    "al.",
204: (8)                    "a.m.",
205: (8)                    "c.",
206: (8)                    "cf.",
207: (8)                    "comp.",
208: (8)                    "conf.",
209: (8)                    "def.",
210: (8)                    "ed.",
211: (8)                    "e.g.",
212: (8)                    "esp.",
213: (8)                    "etc.",
214: (8)                    "ex.",
215: (8)                    "f.",
216: (8)                    "fig.",
217: (8)                    "gen.",
218: (8)                    "id.",
219: (8)                    "i.e.",
220: (8)                    "int.",
221: (8)                    "l.",
222: (8)                    "m.",
223: (8)                    "Med.",
224: (8)                    "Mil.",
225: (8)                    "Mr.",
226: (8)                    "n.",
227: (8)                    "n.q.",
228: (8)                    "orig.",
229: (8)                    "pl.",
230: (8)                    "pred.",
231: (8)                    "pres.",
232: (8)                    "p.m.",
233: (8)                    "ref.",
234: (8)                    "v.",
235: (8)                    "vs.",
236: (8)                    "w/",
237: (4)                )
238: (0)            )
239: (0)            RE_ABBR1 = re.compile(r"^[A-Za-z]\.$")  # single letter, "T. De Smedt"
240: (0)            RE_ABBR2 = re.compile(r"^([A-Za-z]\.)+$")  # alternating letters, "U.S."
241: (0)            RE_ABBR3 = re.compile(
242: (4)                "^[A-Z]["
243: (4)                + "|".join(  # capital followed by consonants, "Mr."
244: (8)                    "bcdfghjklmnpqrstvwxz"
245: (4)                )
246: (4)                + "]+.$"
247: (0)            )
248: (0)            # Handle emoticons.
249: (0)            EMOTICONS = {  # (facial expression, sentiment)-keys
250: (4)                ("love", +1.00): set(("<3", "♥")),
251: (4)                ("grin", +1.00): set(
252: (8)                    (">:D", ":-D", ":D", "=-D", "=D", "X-D", "x-D", "XD", "xD", "8-D")
253: (4)                ),
254: (4)                ("taunt", +0.75): set(
255: (8)                    (">:P", ":-P", ":P", ":-p", ":p", ":-b", ":b", ":c)", ":o)", ":^)")
256: (4)                ),
257: (4)                ("smile", +0.50): set(
258: (8)                    (">:)", ":-)", ":)", "=)", "=]", ":]", ":}", ":>", ":3", "8)", "8-)")
259: (4)                ),
260: (4)                ("wink", +0.25): set((">;]", ";-)", ";)", ";-]", ";]", ";D", ";^)", "*-)", "*)")),
261: (4)                ("gasp", +0.05): set((">:o", ":-O", ":O", ":o", ":-o", "o_O", "o.O", "°O°", "°o°")),
262: (4)                ("worry", -0.25): set(
263: (8)                    (">:/", ":-/", ":/", ":\\", ">:\\", ":-.", ":-s", ":s", ":S", ":-S", ">.>")
264: (4)                ),
265: (4)                ("frown", -0.75): set(
266: (8)                    (">:[", ":-(", ":(", "=(", ":-[", ":[", ":{", ":-<", ":c", ":-c", "=/")
267: (4)                ),
268: (4)                ("cry", -1.00): set((":'(", ":'''(", ";'(")),
269: (0)            }
270: (0)            RE_EMOTICONS = [
271: (4)                r" ?".join([re.escape(each) for each in e]) for v in EMOTICONS.values() for e in v
272: (0)            ]
273: (0)            RE_EMOTICONS = re.compile(r"(%s)($|\s)" % "|".join(RE_EMOTICONS))
274: (0)            # Handle sarcasm punctuation (!).
275: (0)            RE_SARCASM = re.compile(r"\( ?\! ?\)")
276: (0)            # Handle common contractions.
277: (0)            replacements = {
278: (4)                "'d": " 'd",
279: (4)                "'m": " 'm",
280: (4)                "'s": " 's",
281: (4)                "'ll": " 'll",
282: (4)                "'re": " 're",
283: (4)                "'ve": " 've",
284: (4)                "n't": " n't",
285: (0)            }
286: (0)            # Handle paragraph line breaks (\n\n marks end of sentence).
287: (0)            EOS = "END-OF-SENTENCE"
288: (0)            def find_tokens(
289: (4)                string,
290: (4)                punctuation=PUNCTUATION,
291: (4)                abbreviations=ABBREVIATIONS,
292: (4)                replace=replacements,
293: (4)                linebreak=r"\n{2,}",
294: (0)            ):
295: (4)                """Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
296: (4)                Handles common cases of abbreviations (e.g., etc., ...).
297: (4)                Punctuation marks are split from other words. Periods (or ?!) mark the end of a sentence.
298: (4)                Headings without an ending period are inferred by line breaks.
299: (4)                """
300: (4)                # Handle periods separately.
301: (4)                punctuation = tuple(punctuation.replace(".", ""))
302: (4)                # Handle replacements (contractions).
303: (4)                for a, b in list(replace.items()):
304: (8)                    string = re.sub(a, b, string)
305: (4)                # Handle Unicode quotes.
306: (4)                if isinstance(string, str):
307: (8)                    string = (
308: (12)                       str(string)
309: (12)                       .replace("“", " “ ")
310: (12)                       .replace("”", " ” ")
311: (12)                       .replace("‘", " ‘ ")
312: (12)                       .replace("’", " ’ ")
313: (12)                       .replace("'", " ' ")
314: (12)                       .replace('"', ' " ')
315: (8)                    )
316: (4)                # Collapse whitespace.
317: (4)                string = re.sub("\r\n", "\n", string)
318: (4)                string = re.sub(linebreak, " %s " % EOS, string)
319: (4)                string = re.sub(r"\s+", " ", string)
320: (4)                tokens = []
321: (4)                for t in TOKEN.findall(string + " "):
322: (8)                    if len(t) > 0:
323: (12)                       tail = []
324: (12)                       while t.startswith(punctuation) and t not in replace:
325: (16)                           # Split leading punctuation.
326: (16)                           if t.startswith(punctuation):
327: (20)                               tokens.append(t[0])
328: (20)                               t = t[1:]
329: (12)                       while t.endswith(punctuation + (".",)) and t not in replace:
330: (16)                           # Split trailing punctuation.
331: (16)                           if t.endswith(punctuation):
332: (20)                               tail.append(t[-1])
333: (20)                               t = t[:-1]
334: (16)                           # Split ellipsis (...) before splitting period.
335: (16)                           if t.endswith("..."):
336: (20)                               tail.append("...")
337: (20)                               t = t[:-3].rstrip(".")
338: (16)                           # Split period (if not an abbreviation).
339: (16)                           if t.endswith("."):
340: (20)                               if (
341: (24)                                   t in abbreviations
342: (24)                                   or RE_ABBR1.match(t) is not None
343: (24)                                   or RE_ABBR2.match(t) is not None
344: (24)                                   or RE_ABBR3.match(t) is not None
345: (20)                               ):
346: (24)                                   break
347: (20)                               else:
348: (24)                                   tail.append(t[-1])
349: (24)                                   t = t[:-1]
350: (12)                       if t != "":
351: (16)                           tokens.append(t)
352: (12)                       tokens.extend(reversed(tail))
353: (4)                sentences, i, j = [[]], 0, 0
354: (4)                while j < len(tokens):
355: (8)                    if tokens[j] in ("...", ".", "!", "?", EOS):
356: (12)                       # Handle citations, trailing parenthesis, repeated punctuation (!?).
357: (12)                       while j < len(tokens) and tokens[j] in (
358: (16)                           "'",
359: (16)                           '"',
360: (16)                           "”",
361: (16)                           "’",
362: (16)                           "...",
363: (16)                           ".",
364: (16)                           "!",
365: (16)                           "?",
366: (16)                           ")",
367: (16)                           EOS,
368: (12)                       ):
369: (16)                           if tokens[j] in ("'", '"') and sentences[-1].count(tokens[j]) % 2 == 0:
370: (20)                               break  # Balanced quotes.
371: (16)                           j += 1
372: (12)                       sentences[-1].extend(t for t in tokens[i:j] if t != EOS)
373: (12)                       sentences.append([])
374: (12)                       i = j
375: (8)                    j += 1
376: (4)                sentences[-1].extend(tokens[i:j])
377: (4)                sentences = (" ".join(s) for s in sentences if len(s) > 0)
378: (4)                sentences = (RE_SARCASM.sub("(!)", s) for s in sentences)
379: (4)                sentences = [
380: (8)                    RE_EMOTICONS.sub(lambda m: m.group(1).replace(" ", "") + m.group(2), s)
381: (8)                    for s in sentences
382: (4)                ]
383: (4)                return sentences
384: (0)            #### LEXICON #######################################################################################
385: (0)            # --- LEXICON ---------------------------------------------------------------------------------------
386: (0)            # Pattern's text parsers are based on Brill's algorithm.
387: (0)            # Brill's algorithm automatically acquires a lexicon of known words,
388: (0)            # and a set of rules for tagging unknown words from a training corpus.
389: (0)            # Lexical rules are used to tag unknown words, based on the word morphology (prefix, suffix, ...).
390: (0)            # Contextual rules are used to tag all words, based on the word's role in the sentence.
391: (0)            # Named entity rules are used to discover proper nouns (NNP's).
392: (0)            def _read(path, encoding="utf-8", comment=";;;"):
393: (4)                """Returns an iterator over the lines in the file at the given path,
394: (4)                stripping comments and decoding each line to Unicode.
395: (4)                """
396: (4)                if path:
397: (8)                    if isinstance(path, basestring) and os.path.exists(path):
398: (12)                       # From file path.
399: (12)                       f = open(path, encoding="utf-8")
400: (8)                    elif isinstance(path, basestring):
401: (12)                       # From string.
402: (12)                       f = path.splitlines()
403: (8)                    elif hasattr(path, "read"):
404: (12)                       # From string buffer.
405: (12)                       f = path.read().splitlines()
406: (8)                    else:
407: (12)                       f = path
408: (8)                    for i, line in enumerate(f):
409: (12)                       line = (
410: (16)                           line.strip(codecs.BOM_UTF8)
411: (16)                           if i == 0 and isinstance(line, bytes)
412: (16)                           else line
413: (12)                       )
414: (12)                       line = line.strip()
415: (12)                       line = decode_utf8(line)
416: (12)                       if not line or (comment and line.startswith(comment)):
417: (16)                           continue
418: (12)                       yield line
419: (4)                return
420: (0)            class Lexicon(lazydict):
421: (4)                def __init__(
422: (8)                    self,
423: (8)                    path="",
424: (8)                    morphology=None,
425: (8)                    context=None,
426: (8)                    entities=None,
427: (8)                    NNP="NNP",
428: (8)                    language=None,
429: (4)                ):
430: (8)                    """A dictionary of words and their part-of-speech tags.
431: (8)                    For unknown words, rules for word morphology, context and named entities can be used.
432: (8)                    """
433: (8)                    self._path = path
434: (8)                    self._language = language
435: (8)                    self.morphology = Morphology(self, path=morphology)
436: (8)                    self.context = Context(self, path=context)
437: (8)                    self.entities = Entities(self, path=entities, tag=NNP)
438: (4)                def load(self):
439: (8)                    # Arnold NNP x
440: (8)                    dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if x.strip()))
441: (4)                @property
442: (4)                def path(self):
443: (8)                    return self._path
444: (4)                @property
445: (4)                def language(self):
446: (8)                    return self._language
447: (0)            # --- MORPHOLOGICAL RULES ---------------------------------------------------------------------------
448: (0)            # Brill's algorithm generates lexical (i.e., morphological) rules in the following format:
449: (0)            # NN s fhassuf 1 NNS x => unknown words ending in -s and tagged NN change to NNS.
450: (0)            #     ly hassuf 2 RB x => unknown words ending in -ly change to RB.
451: (0)            class Rules:
452: (4)                def __init__(self, lexicon=None, cmd=None):
453: (8)                    if cmd is None:
454: (12)                       cmd = {}
455: (8)                    if lexicon is None:
456: (12)                       lexicon = {}
457: (8)                    self.lexicon, self.cmd = lexicon, cmd
458: (4)                def apply(self, x):
459: (8)                    """Applies the rule to the given token or list of tokens."""
460: (8)                    return x
461: (0)            class Morphology(lazylist, Rules):
462: (4)                def __init__(self, lexicon=None, path=""):
463: (8)                    """A list of rules based on word morphology (prefix, suffix)."""
464: (8)                    if lexicon is None:
465: (12)                       lexicon = {}
466: (8)                    cmd = (
467: (12)                       "char",  # Word contains x.
468: (12)                       "haspref",  # Word starts with x.
469: (12)                       "hassuf",  # Word end with x.
470: (12)                       "addpref",  # x + word is in lexicon.
471: (12)                       "addsuf",  # Word + x is in lexicon.
472: (12)                       "deletepref",  # Word without x at the start is in lexicon.
473: (12)                       "deletesuf",  # Word without x at the end is in lexicon.
474: (12)                       "goodleft",  # Word preceded by word x.
475: (12)                       "goodright",  # Word followed by word x.
476: (8)                    )
477: (8)                    cmd = dict.fromkeys(cmd, True)
478: (8)                    cmd.update(("f" + k, v) for k, v in list(cmd.items()))
479: (8)                    Rules.__init__(self, lexicon, cmd)
480: (8)                    self._path = path
481: (4)                @property
482: (4)                def path(self):
483: (8)                    return self._path
484: (4)                def load(self):
485: (8)                    # ["NN", "s", "fhassuf", "1", "NNS", "x"]
486: (8)                    list.extend(self, (x.split() for x in _read(self._path)))
487: (4)                def apply(self, token, previous=(None, None), next=(None, None)):
488: (8)                    """Applies lexical rules to the given token, which is a [word, tag] list."""
489: (8)                    w = token[0]
490: (8)                    for r in self:
491: (12)                       if r[1] in self.cmd:  # Rule = ly hassuf 2 RB x
492: (16)                           f, x, pos, cmd = bool(0), r[0], r[-2], r[1].lower()
493: (12)                       if r[2] in self.cmd:  # Rule = NN s fhassuf 1 NNS x
494: (16)                           f, x, pos, cmd = bool(1), r[1], r[-2], r[2].lower().lstrip("f")
495: (12)                       if f and token[1] != r[0]:
496: (16)                           continue
497: (12)                       if (
498: (16)                           (cmd == "char" and x in w)
499: (16)                           or (cmd == "haspref" and w.startswith(x))
500: (16)                           or (cmd == "hassuf" and w.endswith(x))
501: (16)                           or (cmd == "addpref" and x + w in self.lexicon)
502: (16)                           or (cmd == "addsuf" and w + x in self.lexicon)
503: (16)                           or (
504: (20)                               cmd == "deletepref"
505: (20)                               and w.startswith(x)
506: (20)                               and w[len(x) :] in self.lexicon
507: (16)                           )
508: (16)                           or (
509: (20)                               cmd == "deletesuf"
510: (20)                               and w.endswith(x)
511: (20)                               and w[: -len(x)] in self.lexicon
512: (16)                           )
513: (16)                           or (cmd == "goodleft" and x == next[0])
514: (16)                           or (cmd == "goodright" and x == previous[0])
515: (12)                       ):
516: (16)                           token[1] = pos
517: (8)                    return token
518: (4)                def insert(self, i, tag, affix, cmd="hassuf", tagged=None):
519: (8)                    """Inserts a new rule that assigns the given tag to words with the given affix,
520: (8)                    e.g., Morphology.append("RB", "-ly").
521: (8)                    """
522: (8)                    if affix.startswith("-") and affix.endswith("-"):
523: (12)                       affix, cmd = affix[+1:-1], "char"
524: (8)                    if affix.startswith("-"):
525: (12)                       affix, cmd = affix[+1:-0], "hassuf"
526: (8)                    if affix.endswith("-"):
527: (12)                       affix, cmd = affix[+0:-1], "haspref"
528: (8)                    if tagged:
529: (12)                       r = [tagged, affix, "f" + cmd.lstrip("f"), tag, "x"]
530: (8)                    else:
531: (12)                       r = [affix, cmd.lstrip("f"), tag, "x"]
532: (8)                    lazylist.insert(self, i, r)
533: (4)                def append(self, *args, **kwargs):
534: (8)                    self.insert(len(self) - 1, *args, **kwargs)
535: (4)                def extend(self, rules=None):
536: (8)                    if rules is None:
537: (12)                       rules = []
538: (8)                    for r in rules:
539: (12)                       self.append(*r)
540: (0)            # --- CONTEXT RULES ---------------------------------------------------------------------------------
541: (0)            # Brill's algorithm generates contextual rules in the following format:
542: (0)            # VBD VB PREVTAG TO => unknown word tagged VBD changes to VB if preceded by a word tagged TO.
543: (0)            class Context(lazylist, Rules):
544: (4)                def __init__(self, lexicon=None, path=""):
545: (8)                    """A list of rules based on context (preceding and following words)."""
546: (8)                    if lexicon is None:
547: (12)                       lexicon = {}
548: (8)                    cmd = (
549: (12)                       "prevtag",  # Preceding word is tagged x.
550: (12)                       "nexttag",  # Following word is tagged x.
551: (12)                       "prev2tag",  # Word 2 before is tagged x.
552: (12)                       "next2tag",  # Word 2 after is tagged x.
553: (12)                       "prev1or2tag",  # One of 2 preceding words is tagged x.
554: (12)                       "next1or2tag",  # One of 2 following words is tagged x.
555: (12)                       "prev1or2or3tag",  # One of 3 preceding words is tagged x.
556: (12)                       "next1or2or3tag",  # One of 3 following words is tagged x.
557: (12)                       "surroundtag",  # Preceding word is tagged x and following word is tagged y.
558: (12)                       "curwd",  # Current word is x.
559: (12)                       "prevwd",  # Preceding word is x.
560: (12)                       "nextwd",  # Following word is x.
561: (12)                       "prev1or2wd",  # One of 2 preceding words is x.
562: (12)                       "next1or2wd",  # One of 2 following words is x.
563: (12)                       "next1or2or3wd",  # One of 3 preceding words is x.
564: (12)                       "prev1or2or3wd",  # One of 3 following words is x.
565: (12)                       "prevwdtag",  # Preceding word is x and tagged y.
566: (12)                       "nextwdtag",  # Following word is x and tagged y.
567: (12)                       "wdprevtag",  # Current word is y and preceding word is tagged x.
568: (12)                       "wdnexttag",  # Current word is x and following word is tagged y.
569: (12)                       "wdand2aft",  # Current word is x and word 2 after is y.
570: (12)                       "wdand2tagbfr",  # Current word is y and word 2 before is tagged x.
571: (12)                       "wdand2tagaft",  # Current word is x and word 2 after is tagged y.
572: (12)                       "lbigram",  # Current word is y and word before is x.
573: (12)                       "rbigram",  # Current word is x and word after is y.
574: (12)                       "prevbigram",  # Preceding word is tagged x and word before is tagged y.
575: (12)                       "nextbigram",  # Following word is tagged x and word after is tagged y.
576: (8)                    )
577: (8)                    Rules.__init__(self, lexicon, dict.fromkeys(cmd, True))
578: (8)                    self._path = path
579: (4)                @property
580: (4)                def path(self):
581: (8)                    return self._path
582: (4)                def load(self):
583: (8)                    # ["VBD", "VB", "PREVTAG", "TO"]
584: (8)                    list.extend(self, (x.split() for x in _read(self._path)))
585: (4)                def apply(self, tokens):
586: (8)                    """Applies contextual rules to the given list of tokens,
587: (8)                    where each token is a [word, tag] list.
588: (8)                    """
589: (8)                    o = [("STAART", "STAART")] * 3  # Empty delimiters for look ahead/back.
590: (8)                    t = o + tokens + o
591: (8)                    for i, token in enumerate(t):
592: (12)                       for r in self:
593: (16)                           if token[1] == "STAART":
594: (20)                               continue
595: (16)                           if token[1] != r[0] and r[0] != "*":
596: (20)                               continue
597: (16)                           cmd, x, y = r[2], r[3], r[4] if len(r) > 4 else ""
598: (16)                           cmd = cmd.lower()
599: (16)                           if (
600: (20)                               (cmd == "prevtag" and x == t[i - 1][1])
601: (20)                               or (cmd == "nexttag" and x == t[i + 1][1])
602: (20)                               or (cmd == "prev2tag" and x == t[i - 2][1])
603: (20)                               or (cmd == "next2tag" and x == t[i + 2][1])
604: (20)                               or (cmd == "prev1or2tag" and x in (t[i - 1][1], t[i - 2][1]))
605: (20)                               or (cmd == "next1or2tag" and x in (t[i + 1][1], t[i + 2][1]))
606: (20)                               or (
607: (24)                                   cmd == "prev1or2or3tag"
608: (24)                                   and x in (t[i - 1][1], t[i - 2][1], t[i - 3][1])
609: (20)                               )
610: (20)                               or (
611: (24)                                   cmd == "next1or2or3tag"
612: (24)                                   and x in (t[i + 1][1], t[i + 2][1], t[i + 3][1])
613: (20)                               )
614: (20)                               or (cmd == "surroundtag" and x == t[i - 1][1] and y == t[i + 1][1])
615: (20)                               or (cmd == "curwd" and x == t[i + 0][0])
616: (20)                               or (cmd == "prevwd" and x == t[i - 1][0])
617: (20)                               or (cmd == "nextwd" and x == t[i + 1][0])
618: (20)                               or (cmd == "prev1or2wd" and x in (t[i - 1][0], t[i - 2][0]))
619: (20)                               or (cmd == "next1or2wd" and x in (t[i + 1][0], t[i + 2][0]))
620: (20)                               or (cmd == "prevwdtag" and x == t[i - 1][0] and y == t[i - 1][1])
621: (20)                               or (cmd == "nextwdtag" and x == t[i + 1][0] and y == t[i + 1][1])
622: (20)                               or (cmd == "wdprevtag" and x == t[i - 1][1] and y == t[i + 0][0])
623: (20)                               or (cmd == "wdnexttag" and x == t[i + 0][0] and y == t[i + 1][1])
624: (20)                               or (cmd == "wdand2aft" and x == t[i + 0][0] and y == t[i + 2][0])
625: (20)                               or (cmd == "wdand2tagbfr" and x == t[i - 2][1] and y == t[i + 0][0])
626: (20)                               or (cmd == "wdand2tagaft" and x == t[i + 0][0] and y == t[i + 2][1])
627: (20)                               or (cmd == "lbigram" and x == t[i - 1][0] and y == t[i + 0][0])
628: (20)                               or (cmd == "rbigram" and x == t[i + 0][0] and y == t[i + 1][0])
629: (20)                               or (cmd == "prevbigram" and x == t[i - 2][1] and y == t[i - 1][1])
630: (20)                               or (cmd == "nextbigram" and x == t[i + 1][1] and y == t[i + 2][1])
631: (16)                           ):
632: (20)                               t[i] = [t[i][0], r[1]]
633: (8)                    return t[len(o) : -len(o)]
634: (4)                def insert(self, i, tag1, tag2, cmd="prevtag", x=None, y=None):
635: (8)                    """Inserts a new rule that updates words with tag1 to tag2,
636: (8)                    given constraints x and y, e.g., Context.append("TO < NN", "VB")
637: (8)                    """
638: (8)                    if " < " in tag1 and not x and not y:
639: (12)                       tag1, x = tag1.split(" < ")
640: (12)                       cmd = "prevtag"
641: (8)                    if " > " in tag1 and not x and not y:
642: (12)                       x, tag1 = tag1.split(" > ")
643: (12)                       cmd = "nexttag"
644: (8)                    lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])
645: (4)                def append(self, *args, **kwargs):
646: (8)                    self.insert(len(self) - 1, *args, **kwargs)
647: (4)                def extend(self, rules=None):
648: (8)                    if rules is None:
649: (12)                       rules = []
650: (8)                    for r in rules:
651: (12)                       self.append(*r)
652: (0)            # --- NAMED ENTITY RECOGNIZER -----------------------------------------------------------------------
653: (0)            RE_ENTITY1 = re.compile(r"^http://")  # http://www.domain.com/path
654: (0)            RE_ENTITY2 = re.compile(r"^www\..*?\.[com|org|net|edu|de|uk]$")  # www.domain.com
655: (0)            RE_ENTITY3 = re.compile(r"^[\w\-\.\+]+@(\w[\w\-]+\.)+[\w\-]+$")  # name@domain.com
656: (0)            class Entities(lazydict, Rules):
657: (4)                def __init__(self, lexicon=None, path="", tag="NNP"):
658: (8)                    """A dictionary of named entities and their labels.
659: (8)                    For domain names and e-mail adresses, regular expressions are used.
660: (8)                    """
661: (8)                    if lexicon is None:
662: (12)                       lexicon = {}
663: (8)                    cmd = (
664: (12)                       "pers",  # Persons: George/NNP-PERS
665: (12)                       "loc",  # Locations: Washington/NNP-LOC
666: (12)                       "org",  # Organizations: Google/NNP-ORG
667: (8)                    )
668: (8)                    Rules.__init__(self, lexicon, cmd)
669: (8)                    self._path = path
670: (8)                    self.tag = tag
671: (4)                @property
672: (4)                def path(self):
673: (8)                    return self._path
674: (4)                def load(self):
675: (8)                    # ["Alexander", "the", "Great", "PERS"]
676: (8)                    # {"alexander": [["alexander", "the", "great", "pers"], ...]}
677: (8)                    for x in _read(self.path):
678: (12)                       x = [x.lower() for x in x.split()]
679: (12)                       dict.setdefault(self, x[0], []).append(x)
680: (4)                def apply(self, tokens):
681: (8)                    """Applies the named entity recognizer to the given list of tokens,
682: (8)                    where each token is a [word, tag] list.
683: (8)                    """
684: (8)                    # Note: we could also scan for patterns, e.g.,
685: (8)                    # "my|his|her name is|was *" => NNP-PERS.
686: (8)                    i = 0
687: (8)                    while i < len(tokens):
688: (12)                       w = tokens[i][0].lower()
689: (12)                       if RE_ENTITY1.match(w) or RE_ENTITY2.match(w) or RE_ENTITY3.match(w):
690: (16)                           tokens[i][1] = self.tag
691: (12)                       if w in self:
692: (16)                           for e in self[w]:
693: (20)                               # Look ahead to see if successive words match the named entity.
694: (20)                               e, tag = (
695: (24)                                   (e[:-1], "-" + e[-1].upper()) if e[-1] in self.cmd else (e, "")
696: (20)                               )
697: (20)                               b = True
698: (20)                               for j, e in enumerate(e):
699: (24)                                   if i + j >= len(tokens) or tokens[i + j][0].lower() != e:
700: (28)                                       b = False
701: (28)                                       break
702: (20)                               if b:
703: (24)                                   for token in tokens[i : i + j + 1]:
704: (28)                                       token[1] = (
705: (32)                                           token[1] == "NNPS" and token[1] or self.tag
706: (28)                                       ) + tag
707: (24)                                   i += j
708: (24)                                   break
709: (12)                       i += 1
710: (8)                    return tokens
711: (4)                def append(self, entity, name="pers"):
712: (8)                    """Appends a named entity to the lexicon,
713: (8)                    e.g., Entities.append("Hooloovoo", "PERS")
714: (8)                    """
715: (8)                    e = [s.lower() for s in entity.split(" ") + [name]]
716: (8)                    self.setdefault(e[0], []).append(e)
717: (4)                def extend(self, entities):
718: (8)                    for entity, name in entities:
719: (12)                       self.append(entity, name)
720: (0)            ### SENTIMENT POLARITY LEXICON #####################################################################
721: (0)            # A sentiment lexicon can be used to discern objective facts from subjective opinions in text.
722: (0)            # Each word in the lexicon has scores for:
723: (0)            # 1)     polarity: negative vs. positive    (-1.0 => +1.0)
724: (0)            # 2) subjectivity: objective vs. subjective (+0.0 => +1.0)
725: (0)            # 3)    intensity: modifies next word?      (x0.5 => x2.0)
726: (0)            # For English, adverbs are used as modifiers (e.g., "very good").
727: (0)            # For Dutch, adverbial adjectives are used as modifiers
728: (0)            # ("hopeloos voorspelbaar", "ontzettend spannend", "verschrikkelijk goed").
729: (0)            # Negation words (e.g., "not") reverse the polarity of the following word.
730: (0)            # Sentiment()(txt) returns an averaged (polarity, subjectivity)-tuple.
731: (0)            # Sentiment().assessments(txt) returns a list of (chunk, polarity, subjectivity, label)-tuples.
732: (0)            # Semantic labels are useful for fine-grained analysis, e.g.,
733: (0)            # negative words + positive emoticons could indicate cynicism.
734: (0)            # Semantic labels:
735: (0)            MOOD = "mood"  # emoticons, emojis
736: (0)            IRONY = "irony"  # sarcasm mark (!)
737: (0)            NOUN, VERB, ADJECTIVE, ADVERB = "NN", "VB", "JJ", "RB"
738: (0)            RE_SYNSET = re.compile(r"^[acdnrv][-_][0-9]+$")
739: (0)            def avg(list):
740: (4)                return sum(list) / float(len(list) or 1)
741: (0)            class Score(tuple):
742: (4)                def __new__(self, polarity, subjectivity, assessments=None):
743: (8)                    """A (polarity, subjectivity)-tuple with an assessments property."""
744: (8)                    if assessments is None:
745: (12)                       assessments = []
746: (8)                    return tuple.__new__(self, [polarity, subjectivity])
747: (4)                def __init__(self, polarity, subjectivity, assessments=None):
748: (8)                    if assessments is None:
749: (12)                       assessments = []
750: (8)                    self.assessments = assessments
751: (0)            class Sentiment(lazydict):
752: (4)                def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
753: (8)                    """A dictionary of words (adjectives) and polarity scores (positive/negative).
754: (8)                    The value for each word is a dictionary of part-of-speech tags.
755: (8)                    The value for each word POS-tag is a tuple with values for
756: (8)                    polarity (-1.0-1.0), subjectivity (0.0-1.0) and intensity (0.5-2.0).
757: (8)                    """
758: (8)                    self._path = path  # XML file path.
759: (8)                    self._language = None  # XML language attribute ("en", "fr", ...)
760: (8)                    self._confidence = None  # XML confidence attribute threshold (>=).
761: (8)                    self._synset = synset  # XML synset attribute ("wordnet_id", "cornetto_id", ...)
762: (8)                    self._synsets = {}  # {"a-01123879": (1.0, 1.0, 1.0)}
763: (8)                    self.labeler = {}  # {"dammit": "profanity"}
764: (8)                    self.tokenizer = kwargs.get("tokenizer", find_tokens)
765: (8)                    self.negations = kwargs.get("negations", ("no", "not", "n't", "never"))
766: (8)                    self.modifiers = kwargs.get("modifiers", ("RB",))
767: (8)                    self.modifier = kwargs.get("modifier", lambda w: w.endswith("ly"))
768: (4)                @property
769: (4)                def path(self):
770: (8)                    return self._path
771: (4)                @property
772: (4)                def language(self):
773: (8)                    return self._language
774: (4)                @property
775: (4)                def confidence(self):
776: (8)                    return self._confidence
777: (4)                def load(self, path=None):
778: (8)                    """Loads the XML-file (with sentiment annotations) from the given path.
779: (8)                    By default, Sentiment.path is lazily loaded.
780: (8)                    """
781: (8)                    # <word form="great" wordnet_id="a-01123879" pos="JJ" polarity="1.0" subjectivity="1.0" intensity="1.0" />
782: (8)                    # <word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" />
783: (8)                    if not path:
784: (12)                       path = self._path
785: (8)                    if not os.path.exists(path):
786: (12)                       return
787: (8)                    words, synsets, labels = {}, {}, {}
788: (8)                    xml = ElementTree.parse(path)
789: (8)                    xml = xml.getroot()
790: (8)                    for w in xml.findall("word"):
791: (12)                       if self._confidence is None or self._confidence <= float(
792: (16)                           w.attrib.get("confidence", 0.0)
793: (12)                       ):
794: (16)                           w, pos, p, s, i, label, synset = (
795: (20)                               w.attrib.get("form"),
796: (20)                               w.attrib.get("pos"),
797: (20)                               w.attrib.get("polarity", 0.0),
798: (20)                               w.attrib.get("subjectivity", 0.0),
799: (20)                               w.attrib.get("intensity", 1.0),
800: (20)                               w.attrib.get("label"),
801: (20)                               w.attrib.get(self._synset),  # wordnet_id, cornetto_id, ...
802: (16)                           )
803: (16)                           psi = (float(p), float(s), float(i))
804: (16)                           if w:
805: (20)                               words.setdefault(w, {}).setdefault(pos, []).append(psi)
806: (16)                           if w and label:
807: (20)                               labels[w] = label
808: (16)                           if synset:
809: (20)                               synsets.setdefault(synset, []).append(psi)
810: (8)                    self._language = xml.attrib.get("language", self._language)
811: (8)                    # Average scores of all word senses per part-of-speech tag.
812: (8)                    for w in words:
813: (12)                       words[w] = dict(
814: (16)                           (pos, [avg(each) for each in zip(*psi)])
815: (16)                           for pos, psi in words[w].items()
816: (12)                       )
817: (8)                    # Average scores of all part-of-speech tags.
818: (8)                    for w, pos in list(words.items()):
819: (12)                       words[w][None] = [avg(each) for each in zip(*pos.values())]
820: (8)                    # Average scores of all synonyms per synset.
821: (8)                    for id, psi in synsets.items():
822: (12)                       synsets[id] = [avg(each) for each in zip(*psi)]
823: (8)                    dict.update(self, words)
824: (8)                    dict.update(self.labeler, labels)
825: (8)                    dict.update(self._synsets, synsets)
826: (4)                def synset(self, id, pos=ADJECTIVE):
827: (8)                    """Returns a (polarity, subjectivity)-tuple for the given synset id.
828: (8)                    For example, the adjective "horrible" has id 193480 in WordNet:
829: (8)                    Sentiment.synset(193480, pos="JJ") => (-0.6, 1.0, 1.0).
830: (8)                    """
831: (8)                    id = str(id).zfill(8)
832: (8)                    if not id.startswith(("n-", "v-", "a-", "r-")):
833: (12)                       if pos == NOUN:
834: (16)                           id = "n-" + id
835: (12)                       if pos == VERB:
836: (16)                           id = "v-" + id
837: (12)                       if pos == ADJECTIVE:
838: (16)                           id = "a-" + id
839: (12)                       if pos == ADVERB:
840: (16)                           id = "r-" + id
841: (8)                    if dict.__len__(self) == 0:
842: (12)                       self.load()
843: (8)                    return tuple(self._synsets.get(id, (0.0, 0.0))[:2])
844: (4)                def __call__(self, s, negation=True, **kwargs):
845: (8)                    """Returns a (polarity, subjectivity)-tuple for the given sentence,
846: (8)                    with polarity between -1.0 and 1.0 and subjectivity between 0.0 and 1.0.
847: (8)                    The sentence can be a string, Synset, Text, Sentence, Chunk, Word, Document, Vector.
848: (8)                    An optional weight parameter can be given,
849: (8)                    as a function that takes a list of words and returns a weight.
850: (8)                    """
851: (8)                    def avg(assessments, weighted=lambda w: 1):
852: (12)                       s, n = 0, 0
853: (12)                       for words, score in assessments:
854: (16)                           w = weighted(words)
855: (16)                           s += w * score
856: (16)                           n += w
857: (12)                       return s / float(n or 1)
858: (8)                    # A pattern.en.wordnet.Synset.
859: (8)                    # Sentiment(synsets("horrible", "JJ")[0]) => (-0.6, 1.0)
860: (8)                    if hasattr(s, "gloss"):
861: (12)                       a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)]
862: (8)                    # A synset id.
863: (8)                    # Sentiment("a-00193480") => horrible => (-0.6, 1.0)   (English WordNet)
864: (8)                    # Sentiment("c_267") => verschrikkelijk => (-0.9, 1.0) (Dutch Cornetto)
865: (8)                    elif (
866: (12)                       isinstance(s, basestring) and RE_SYNSET.match(s) and hasattr(s, "synonyms")
867: (8)                    ):
868: (12)                       a = [(s.synonyms[0],) + self.synset(s.id, pos=s.pos) + (None,)]
869: (8)                    # A string of words.
870: (8)                    # Sentiment("a horrible movie") => (-0.6, 1.0)
871: (8)                    elif isinstance(s, basestring):
872: (12)                       a = self.assessments(
873: (16)                           ((w.lower(), None) for w in " ".join(self.tokenizer(s)).split()),
874: (16)                           negation,
875: (12)                       )
876: (8)                    # A pattern.en.Text.
877: (8)                    elif hasattr(s, "sentences"):
878: (12)                       a = self.assessments(
879: (16)                           (
880: (20)                               (w.lemma or w.string.lower(), w.pos[:2])
881: (20)                               for w in chain.from_iterable(s)
882: (16)                           ),
883: (16)                           negation,
884: (12)                       )
885: (8)                    # A pattern.en.Sentence or pattern.en.Chunk.
886: (8)                    elif hasattr(s, "lemmata"):
887: (12)                       a = self.assessments(
888: (16)                           ((w.lemma or w.string.lower(), w.pos[:2]) for w in s.words), negation
889: (12)                       )
890: (8)                    # A pattern.en.Word.
891: (8)                    elif hasattr(s, "lemma"):
892: (12)                       a = self.assessments(((s.lemma or s.string.lower(), s.pos[:2]),), negation)
893: (8)                    # A pattern.vector.Document.
894: (8)                    # Average score = weighted average using feature weights.
895: (8)                    # Bag-of words is unordered: inject None between each two words
896: (8)                    # to stop assessments() from scanning for preceding negation & modifiers.
897: (8)                    elif hasattr(s, "terms"):
898: (12)                       a = self.assessments(
899: (16)                           chain.from_iterable(((w, None), (None, None)) for w in s), negation
900: (12)                       )
901: (12)                       kwargs.setdefault("weight", lambda w: s.terms[w[0]])
902: (8)                    # A dict of (word, weight)-items.
903: (8)                    elif isinstance(s, dict):
904: (12)                       a = self.assessments(
905: (16)                           chain.from_iterable(((w, None), (None, None)) for w in s), negation
906: (12)                       )
907: (12)                       kwargs.setdefault("weight", lambda w: s[w[0]])
908: (8)                    # A list of words.
909: (8)                    elif isinstance(s, list):
910: (12)                       a = self.assessments(((w, None) for w in s), negation)
911: (8)                    else:
912: (12)                       a = []
913: (8)                    weight = kwargs.get("weight", lambda w: 1)  # [(w, p) for w, p, s, x in a]
914: (8)                    return Score(
915: (12)                       polarity=avg([(w, p) for w, p, s, x in a], weight),
916: (12)                       subjectivity=avg([(w, s) for w, p, s, x in a], weight),
917: (12)                       assessments=a,
918: (8)                    )
919: (4)                def assessments(self, words=None, negation=True):
920: (8)                    """Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
921: (8)                    where chunk is a list of successive words: a known word optionally
922: (8)                    preceded by a modifier ("very good") or a negation ("not good").
923: (8)                    """
924: (8)                    if words is None:
925: (12)                       words = []
926: (8)                    a = []
927: (8)                    m = None  # Preceding modifier (i.e., adverb or adjective).
928: (8)                    n = None  # Preceding negation (e.g., "not beautiful").
929: (8)                    for w, pos in words:
930: (12)                       # Only assess known words, preferably by part-of-speech tag.
931: (12)                       # Including unknown words (polarity 0.0 and subjectivity 0.0) lowers the average.
932: (12)                       if w is None:
933: (16)                           continue
934: (12)                       if w in self and pos in self[w]:
935: (16)                           p, s, i = self[w][pos]
936: (16)                           # Known word not preceded by a modifier ("good").
937: (16)                           if m is None:
938: (20)                               a.append(dict(w=[w], p=p, s=s, i=i, n=1, x=self.labeler.get(w)))
939: (16)                           # Known word preceded by a modifier ("really good").
940: (16)                           if m is not None:
941: (20)                               a[-1]["w"].append(w)
942: (20)                               a[-1]["p"] = max(-1.0, min(p * a[-1]["i"], +1.0))
943: (20)                               a[-1]["s"] = max(-1.0, min(s * a[-1]["i"], +1.0))
944: (20)                               a[-1]["i"] = i
945: (20)                               a[-1]["x"] = self.labeler.get(w)
946: (16)                           # Known word preceded by a negation ("not really good").
947: (16)                           if n is not None:
948: (20)                               a[-1]["w"].insert(0, n)
949: (20)                               a[-1]["i"] = 1.0 / a[-1]["i"]
950: (20)                               a[-1]["n"] = -1
951: (16)                           # Known word may be a negation.
952: (16)                           # Known word may be modifying the next word (i.e., it is a known adverb).
953: (16)                           m = None
954: (16)                           n = None
955: (16)                           if (
956: (20)                               pos
957: (20)                               and pos in self.modifiers
958: (20)                               or any(map(self[w].__contains__, self.modifiers))
959: (16)                           ):
960: (20)                               m = (w, pos)
961: (16)                           if negation and w in self.negations:
962: (20)                               n = w
963: (12)                       else:
964: (16)                           # Unknown word may be a negation ("not good").
965: (16)                           if negation and w in self.negations:
966: (20)                               n = w
967: (16)                           # Unknown word. Retain negation across small words ("not a good").
968: (16)                           elif n and len(w.strip("'")) > 1:
969: (20)                               n = None
970: (16)                           # Unknown word may be a negation preceded by a modifier ("really not good").
971: (16)                           if (
972: (20)                               n is not None
973: (20)                               and m is not None
974: (20)                               and (pos in self.modifiers or self.modifier(m[0]))
975: (16)                           ):
976: (20)                               a[-1]["w"].append(n)
977: (20)                               a[-1]["n"] = -1
978: (20)                               n = None
979: (16)                           # Unknown word. Retain modifier across small words ("really is a good").
980: (16)                           elif m and len(w) > 2:
981: (20)                               m = None
982: (16)                           # Exclamation marks boost previous word.
983: (16)                           if w == "!" and len(a) > 0:
984: (20)                               a[-1]["w"].append("!")
985: (20)                               a[-1]["p"] = max(-1.0, min(a[-1]["p"] * 1.25, +1.0))
986: (16)                           # Exclamation marks in parentheses indicate sarcasm.
987: (16)                           if w == "(!)":
988: (20)                               a.append(dict(w=[w], p=0.0, s=1.0, i=1.0, n=1, x=IRONY))
989: (16)                           # EMOTICONS: {("grin", +1.0): set((":-D", ":D"))}
990: (16)                           if (
991: (20)                               w.isalpha() is False and len(w) <= 5 and w not in PUNCTUATION
992: (16)                           ):  # speedup
993: (20)                               for (_type, p), e in EMOTICONS.items():
994: (24)                                   if w in map(lambda e: e.lower(), e):
995: (28)                                       a.append(dict(w=[w], p=p, s=1.0, i=1.0, n=1, x=MOOD))
996: (28)                                       break
997: (8)                    for i in range(len(a)):
998: (12)                       w = a[i]["w"]
999: (12)                       p = a[i]["p"]
1000: (12)                      s = a[i]["s"]
1001: (12)                      n = a[i]["n"]
1002: (12)                      x = a[i]["x"]
1003: (12)                      # "not good" = slightly bad, "not bad" = slightly good.
1004: (12)                      a[i] = (w, p * -0.5 if n < 0 else p, s, x)
1005: (8)                   return a
1006: (4)               def annotate(
1007: (8)                   self, word, pos=None, polarity=0.0, subjectivity=0.0, intensity=1.0, label=None
1008: (4)               ):
1009: (8)                   """Annotates the given word with polarity, subjectivity and intensity scores,
1010: (8)                   and optionally a semantic label (e.g., MOOD for emoticons, IRONY for "(!)").
1011: (8)                   """
1012: (8)                   w = self.setdefault(word, {})
1013: (8)                   w[pos] = w[None] = (polarity, subjectivity, intensity)
1014: (8)                   if label:
1015: (12)                      self.labeler[word] = label
1016: (0)           # --- PART-OF-SPEECH TAGGER -------------------------------------------------------------------------
1017: (0)           # Unknown words are recognized as numbers if they contain only digits and -,.:/%$
1018: (0)           CD = re.compile(r"^[0-9\-\,\.\:\/\%\$]+$")
1019: (0)           def _suffix_rules(token, tag="NN"):
1020: (4)               """Default morphological tagging rules for English, based on word suffixes."""
1021: (4)               if isinstance(token, (list, tuple)):
1022: (8)                   token, tag = token
1023: (4)               if token.endswith("ing"):
1024: (8)                   tag = "VBG"
1025: (4)               if token.endswith("ly"):
1026: (8)                   tag = "RB"
1027: (4)               if token.endswith("s") and not token.endswith(("is", "ous", "ss")):
1028: (8)                   tag = "NNS"
1029: (4)               if (
1030: (8)                   token.endswith(
1031: (12)                      ("able", "al", "ful", "ible", "ient", "ish", "ive", "less", "tic", "ous")
1032: (8)                   )
1033: (8)                   or "-" in token
1034: (4)               ):
1035: (8)                   tag = "JJ"
1036: (4)               if token.endswith("ed"):
1037: (8)                   tag = "VBN"
1038: (4)               if token.endswith(("ate", "ify", "ise", "ize")):
1039: (8)                   tag = "VBP"
1040: (4)               return [token, tag]
1041: (0)           def find_tags(
1042: (4)               tokens,
1043: (4)               lexicon=None,
1044: (4)               model=None,
1045: (4)               morphology=None,
1046: (4)               context=None,
1047: (4)               entities=None,
1048: (4)               default=("NN", "NNP", "CD"),
1049: (4)               language="en",
1050: (4)               map=None,
1051: (4)               **kwargs,
1052: (0)           ):
1053: (4)               """Returns a list of [token, tag]-items for the given list of tokens:
1054: (4)               ["The", "cat", "purs"] => [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
1055: (4)               Words are tagged using the given lexicon of (word, tag)-items.
1056: (4)               Unknown words are tagged NN by default.
1057: (4)               Unknown words that start with a capital letter are tagged NNP (unless language="de").
1058: (4)               Unknown words that consist only of digits and punctuation marks are tagged CD.
1059: (4)               Unknown words are then improved with morphological rules.
1060: (4)               All words are improved with contextual rules.
1061: (4)               If a model is given, uses model for unknown words instead of morphology and context.
1062: (4)               If map is a function, it is applied to each (token, tag) after applying all rules.
1063: (4)               """
1064: (4)               if lexicon is None:
1065: (8)                   lexicon = {}
1066: (4)               tagged = []
1067: (4)               # Tag known words.
1068: (4)               for i, token in enumerate(tokens):
1069: (8)                   tagged.append(
1070: (12)                      [token, lexicon.get(token, i == 0 and lexicon.get(token.lower()) or None)]
1071: (8)                   )
1072: (4)               # Tag unknown words.
1073: (4)               for i, (token, tag) in enumerate(tagged):
1074: (8)                   prev, next = (None, None), (None, None)
1075: (8)                   if i > 0:
1076: (12)                      prev = tagged[i - 1]
1077: (8)                   if i < len(tagged) - 1:
1078: (12)                      next = tagged[i + 1]
1079: (8)                   if tag is None or token in (model is not None and model.unknown or ()):
1080: (12)                      # Use language model (i.e., SLP).
1081: (12)                      if model is not None:
1082: (16)                          tagged[i] = model.apply([token, None], prev, next)
1083: (12)                      # Use NNP for capitalized words (except in German).
1084: (12)                      elif token.istitle() and language != "de":
1085: (16)                          tagged[i] = [token, default[1]]
1086: (12)                      # Use CD for digits and numbers.
1087: (12)                      elif CD.match(token) is not None:
1088: (16)                          tagged[i] = [token, default[2]]
1089: (12)                      # Use suffix rules (e.g., -ly = RB).
1090: (12)                      elif morphology is not None:
1091: (16)                          tagged[i] = morphology.apply([token, default[0]], prev, next)
1092: (12)                      # Use suffix rules (English default).
1093: (12)                      elif language == "en":
1094: (16)                          tagged[i] = _suffix_rules([token, default[0]])
1095: (12)                      # Use most frequent tag (NN).
1096: (12)                      else:
1097: (16)                          tagged[i] = [token, default[0]]
1098: (4)               # Tag words by context.
1099: (4)               if context is not None and model is None:
1100: (8)                   tagged = context.apply(tagged)
1101: (4)               # Tag named entities.
1102: (4)               if entities is not None:
1103: (8)                   tagged = entities.apply(tagged)
1104: (4)               # Map tags with a custom function.
1105: (4)               if map is not None:
1106: (8)                   tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
1107: (4)               return tagged
1108: (0)           # --- PHRASE CHUNKER --------------------------------------------------------------------------------
1109: (0)           SEPARATOR = "/"
1110: (0)           NN = r"NN|NNS|NNP|NNPS|NNPS?\-[A-Z]{3,4}|PR|PRP|PRP\$"
1111: (0)           VB = r"VB|VBD|VBG|VBN|VBP|VBZ"
1112: (0)           JJ = r"JJ|JJR|JJS"
1113: (0)           RB = r"(?<!W)RB|RBR|RBS"
1114: (0)           # Chunking rules.
1115: (0)           # CHUNKS[0] = Germanic: RB + JJ precedes NN ("the round table").
1116: (0)           # CHUNKS[1] = Romance: RB + JJ precedes or follows NN ("la table ronde", "une jolie fille").
1117: (0)           CHUNKS = [
1118: (4)               [
1119: (8)                   # Germanic languages: en, de, nl, ...
1120: (8)                   (
1121: (12)                      "NP",
1122: (12)                      re.compile(
1123: (16)                          r"(("
1124: (16)                          + NN
1125: (16)                          + ")/)*((DT|CD|CC|CJ)/)*(("
1126: (16)                          + RB
1127: (16)                          + "|"
1128: (16)                          + JJ
1129: (16)                          + ")/)*(("
1130: (16)                          + NN
1131: (16)                          + ")/)+"
1132: (12)                      ),
1133: (8)                   ),
1134: (8)                   ("VP", re.compile(r"(((MD|" + RB + ")/)*((" + VB + ")/)+)+")),
1135: (8)                   ("VP", re.compile(r"((MD)/)")),
1136: (8)                   ("PP", re.compile(r"((IN|PP|TO)/)+")),
1137: (8)                   ("ADJP", re.compile(r"((CC|CJ|" + RB + "|" + JJ + ")/)*((" + JJ + ")/)+")),
1138: (8)                   ("ADVP", re.compile(r"((" + RB + "|WRB)/)+")),
1139: (4)               ],
1140: (4)               [
1141: (8)                   # Romance languages: es, fr, it, ...
1142: (8)                   (
1143: (12)                      "NP",
1144: (12)                      re.compile(
1145: (16)                          r"(("
1146: (16)                          + NN
1147: (16)                          + ")/)*((DT|CD|CC|CJ)/)*(("
1148: (16)                          + RB
1149: (16)                          + "|"
1150: (16)                          + JJ
1151: (16)                          + ")/)*(("
1152: (16)                          + NN
1153: (16)                          + ")/)+(("
1154: (16)                          + RB
1155: (16)                          + "|"
1156: (16)                          + JJ
1157: (16)                          + ")/)*"
1158: (12)                      ),
1159: (8)                   ),
1160: (8)                   ("VP", re.compile(r"(((MD|" + RB + ")/)*((" + VB + ")/)+((" + RB + ")/)*)+")),
1161: (8)                   ("VP", re.compile(r"((MD)/)")),
1162: (8)                   ("PP", re.compile(r"((IN|PP|TO)/)+")),
1163: (8)                   ("ADJP", re.compile(r"((CC|CJ|" + RB + "|" + JJ + ")/)*((" + JJ + ")/)+")),
1164: (8)                   ("ADVP", re.compile(r"((" + RB + "|WRB)/)+")),
1165: (4)               ],
1166: (0)           ]
1167: (0)           # Handle ADJP before VP, so that
1168: (0)           # RB prefers next ADJP over previous VP.
1169: (0)           CHUNKS[0].insert(1, CHUNKS[0].pop(3))
1170: (0)           CHUNKS[1].insert(1, CHUNKS[1].pop(3))
1171: (0)           def find_chunks(tagged, language="en"):
1172: (4)               """The input is a list of [token, tag]-items.
1173: (4)               The output is a list of [token, tag, chunk]-items:
1174: (4)               The/DT nice/JJ fish/NN is/VBZ dead/JJ ./. =>
1175: (4)               The/DT/B-NP nice/JJ/I-NP fish/NN/I-NP is/VBZ/B-VP dead/JJ/B-ADJP ././O
1176: (4)               """
1177: (4)               chunked = [x for x in tagged]
1178: (4)               tags = "".join(f"{tag}{SEPARATOR}" for token, tag in tagged)
1179: (4)               # Use Germanic or Romance chunking rules according to given language.
1180: (4)               for tag, rule in CHUNKS[
1181: (8)                   int(language in ("ca", "es", "pt", "fr", "it", "pt", "ro"))
1182: (4)               ]:
1183: (8)                   for m in rule.finditer(tags):
1184: (12)                      # Find the start of chunks inside the tags-string.
1185: (12)                      # Number of preceding separators = number of preceding tokens.
1186: (12)                      i = m.start()
1187: (12)                      j = tags[:i].count(SEPARATOR)
1188: (12)                      n = m.group(0).count(SEPARATOR)
1189: (12)                      for k in range(j, j + n):
1190: (16)                          if len(chunked[k]) == 3:
1191: (20)                              continue
1192: (16)                          if len(chunked[k]) < 3:
1193: (20)                              # A conjunction can not be start of a chunk.
1194: (20)                              if k == j and chunked[k][1] in ("CC", "CJ", "KON", "Conj(neven)"):
1195: (24)                                  j += 1
1196: (20)                              # Mark first token in chunk with B-.
1197: (20)                              elif k == j:
1198: (24)                                  chunked[k].append("B-" + tag)
1199: (20)                              # Mark other tokens in chunk with I-.
1200: (20)                              else:
1201: (24)                                  chunked[k].append("I-" + tag)
1202: (4)               # Mark chinks (tokens outside of a chunk) with O-.
1203: (4)               for chink in filter(lambda x: len(x) < 3, chunked):
1204: (8)                   chink.append("O")
1205: (4)               # Post-processing corrections.
1206: (4)               for i, (_word, tag, chunk) in enumerate(chunked):
1207: (8)                   if tag.startswith("RB") and chunk == "B-NP":
1208: (12)                      # "Very nice work" (NP) <=> "Perhaps" (ADVP) + "you" (NP).
1209: (12)                      if i < len(chunked) - 1 and not chunked[i + 1][1].startswith("JJ"):
1210: (16)                          chunked[i + 0][2] = "B-ADVP"
1211: (16)                          chunked[i + 1][2] = "B-NP"
1212: (4)               return chunked
1213: (0)           def find_prepositions(chunked):
1214: (4)               """The input is a list of [token, tag, chunk]-items.
1215: (4)               The output is a list of [token, tag, chunk, preposition]-items.
1216: (4)               PP-chunks followed by NP-chunks make up a PNP-chunk.
1217: (4)               """
1218: (4)               # Tokens that are not part of a preposition just get the O-tag.
1219: (4)               for ch in chunked:
1220: (8)                   ch.append("O")
1221: (4)               for i, chunk in enumerate(chunked):
1222: (8)                   if chunk[2].endswith("PP") and chunk[-1] == "O":
1223: (12)                      # Find PP followed by other PP, NP with nouns and pronouns, VP with a gerund.
1224: (12)                      if i < len(chunked) - 1 and (
1225: (16)                          chunked[i + 1][2].endswith(("NP", "PP"))
1226: (16)                          or chunked[i + 1][1] in ("VBG", "VBN")
1227: (12)                      ):
1228: (16)                          chunk[-1] = "B-PNP"
1229: (16)                          pp = True
1230: (16)                          for ch in chunked[i + 1 :]:
1231: (20)                              if not (ch[2].endswith(("NP", "PP")) or ch[1] in ("VBG", "VBN")):
1232: (24)                                  break
1233: (20)                              if ch[2].endswith("PP") and pp:
1234: (24)                                  ch[-1] = "I-PNP"
1235: (20)                              if not ch[2].endswith("PP"):
1236: (24)                                  ch[-1] = "I-PNP"
1237: (24)                                  pp = False
1238: (4)               return chunked
1239: (0)           #### PARSER ########################################################################################
1240: (0)           # --- PARSER ----------------------------------------------------------------------------------------
1241: (0)           # A shallow parser can be used to retrieve syntactic-semantic information from text
1242: (0)           # in an efficient way (usually at the expense of deeper configurational syntactic information).
1243: (0)           # The shallow parser in Pattern is meant to handle the following tasks:
1244: (0)           # 1)  Tokenization: split punctuation marks from words and find sentence periods.
1245: (0)           # 2)       Tagging: find the part-of-speech tag of each word (noun, verb, ...) in a sentence.
1246: (0)           # 3)      Chunking: find words that belong together in a phrase.
1247: (0)           # 4) Role labeling: find the subject and object of the sentence.
1248: (0)           # 5) Lemmatization: find the base form of each word ("was" => "is").
1249: (0)           #    WORD     TAG     CHUNK      PNP        ROLE        LEMMA
1250: (0)           # ------------------------------------------------------------------
1251: (0)           #     The      DT      B-NP        O        NP-SBJ-1      the
1252: (0)           #   black      JJ      I-NP        O        NP-SBJ-1      black
1253: (0)           #     cat      NN      I-NP        O        NP-SBJ-1      cat
1254: (0)           #     sat      VB      B-VP        O        VP-1          sit
1255: (0)           #      on      IN      B-PP      B-PNP      PP-LOC        on
1256: (0)           #     the      DT      B-NP      I-PNP      NP-OBJ-1      the
1257: (0)           #     mat      NN      I-NP      I-PNP      NP-OBJ-1      mat
1258: (0)           #       .      .        O          O          O           .
1259: (0)           # The example demonstrates what information can be retrieved:
1260: (0)           #
1261: (0)           # - the period is split from "mat." = the end of the sentence,
1262: (0)           # - the words are annotated: NN (noun), VB (verb), JJ (adjective), DT (determiner), ...
1263: (0)           # - the phrases are annotated: NP (noun phrase), VP (verb phrase), PNP (preposition), ...
1264: (0)           # - the phrases are labeled: SBJ (subject), OBJ (object), LOC (location), ...
1265: (0)           # - the phrase start is marked: B (begin), I (inside), O (outside),
1266: (0)           # - the past tense "sat" is lemmatized => "sit".
1267: (0)           # By default, the English parser uses the Penn Treebank II tagset:
1268: (0)           # http://www.clips.ua.ac.be/pages/penn-treebank-tagset
1269: (0)           PTB = PENN = "penn"
1270: (0)           class Parser:
1271: (4)               def __init__(self, lexicon=None, default=("NN", "NNP", "CD"), language=None):
1272: (8)                   """A simple shallow parser using a Brill-based part-of-speech tagger.
1273: (8)                   The given lexicon is a dictionary of known words and their part-of-speech tag.
1274: (8)                   The given default tags are used for unknown words.
1275: (8)                   Unknown words that start with a capital letter are tagged NNP (except for German).
1276: (8)                   Unknown words that contain only digits and punctuation are tagged CD.
1277: (8)                   The given language can be used to discern between
1278: (8)                   Germanic and Romance languages for phrase chunking.
1279: (8)                   """
1280: (8)                   if lexicon is None:
1281: (12)                      lexicon = {}
1282: (8)                   self.lexicon = lexicon
1283: (8)                   self.default = default
1284: (8)                   self.language = language
1285: (4)               def find_tokens(self, string, **kwargs):
1286: (8)                   """Returns a list of sentences from the given string.
1287: (8)                   Punctuation marks are separated from each word by a space.
1288: (8)                   """
1289: (8)                   # "The cat purs." => ["The cat purs ."]
1290: (8)                   return find_tokens(
1291: (12)                      str(string),
1292: (12)                      punctuation=kwargs.get("punctuation", PUNCTUATION),
1293: (12)                      abbreviations=kwargs.get("abbreviations", ABBREVIATIONS),
1294: (12)                      replace=kwargs.get("replace", replacements),
1295: (12)                      linebreak=r"\n{2,}",
1296: (8)                   )
1297: (4)               def find_tags(self, tokens, **kwargs):
1298: (8)                   """Annotates the given list of tokens with part-of-speech tags.
1299: (8)                   Returns a list of tokens, where each token is now a [word, tag]-list.
1300: (8)                   """
1301: (8)                   # ["The", "cat", "purs"] => [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
1302: (8)                   return find_tags(
1303: (12)                      tokens,
1304: (12)                      language=kwargs.get("language", self.language),
1305: (12)                      lexicon=kwargs.get("lexicon", self.lexicon),
1306: (12)                      default=kwargs.get("default", self.default),
1307: (12)                      map=kwargs.get("map", None),
1308: (8)                   )
1309: (4)               def find_chunks(self, tokens, **kwargs):
1310: (8)                   """Annotates the given list of tokens with chunk tags.
1311: (8)                   Several tags can be added, for example chunk + preposition tags.
1312: (8)                   """
1313: (8)                   # [["The", "DT"], ["cat", "NN"], ["purs", "VB"]] =>
1314: (8)                   # [["The", "DT", "B-NP"], ["cat", "NN", "I-NP"], ["purs", "VB", "B-VP"]]
1315: (8)                   return find_prepositions(
1316: (12)                      find_chunks(tokens, language=kwargs.get("language", self.language))
1317: (8)                   )
1318: (4)               def find_prepositions(self, tokens, **kwargs):
1319: (8)                   """Annotates the given list of tokens with prepositional noun phrase tags."""
1320: (8)                   return find_prepositions(tokens)  # See also Parser.find_chunks().
1321: (4)               def find_labels(self, tokens, **kwargs):
1322: (8)                   """Annotates the given list of tokens with verb/predicate tags."""
1323: (8)                   return find_relations(tokens)
1324: (4)               def find_lemmata(self, tokens, **kwargs):
1325: (8)                   """Annotates the given list of tokens with word lemmata."""
1326: (8)                   return [token + [token[0].lower()] for token in tokens]
1327: (4)               def parse(
1328: (8)                   self,
1329: (8)                   s,
1330: (8)                   tokenize=True,
1331: (8)                   tags=True,
1332: (8)                   chunks=True,
1333: (8)                   relations=False,
1334: (8)                   lemmata=False,
1335: (8)                   encoding="utf-8",
1336: (8)                   **kwargs,
1337: (4)               ):
1338: (8)                   """Takes a string (sentences) and returns a tagged Unicode string (TaggedString).
1339: (8)                   Sentences in the output are separated by newlines.
1340: (8)                   With tokenize=True, punctuation is split from words and sentences are separated by \n.
1341: (8)                   With tags=True, part-of-speech tags are parsed (NN, VB, IN, ...).
1342: (8)                   With chunks=True, phrase chunk tags are parsed (NP, VP, PP, PNP, ...).
1343: (8)                   With relations=True, semantic role labels are parsed (SBJ, OBJ).
1344: (8)                   With lemmata=True, word lemmata are parsed.
1345: (8)                   Optional parameters are passed to
1346: (8)                   the tokenizer, tagger, chunker, labeler and lemmatizer.
1347: (8)                   """
1348: (8)                   # Tokenizer.
1349: (8)                   if tokenize:
1350: (12)                      s = self.find_tokens(s, **kwargs)
1351: (8)                   if isinstance(s, (list, tuple)):
1352: (12)                      s = [isinstance(s, basestring) and s.split(" ") or s for s in s]
1353: (8)                   if isinstance(s, basestring):
1354: (12)                      s = [s.split(" ") for s in s.split("\n")]
1355: (8)                   # Unicode.
1356: (8)                   for i in range(len(s)):
1357: (12)                      for j in range(len(s[i])):
1358: (16)                          if isinstance(s[i][j], bytes):
1359: (20)                              s[i][j] = decode_string(s[i][j], encoding)
1360: (12)                      # Tagger (required by chunker, labeler & lemmatizer).
1361: (12)                      if tags or chunks or relations or lemmata:
1362: (16)                          s[i] = self.find_tags(s[i], **kwargs)
1363: (12)                      else:
1364: (16)                          s[i] = [[w] for w in s[i]]
1365: (12)                      # Chunker.
1366: (12)                      if chunks or relations:
1367: (16)                          s[i] = self.find_chunks(s[i], **kwargs)
1368: (12)                      # Labeler.
1369: (12)                      if relations:
1370: (16)                          s[i] = self.find_labels(s[i], **kwargs)
1371: (12)                      # Lemmatizer.
1372: (12)                      if lemmata:
1373: (16)                          s[i] = self.find_lemmata(s[i], **kwargs)
1374: (8)                   # Slash-formatted tagged string.
1375: (8)                   # With collapse=False (or split=True), returns raw list
1376: (8)                   # (this output is not usable by tree.Text).
1377: (8)                   if not kwargs.get("collapse", True) or kwargs.get("split", False):
1378: (12)                      return s
1379: (8)                   # Construct TaggedString.format.
1380: (8)                   # (this output is usable by tree.Text).
1381: (8)                   format = ["word"]
1382: (8)                   if tags:
1383: (12)                      format.append("part-of-speech")
1384: (8)                   if chunks:
1385: (12)                      format.extend(("chunk", "preposition"))
1386: (8)                   if relations:
1387: (12)                      format.append("relation")
1388: (8)                   if lemmata:
1389: (12)                      format.append("lemma")
1390: (8)                   # Collapse raw list.
1391: (8)                   # Sentences are separated by newlines, tokens by spaces, tags by slashes.
1392: (8)                   # Slashes in words are encoded with &slash;
1393: (8)                   for i in range(len(s)):
1394: (12)                      for j in range(len(s[i])):
1395: (16)                          s[i][j][0] = s[i][j][0].replace("/", "&slash;")
1396: (16)                          s[i][j] = "/".join(s[i][j])
1397: (12)                      s[i] = " ".join(s[i])
1398: (8)                   s = "\n".join(s)
1399: (8)                   s = TaggedString(
1400: (12)                      str(s), format, language=kwargs.get("language", self.language)
1401: (8)                   )
1402: (8)                   return s
1403: (0)           # --- TAGGED STRING ---------------------------------------------------------------------------------
1404: (0)           # Pattern.parse() returns a TaggedString: a Unicode string with "tags" and "language" attributes.
1405: (0)           # The pattern.text.tree.Text class uses this attribute to determine the token format and
1406: (0)           # transform the tagged string to a parse tree of nested Sentence, Chunk and Word objects.
1407: (0)           TOKENS = "tokens"
1408: (0)           class TaggedString(str):
1409: (4)               def __new__(self, string, tags=None, language=None):
1410: (8)                   """Unicode string with tags and language attributes.
1411: (8)                   For example: TaggedString("cat/NN/NP", tags=["word", "pos", "chunk"]).
1412: (8)                   """
1413: (8)                   # From a TaggedString:
1414: (8)                   if tags is None:
1415: (12)                      tags = ["word"]
1416: (8)                   if isinstance(string, str) and hasattr(string, "tags"):
1417: (12)                      tags, language = string.tags, string.language
1418: (8)                   # From a TaggedString.split(TOKENS) list:
1419: (8)                   if isinstance(string, list):
1420: (12)                      string = [
1421: (16)                          [[x.replace("/", "&slash;") for x in token] for token in s]
1422: (16)                          for s in string
1423: (12)                      ]
1424: (12)                      string = "\n".join(" ".join("/".join(token) for token in s) for s in string)
1425: (8)                   s = str.__new__(self, string)
1426: (8)                   s.tags = list(tags)
1427: (8)                   s.language = language
1428: (8)                   return s
1429: (4)               def split(self, sep=TOKENS):
1430: (8)                   """Returns a list of sentences, where each sentence is a list of tokens,
1431: (8)                   where each token is a list of word + tags.
1432: (8)                   """
1433: (8)                   if sep != TOKENS:
1434: (12)                      return str.split(self, sep)
1435: (8)                   if len(self) == 0:
1436: (12)                      return []
1437: (8)                   return [
1438: (12)                      [
1439: (16)                          [x.replace("&slash;", "/") for x in token.split("/")]
1440: (16)                          for token in sentence.split(" ")
1441: (12)                      ]
1442: (12)                      for sentence in str.split(self, "\n")
1443: (8)                   ]
1444: (0)           #### SPELLING CORRECTION ###########################################################################
1445: (0)           # Based on: Peter Norvig, "How to Write a Spelling Corrector", http://norvig.com/spell-correct.html
1446: (0)           class Spelling(lazydict):
1447: (4)               ALPHA = "abcdefghijklmnopqrstuvwxyz"
1448: (4)               def __init__(self, path=""):
1449: (8)                   self._path = path
1450: (4)               def load(self):
1451: (8)                   for x in _read(self._path):
1452: (12)                      x = x.split()
1453: (12)                      dict.__setitem__(self, x[0], int(x[1]))
1454: (4)               @property
1455: (4)               def path(self):
1456: (8)                   return self._path
1457: (4)               @property
1458: (4)               def language(self):
1459: (8)                   return self._language
1460: (4)               @classmethod
1461: (4)               def train(self, s, path="spelling.txt"):
1462: (8)                   """Counts the words in the given string and saves the probabilities at the given path.
1463: (8)                   This can be used to generate a new model for the Spelling() constructor.
1464: (8)                   """
1465: (8)                   model = {}
1466: (8)                   for w in re.findall("[a-z]+", s.lower()):
1467: (12)                      model[w] = w in model and model[w] + 1 or 1
1468: (8)                   model = (f"{k} {v}" for k, v in sorted(model.items()))
1469: (8)                   model = "\n".join(model)
1470: (8)                   f = open(path, "w")
1471: (8)                   f.write(model)
1472: (8)                   f.close()
1473: (4)               def _edit1(self, w):
1474: (8)                   """Returns a set of words with edit distance 1 from the given word."""
1475: (8)                   # Of all spelling errors, 80% is covered by edit distance 1.
1476: (8)                   # Edit distance 1 = one character deleted, swapped, replaced or inserted.
1477: (8)                   split = [(w[:i], w[i:]) for i in range(len(w) + 1)]
1478: (8)                   delete, transpose, replace, insert = (
1479: (12)                      [a + b[1:] for a, b in split if b],
1480: (12)                      [a + b[1] + b[0] + b[2:] for a, b in split if len(b) > 1],
1481: (12)                      [a + c + b[1:] for a, b in split for c in Spelling.ALPHA if b],
1482: (12)                      [a + c + b[0:] for a, b in split for c in Spelling.ALPHA],
1483: (8)                   )
1484: (8)                   return set(delete + transpose + replace + insert)
1485: (4)               def _edit2(self, w):
1486: (8)                   """Returns a set of words with edit distance 2 from the given word"""
1487: (8)                   # Of all spelling errors, 99% is covered by edit distance 2.
1488: (8)                   # Only keep candidates that are actually known words (20% speedup).
1489: (8)                   return set(e2 for e1 in self._edit1(w) for e2 in self._edit1(e1) if e2 in self)
1490: (4)               def _known(self, words=None):
1491: (8)                   """Returns the given list of words filtered by known words."""
1492: (8)                   if words is None:
1493: (12)                      words = []
1494: (8)                   return set(w for w in words if w in self)
1495: (4)               def suggest(self, w):
1496: (8)                   """Return a list of (word, confidence) spelling corrections for the given word,
1497: (8)                   based on the probability of known words with edit distance 1-2 from the given word.
1498: (8)                   """
1499: (8)                   if len(self) == 0:
1500: (12)                      self.load()
1501: (8)                   if len(w) == 1:
1502: (12)                      return [(w, 1.0)]  # I
1503: (8)                   if w in PUNCTUATION:
1504: (12)                      return [(w, 1.0)]  # .?!
1505: (8)                   if w in string.whitespace:
1506: (12)                      return [(w, 1.0)]  # \n
1507: (8)                   if w.replace(".", "").isdigit():
1508: (12)                      return [(w, 1.0)]  # 1.5
1509: (8)                   candidates = (
1510: (12)                      self._known([w])
1511: (12)                      or self._known(self._edit1(w))
1512: (12)                      or self._known(self._edit2(w))
1513: (12)                      or [w]
1514: (8)                   )
1515: (8)                   candidates = [(self.get(c, 0.0), c) for c in candidates]
1516: (8)                   s = float(sum(p for p, word in candidates) or 1)
1517: (8)                   candidates = sorted(((p / s, word) for p, word in candidates), reverse=True)
1518: (8)                   if w.istitle():  # Preserve capitalization
1519: (12)                      candidates = [(word.title(), p) for p, word in candidates]
1520: (8)                   else:
1521: (12)                      candidates = [(word, p) for p, word in candidates]
1522: (8)                   return candidates

----------------------------------------

File 5 - . \mixins.py:

1: (0)              import sys
2: (0)              class ComparableMixin:
3: (4)                  """Implements rich operators for an object."""
4: (4)                  def _compare(self, other, method):
5: (8)                      try:
6: (12)                         return method(self._cmpkey(), other._cmpkey())
7: (8)                      except (AttributeError, TypeError):
8: (12)                         # _cmpkey not implemented, or return different type,
9: (12)                         # so I can't compare with "other". Try the reverse comparison
10: (12)                        return NotImplemented
11: (4)                 def __lt__(self, other):
12: (8)                     return self._compare(other, lambda s, o: s < o)
13: (4)                 def __le__(self, other):
14: (8)                     return self._compare(other, lambda s, o: s <= o)
15: (4)                 def __eq__(self, other):
16: (8)                     return self._compare(other, lambda s, o: s == o)
17: (4)                 def __ge__(self, other):
18: (8)                     return self._compare(other, lambda s, o: s >= o)
19: (4)                 def __gt__(self, other):
20: (8)                     return self._compare(other, lambda s, o: s > o)
21: (4)                 def __ne__(self, other):
22: (8)                     return self._compare(other, lambda s, o: s != o)
23: (0)             class BlobComparableMixin(ComparableMixin):
24: (4)                 """Allow blob objects to be comparable with both strings and blobs."""
25: (4)                 def _compare(self, other, method):
26: (8)                     if isinstance(other, (str, bytes)):
27: (12)                        # Just compare with the other string
28: (12)                        return method(self._cmpkey(), other)
29: (8)                     return super()._compare(other, method)
30: (0)             class StringlikeMixin:
31: (4)                 """Make blob objects behave like Python strings.
32: (4)                 Expects that classes that use this mixin to have a _strkey() method that
33: (4)                 returns the string to apply string methods to. Using _strkey() instead
34: (4)                 of __str__ ensures consistent behavior between Python 2 and 3.
35: (4)                 """
36: (4)                 def __repr__(self):
37: (8)                     """Returns a string representation for debugging."""
38: (8)                     class_name = self.__class__.__name__
39: (8)                     text = str(self)
40: (8)                     return f'{class_name}("{text}")'
41: (4)                 def __str__(self):
42: (8)                     """Returns a string representation used in print statements
43: (8)                     or str(my_blob)."""
44: (8)                     return self._strkey()
45: (4)                 def __len__(self):
46: (8)                     """Returns the length of the raw text."""
47: (8)                     return len(self._strkey())
48: (4)                 def __iter__(self):
49: (8)                     """Makes the object iterable as if it were a string,
50: (8)                     iterating through the raw string's characters.
51: (8)                     """
52: (8)                     return iter(self._strkey())
53: (4)                 def __contains__(self, sub):
54: (8)                     """Implements the `in` keyword like a Python string."""
55: (8)                     return sub in self._strkey()
56: (4)                 def __getitem__(self, index):
57: (8)                     """Returns a  substring. If index is an integer, returns a Python
58: (8)                     string of a single character. If a range is given, e.g. `blob[3:5]`,
59: (8)                     a new instance of the class is returned.
60: (8)                     """
61: (8)                     if isinstance(index, int):
62: (12)                        return self._strkey()[index]  # Just return a single character
63: (8)                     else:
64: (12)                        # Return a new blob object
65: (12)                        return self.__class__(self._strkey()[index])
66: (4)                 def find(self, sub, start=0, end=sys.maxsize):
67: (8)                     """Behaves like the built-in str.find() method. Returns an integer,
68: (8)                     the index of the first occurrence of the substring argument sub in the
69: (8)                     sub-string given by [start:end].
70: (8)                     """
71: (8)                     return self._strkey().find(sub, start, end)
72: (4)                 def rfind(self, sub, start=0, end=sys.maxsize):
73: (8)                     """Behaves like the built-in str.rfind() method. Returns an integer,
74: (8)                     the index of he last (right-most) occurence of the substring argument
75: (8)                     sub in the sub-sequence given by [start:end].
76: (8)                     """
77: (8)                     return self._strkey().rfind(sub, start, end)
78: (4)                 def index(self, sub, start=0, end=sys.maxsize):
79: (8)                     """Like blob.find() but raise ValueError when the substring
80: (8)                     is not found.
81: (8)                     """
82: (8)                     return self._strkey().index(sub, start, end)
83: (4)                 def rindex(self, sub, start=0, end=sys.maxsize):
84: (8)                     """Like blob.rfind() but raise ValueError when substring is not
85: (8)                     found.
86: (8)                     """
87: (8)                     return self._strkey().rindex(sub, start, end)
88: (4)                 def startswith(self, prefix, start=0, end=sys.maxsize):
89: (8)                     """Returns True if the blob starts with the given prefix."""
90: (8)                     return self._strkey().startswith(prefix, start, end)
91: (4)                 def endswith(self, suffix, start=0, end=sys.maxsize):
92: (8)                     """Returns True if the blob ends with the given suffix."""
93: (8)                     return self._strkey().endswith(suffix, start, end)
94: (4)                 # PEP8 aliases
95: (4)                 starts_with = startswith
96: (4)                 ends_with = endswith
97: (4)                 def title(self):
98: (8)                     """Returns a blob object with the text in title-case."""
99: (8)                     return self.__class__(self._strkey().title())
100: (4)                def format(self, *args, **kwargs):
101: (8)                    """Perform a string formatting operation, like the built-in
102: (8)                    `str.format(*args, **kwargs)`. Returns a blob object.
103: (8)                    """
104: (8)                    return self.__class__(self._strkey().format(*args, **kwargs))
105: (4)                def split(self, sep=None, maxsplit=sys.maxsize):
106: (8)                    """Behaves like the built-in str.split()."""
107: (8)                    return self._strkey().split(sep, maxsplit)
108: (4)                def strip(self, chars=None):
109: (8)                    """Behaves like the built-in str.strip([chars]) method. Returns
110: (8)                    an object with leading and trailing whitespace removed.
111: (8)                    """
112: (8)                    return self.__class__(self._strkey().strip(chars))
113: (4)                def upper(self):
114: (8)                    """Like str.upper(), returns new object with all upper-cased characters."""
115: (8)                    return self.__class__(self._strkey().upper())
116: (4)                def lower(self):
117: (8)                    """Like str.lower(), returns new object with all lower-cased characters."""
118: (8)                    return self.__class__(self._strkey().lower())
119: (4)                def join(self, iterable):
120: (8)                    """Behaves like the built-in `str.join(iterable)` method, except
121: (8)                    returns a blob object.
122: (8)                    Returns a blob which is the concatenation of the strings or blobs
123: (8)                    in the iterable.
124: (8)                    """
125: (8)                    return self.__class__(self._strkey().join(iterable))
126: (4)                def replace(self, old, new, count=sys.maxsize):
127: (8)                    """Return a new blob object with all the occurence of `old` replaced
128: (8)                    by `new`.
129: (8)                    """
130: (8)                    return self.__class__(self._strkey().replace(old, new, count))

----------------------------------------

File 6 - . \formats.py:

1: (0)              """File formats for training and testing data.
2: (0)              Includes a registry of valid file formats. New file formats can be added to the
3: (0)              registry like so: ::
4: (4)                  from textblob import formats
5: (4)                  class PipeDelimitedFormat(formats.DelimitedFormat):
6: (8)                      delimiter = '|'
7: (4)                  formats.register('psv', PipeDelimitedFormat)
8: (0)              Once a format has been registered, classifiers will be able to read data files with
9: (0)              that format. ::
10: (4)                 from textblob.classifiers import NaiveBayesAnalyzer
11: (4)                 with open('training_data.psv', 'r') as fp:
12: (8)                     cl = NaiveBayesAnalyzer(fp, format='psv')
13: (0)             """
14: (0)             import csv
15: (0)             import json
16: (0)             from collections import OrderedDict
17: (0)             from textblob.utils import is_filelike
18: (0)             DEFAULT_ENCODING = "utf-8"
19: (0)             class BaseFormat:
20: (4)                 """Interface for format classes. Individual formats can decide on the
21: (4)                 composition and meaning of ``**kwargs``.
22: (4)                 :param File fp: A file-like object.
23: (4)                 .. versionchanged:: 0.9.0
24: (8)                     Constructor receives a file pointer rather than a file path.
25: (4)                 """
26: (4)                 def __init__(self, fp, **kwargs):
27: (8)                     pass
28: (4)                 def to_iterable(self):
29: (8)                     """Return an iterable object from the data."""
30: (8)                     raise NotImplementedError('Must implement a "to_iterable" method.')
31: (4)                 @classmethod
32: (4)                 def detect(cls, stream):
33: (8)                     """Detect the file format given a filename.
34: (8)                     Return True if a stream is this file format.
35: (8)                     .. versionchanged:: 0.9.0
36: (12)                        Changed from a static method to a class method.
37: (8)                     """
38: (8)                     raise NotImplementedError('Must implement a "detect" class method.')
39: (0)             class DelimitedFormat(BaseFormat):
40: (4)                 """A general character-delimited format."""
41: (4)                 delimiter = ","
42: (4)                 def __init__(self, fp, **kwargs):
43: (8)                     BaseFormat.__init__(self, fp, **kwargs)
44: (8)                     reader = csv.reader(fp, delimiter=self.delimiter)
45: (8)                     self.data = [row for row in reader]
46: (4)                 def to_iterable(self):
47: (8)                     """Return an iterable object from the data."""
48: (8)                     return self.data
49: (4)                 @classmethod
50: (4)                 def detect(cls, stream):
51: (8)                     """Return True if stream is valid."""
52: (8)                     try:
53: (12)                        csv.Sniffer().sniff(stream, delimiters=cls.delimiter)
54: (12)                        return True
55: (8)                     except (csv.Error, TypeError):
56: (12)                        return False
57: (0)             class CSV(DelimitedFormat):
58: (4)                 """CSV format. Assumes each row is of the form ``text,label``.
59: (4)                 ::
60: (8)                     Today is a good day,pos
61: (8)                     I hate this car.,pos
62: (4)                 """
63: (4)                 delimiter = ","
64: (0)             class TSV(DelimitedFormat):
65: (4)                 """TSV format. Assumes each row is of the form ``text\tlabel``."""
66: (4)                 delimiter = "\t"
67: (0)             class JSON(BaseFormat):
68: (4)                 """JSON format.
69: (4)                 Assumes that JSON is formatted as an array of objects with ``text`` and
70: (4)                 ``label`` properties.
71: (4)                 ::
72: (8)                     [
73: (12)                        {"text": "Today is a good day.", "label": "pos"},
74: (12)                        {"text": "I hate this car.", "label": "neg"}
75: (8)                     ]
76: (4)                 """
77: (4)                 def __init__(self, fp, **kwargs):
78: (8)                     BaseFormat.__init__(self, fp, **kwargs)
79: (8)                     self.dict = json.load(fp)
80: (4)                 def to_iterable(self):
81: (8)                     """Return an iterable object from the JSON data."""
82: (8)                     return [(d["text"], d["label"]) for d in self.dict]
83: (4)                 @classmethod
84: (4)                 def detect(cls, stream):
85: (8)                     """Return True if stream is valid JSON."""
86: (8)                     try:
87: (12)                        json.loads(stream)
88: (12)                        return True
89: (8)                     except ValueError:
90: (12)                        return False
91: (0)             _registry = OrderedDict(
92: (4)                 [
93: (8)                     ("csv", CSV),
94: (8)                     ("json", JSON),
95: (8)                     ("tsv", TSV),
96: (4)                 ]
97: (0)             )
98: (0)             def detect(fp, max_read=1024):
99: (4)                 """Attempt to detect a file's format, trying each of the supported
100: (4)                formats. Return the format class that was detected. If no format is
101: (4)                detected, return ``None``.
102: (4)                """
103: (4)                if not is_filelike(fp):
104: (8)                    return None
105: (4)                for Format in _registry.values():
106: (8)                    if Format.detect(fp.read(max_read)):
107: (12)                       fp.seek(0)
108: (12)                       return Format
109: (8)                    fp.seek(0)
110: (4)                return None
111: (0)            def get_registry():
112: (4)                """Return a dictionary of registered formats."""
113: (4)                return _registry
114: (0)            def register(name, format_class):
115: (4)                """Register a new format.
116: (4)                :param str name: The name that will be used to refer to the format, e.g. 'csv'
117: (4)                :param type format_class: The format class to register.
118: (4)                """
119: (4)                get_registry()[name] = format_class

----------------------------------------

File 7 - . \inflect.py:

1: (0)              """Make word inflection default to English. This allows for backwards
2: (0)              compatibility so you can still import text.inflect.
3: (4)                  >>> from textblob.inflect import singularize
4: (0)              is equivalent to
5: (4)                  >>> from textblob.en.inflect import singularize
6: (0)              """
7: (0)              from textblob.en.inflect import pluralize, singularize
8: (0)              __all__ = [
9: (4)                  "singularize",
10: (4)                 "pluralize",
11: (0)             ]

----------------------------------------

File 8 - . \parsers.py:

1: (0)              """Default parsers to English for backwards compatibility so you can still do
2: (0)              >>> from textblob.parsers import PatternParser
3: (0)              which is equivalent to
4: (0)              >>> from textblob.en.parsers import PatternParser
5: (0)              """
6: (0)              from textblob.base import BaseParser
7: (0)              from textblob.en.parsers import PatternParser
8: (0)              __all__ = [
9: (4)                  "BaseParser",
10: (4)                 "PatternParser",
11: (0)             ]

----------------------------------------

File 9 - . \decorators.py:

1: (0)              """Custom decorators."""
2: (0)              from functools import wraps
3: (0)              from textblob.exceptions import MissingCorpusError
4: (0)              class cached_property:
5: (4)                  """A property that is only computed once per instance and then replaces
6: (4)                  itself with an ordinary attribute. Deleting the attribute resets the
7: (4)                  property.
8: (4)                  Credit to Marcel Hellkamp, author of bottle.py.
9: (4)                  """
10: (4)                 def __init__(self, func):
11: (8)                     self.__doc__ = func.__doc__
12: (8)                     self.func = func
13: (4)                 def __get__(self, obj, cls):
14: (8)                     if obj is None:
15: (12)                        return self
16: (8)                     value = obj.__dict__[self.func.__name__] = self.func(obj)
17: (8)                     return value
18: (0)             def requires_nltk_corpus(func):
19: (4)                 """Wraps a function that requires an NLTK corpus. If the corpus isn't found,
20: (4)                 raise a :exc:`MissingCorpusError`.
21: (4)                 """
22: (4)                 @wraps(func)
23: (4)                 def decorated(*args, **kwargs):
24: (8)                     try:
25: (12)                        return func(*args, **kwargs)
26: (8)                     except LookupError as error:
27: (12)                        raise MissingCorpusError() from error
28: (4)                 return decorated

----------------------------------------

File 10 - . \exceptions.py:

1: (0)              MISSING_CORPUS_MESSAGE = """
2: (0)              Looks like you are missing some required data for this feature.
3: (0)              To download the necessary data, simply run
4: (4)                  python -m textblob.download_corpora
5: (0)              or use the NLTK downloader to download the missing data: http://nltk.org/data.html
6: (0)              If this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.
7: (0)              """
8: (0)              class TextBlobError(Exception):
9: (4)                  """A TextBlob-related error."""
10: (4)                 pass
11: (0)             TextBlobException = TextBlobError  # Backwards compat
12: (0)             class MissingCorpusError(TextBlobError):
13: (4)                 """Exception thrown when a user tries to use a feature that requires a
14: (4)                 dataset or model that the user does not have on their system.
15: (4)                 """
16: (4)                 def __init__(self, message=MISSING_CORPUS_MESSAGE, *args, **kwargs):
17: (8)                     super().__init__(message, *args, **kwargs)
18: (0)             MissingCorpusException = MissingCorpusError  # Backwards compat
19: (0)             class DeprecationError(TextBlobError):
20: (4)                 """Raised when user uses a deprecated feature."""
21: (4)                 pass
22: (0)             class TranslatorError(TextBlobError):
23: (4)                 """Raised when an error occurs during language translation or detection."""
24: (4)                 pass
25: (0)             class NotTranslated(TranslatorError):
26: (4)                 """Raised when text is unchanged after translation. This may be due to the language
27: (4)                 being unsupported by the translator.
28: (4)                 """
29: (4)                 pass
30: (0)             class FormatError(TextBlobError):
31: (4)                 """Raised if a data file with an unsupported format is passed to a classifier."""
32: (4)                 pass

----------------------------------------

File 11 - . \classifiers.py:

1: (0)              """Various classifier implementations. Also includes basic feature extractor
2: (0)              methods.
3: (0)              Example Usage:
4: (0)              ::
5: (4)                  >>> from textblob import TextBlob
6: (4)                  >>> from textblob.classifiers import NaiveBayesClassifier
7: (4)                  >>> train = [
8: (4)                  ...     ('I love this sandwich.', 'pos'),
9: (4)                  ...     ('This is an amazing place!', 'pos'),
10: (4)                 ...     ('I feel very good about these beers.', 'pos'),
11: (4)                 ...     ('I do not like this restaurant', 'neg'),
12: (4)                 ...     ('I am tired of this stuff.', 'neg'),
13: (4)                 ...     ("I can't deal with this", 'neg'),
14: (4)                 ...     ("My boss is horrible.", "neg")
15: (4)                 ... ]
16: (4)                 >>> cl = NaiveBayesClassifier(train)
17: (4)                 >>> cl.classify("I feel amazing!")
18: (4)                 'pos'
19: (4)                 >>> blob = TextBlob("The beer is good. But the hangover is horrible.", classifier=cl)
20: (4)                 >>> for s in blob.sentences:
21: (4)                 ...     print(s)
22: (4)                 ...     print(s.classify())
23: (4)                 ...
24: (4)                 The beer is good.
25: (4)                 pos
26: (4)                 But the hangover is horrible.
27: (4)                 neg
28: (0)             .. versionadded:: 0.6.0
29: (0)             """  # noqa: E501
30: (0)             from itertools import chain
31: (0)             import nltk
32: (0)             import textblob.formats as formats
33: (0)             from textblob.decorators import cached_property
34: (0)             from textblob.exceptions import FormatError
35: (0)             from textblob.tokenizers import word_tokenize
36: (0)             from textblob.utils import is_filelike, strip_punc
37: (0)             basestring = (str, bytes)
38: (0)             ### Basic feature extractors ###
39: (0)             def _get_words_from_dataset(dataset):
40: (4)                 """Return a set of all words in a dataset.
41: (4)                 :param dataset: A list of tuples of the form ``(words, label)`` where
42: (8)                     ``words`` is either a string of a list of tokens.
43: (4)                 """
44: (4)                 # Words may be either a string or a list of tokens. Return an iterator
45: (4)                 # of tokens accordingly
46: (4)                 def tokenize(words):
47: (8)                     if isinstance(words, basestring):
48: (12)                        return word_tokenize(words, include_punc=False)
49: (8)                     else:
50: (12)                        return words
51: (4)                 all_words = chain.from_iterable(tokenize(words) for words, _ in dataset)
52: (4)                 return set(all_words)
53: (0)             def _get_document_tokens(document):
54: (4)                 if isinstance(document, basestring):
55: (8)                     tokens = set(
56: (12)                        strip_punc(w, all=False)
57: (12)                        for w in word_tokenize(document, include_punc=False)
58: (8)                     )
59: (4)                 else:
60: (8)                     tokens = set(strip_punc(w, all=False) for w in document)
61: (4)                 return tokens
62: (0)             def basic_extractor(document, train_set):
63: (4)                 """A basic document feature extractor that returns a dict indicating
64: (4)                 what words in ``train_set`` are contained in ``document``.
65: (4)                 :param document: The text to extract features from. Can be a string or an iterable.
66: (4)                 :param list train_set: Training data set, a list of tuples of the form
67: (8)                     ``(words, label)`` OR an iterable of strings.
68: (4)                 """
69: (4)                 try:
70: (8)                     el_zero = next(iter(train_set))  # Infer input from first element.
71: (4)                 except StopIteration:
72: (8)                     return {}
73: (4)                 if isinstance(el_zero, basestring):
74: (8)                     word_features = [w for w in chain([el_zero], train_set)]
75: (4)                 else:
76: (8)                     try:
77: (12)                        assert isinstance(el_zero[0], basestring)
78: (12)                        word_features = _get_words_from_dataset(chain([el_zero], train_set))
79: (8)                     except Exception as error:
80: (12)                        raise ValueError("train_set is probably malformed.") from error
81: (4)                 tokens = _get_document_tokens(document)
82: (4)                 features = dict((f"contains({word})", (word in tokens)) for word in word_features)
83: (4)                 return features
84: (0)             def contains_extractor(document):
85: (4)                 """A basic document feature extractor that returns a dict of words that
86: (4)                 the document contains.
87: (4)                 """
88: (4)                 tokens = _get_document_tokens(document)
89: (4)                 features = dict((f"contains({w})", True) for w in tokens)
90: (4)                 return features
91: (0)             ##### CLASSIFIERS #####
92: (0)             class BaseClassifier:
93: (4)                 """Abstract classifier class from which all classifers inherit. At a
94: (4)                 minimum, descendant classes must implement a ``classify`` method and have
95: (4)                 a ``classifier`` property.
96: (4)                 :param train_set: The training set, either a list of tuples of the form
97: (8)                     ``(text, classification)`` or a file-like object. ``text`` may be either
98: (8)                     a string or an iterable.
99: (4)                 :param callable feature_extractor: A feature extractor function that takes one or
100: (8)                    two arguments: ``document`` and ``train_set``.
101: (4)                :param str format: If ``train_set`` is a filename, the file format, e.g.
102: (8)                    ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
103: (8)                    file format.
104: (4)                :param kwargs: Additional keyword arguments are passed to the constructor
105: (8)                    of the :class:`Format <textblob.formats.BaseFormat>` class used to
106: (8)                    read the data. Only applies when a file-like object is passed as
107: (8)                    ``train_set``.
108: (4)                .. versionadded:: 0.6.0
109: (4)                """
110: (4)                def __init__(
111: (8)                    self, train_set, feature_extractor=basic_extractor, format=None, **kwargs
112: (4)                ):
113: (8)                    self.format_kwargs = kwargs
114: (8)                    self.feature_extractor = feature_extractor
115: (8)                    if is_filelike(train_set):
116: (12)                       self.train_set = self._read_data(train_set, format)
117: (8)                    else:  # train_set is a list of tuples
118: (12)                       self.train_set = train_set
119: (8)                    self._word_set = _get_words_from_dataset(
120: (12)                       self.train_set
121: (8)                    )  # Keep a hidden set of unique words.
122: (8)                    self.train_features = None
123: (4)                def _read_data(self, dataset, format=None):
124: (8)                    """Reads a data file and returns an iterable that can be used
125: (8)                    as testing or training data.
126: (8)                    """
127: (8)                    # Attempt to detect file format if "format" isn't specified
128: (8)                    if not format:
129: (12)                       format_class = formats.detect(dataset)
130: (12)                       if not format_class:
131: (16)                           raise FormatError(
132: (20)                               "Could not automatically detect format for the given "
133: (20)                               "data source."
134: (16)                           )
135: (8)                    else:
136: (12)                       registry = formats.get_registry()
137: (12)                       if format not in registry.keys():
138: (16)                           raise ValueError(f"'{format}' format not supported.")
139: (12)                       format_class = registry[format]
140: (8)                    return format_class(dataset, **self.format_kwargs).to_iterable()
141: (4)                @cached_property
142: (4)                def classifier(self):
143: (8)                    """The classifier object."""
144: (8)                    raise NotImplementedError('Must implement the "classifier" property.')
145: (4)                def classify(self, text):
146: (8)                    """Classifies a string of text."""
147: (8)                    raise NotImplementedError('Must implement a "classify" method.')
148: (4)                def train(self, labeled_featureset):
149: (8)                    """Trains the classifier."""
150: (8)                    raise NotImplementedError('Must implement a "train" method.')
151: (4)                def labels(self):
152: (8)                    """Returns an iterable containing the possible labels."""
153: (8)                    raise NotImplementedError('Must implement a "labels" method.')
154: (4)                def extract_features(self, text):
155: (8)                    """Extracts features from a body of text.
156: (8)                    :rtype: dictionary of features
157: (8)                    """
158: (8)                    # Feature extractor may take one or two arguments
159: (8)                    try:
160: (12)                       return self.feature_extractor(text, self._word_set)
161: (8)                    except (TypeError, AttributeError):
162: (12)                       return self.feature_extractor(text)
163: (0)            class NLTKClassifier(BaseClassifier):
164: (4)                """An abstract class that wraps around the nltk.classify module.
165: (4)                Expects that descendant classes include a class variable ``nltk_class``
166: (4)                which is the class in the nltk.classify module to be wrapped.
167: (4)                Example: ::
168: (8)                    class MyClassifier(NLTKClassifier):
169: (12)                       nltk_class = nltk.classify.svm.SvmClassifier
170: (4)                """
171: (4)                #: The NLTK class to be wrapped. Must be a class within nltk.classify
172: (4)                nltk_class = None
173: (4)                def __init__(
174: (8)                    self, train_set, feature_extractor=basic_extractor, format=None, **kwargs
175: (4)                ):
176: (8)                    super().__init__(train_set, feature_extractor, format, **kwargs)
177: (8)                    self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]
178: (4)                def __repr__(self):
179: (8)                    class_name = self.__class__.__name__
180: (8)                    return f"<{class_name} trained on {len(self.train_set)} instances>"
181: (4)                @cached_property
182: (4)                def classifier(self):
183: (8)                    """The classifier."""
184: (8)                    try:
185: (12)                       return self.train()
186: (8)                    except AttributeError as error:  # nltk_class has not been defined
187: (12)                       raise ValueError(
188: (16)                           "NLTKClassifier must have a nltk_class" " variable that is not None."
189: (12)                       ) from error
190: (4)                def train(self, *args, **kwargs):
191: (8)                    """Train the classifier with a labeled feature set and return
192: (8)                    the classifier. Takes the same arguments as the wrapped NLTK class.
193: (8)                    This method is implicitly called when calling ``classify`` or
194: (8)                    ``accuracy`` methods and is included only to allow passing in arguments
195: (8)                    to the ``train`` method of the wrapped NLTK class.
196: (8)                    .. versionadded:: 0.6.2
197: (8)                    :rtype: A classifier
198: (8)                    """
199: (8)                    try:
200: (12)                       self.classifier = self.nltk_class.train(
201: (16)                           self.train_features, *args, **kwargs
202: (12)                       )
203: (12)                       return self.classifier
204: (8)                    except AttributeError as error:
205: (12)                       raise ValueError(
206: (16)                           "NLTKClassifier must have a nltk_class" " variable that is not None."
207: (12)                       ) from error
208: (4)                def labels(self):
209: (8)                    """Return an iterable of possible labels."""
210: (8)                    return self.classifier.labels()
211: (4)                def classify(self, text):
212: (8)                    """Classifies the text.
213: (8)                    :param str text: A string of text.
214: (8)                    """
215: (8)                    text_features = self.extract_features(text)
216: (8)                    return self.classifier.classify(text_features)
217: (4)                def accuracy(self, test_set, format=None):
218: (8)                    """Compute the accuracy on a test set.
219: (8)                    :param test_set: A list of tuples of the form ``(text, label)``, or a
220: (12)                       file pointer.
221: (8)                    :param format: If ``test_set`` is a filename, the file format, e.g.
222: (12)                       ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
223: (12)                       file format.
224: (8)                    """
225: (8)                    if is_filelike(test_set):
226: (12)                       test_data = self._read_data(test_set, format)
227: (8)                    else:  # test_set is a list of tuples
228: (12)                       test_data = test_set
229: (8)                    test_features = [(self.extract_features(d), c) for d, c in test_data]
230: (8)                    return nltk.classify.accuracy(self.classifier, test_features)
231: (4)                def update(self, new_data, *args, **kwargs):
232: (8)                    """Update the classifier with new training data and re-trains the
233: (8)                    classifier.
234: (8)                    :param new_data: New data as a list of tuples of the form
235: (12)                       ``(text, label)``.
236: (8)                    """
237: (8)                    self.train_set += new_data
238: (8)                    self._word_set.update(_get_words_from_dataset(new_data))
239: (8)                    self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]
240: (8)                    try:
241: (12)                       self.classifier = self.nltk_class.train(
242: (16)                           self.train_features, *args, **kwargs
243: (12)                       )
244: (8)                    except AttributeError as error:  # Descendant has not defined nltk_class
245: (12)                       raise ValueError(
246: (16)                           "NLTKClassifier must have a nltk_class" " variable that is not None."
247: (12)                       ) from error
248: (8)                    return True
249: (0)            class NaiveBayesClassifier(NLTKClassifier):
250: (4)                """A classifier based on the Naive Bayes algorithm, as implemented in
251: (4)                NLTK.
252: (4)                :param train_set: The training set, either a list of tuples of the form
253: (8)                    ``(text, classification)`` or a filename. ``text`` may be either
254: (8)                    a string or an iterable.
255: (4)                :param feature_extractor: A feature extractor function that takes one or
256: (8)                    two arguments: ``document`` and ``train_set``.
257: (4)                :param format: If ``train_set`` is a filename, the file format, e.g.
258: (8)                    ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
259: (8)                    file format.
260: (4)                .. versionadded:: 0.6.0
261: (4)                """
262: (4)                nltk_class = nltk.classify.NaiveBayesClassifier
263: (4)                def prob_classify(self, text):
264: (8)                    """Return the label probability distribution for classifying a string
265: (8)                    of text.
266: (8)                    Example:
267: (8)                    ::
268: (12)                       >>> classifier = NaiveBayesClassifier(train_data)
269: (12)                       >>> prob_dist = classifier.prob_classify("I feel happy this morning.")
270: (12)                       >>> prob_dist.max()
271: (12)                       'positive'
272: (12)                       >>> prob_dist.prob("positive")
273: (12)                       0.7
274: (8)                    :rtype: nltk.probability.DictionaryProbDist
275: (8)                    """
276: (8)                    text_features = self.extract_features(text)
277: (8)                    return self.classifier.prob_classify(text_features)
278: (4)                def informative_features(self, *args, **kwargs):
279: (8)                    """Return the most informative features as a list of tuples of the
280: (8)                    form ``(feature_name, feature_value)``.
281: (8)                    :rtype: list
282: (8)                    """
283: (8)                    return self.classifier.most_informative_features(*args, **kwargs)
284: (4)                def show_informative_features(self, *args, **kwargs):
285: (8)                    """Displays a listing of the most informative features for this
286: (8)                    classifier.
287: (8)                    :rtype: None
288: (8)                    """
289: (8)                    return self.classifier.show_most_informative_features(*args, **kwargs)
290: (0)            class DecisionTreeClassifier(NLTKClassifier):
291: (4)                """A classifier based on the decision tree algorithm, as implemented in
292: (4)                NLTK.
293: (4)                :param train_set: The training set, either a list of tuples of the form
294: (8)                    ``(text, classification)`` or a filename. ``text`` may be either
295: (8)                    a string or an iterable.
296: (4)                :param feature_extractor: A feature extractor function that takes one or
297: (8)                    two arguments: ``document`` and ``train_set``.
298: (4)                :param format: If ``train_set`` is a filename, the file format, e.g.
299: (8)                    ``"csv"`` or ``"json"``. If ``None``, will attempt to detect the
300: (8)                    file format.
301: (4)                .. versionadded:: 0.6.2
302: (4)                """
303: (4)                nltk_class = nltk.classify.decisiontree.DecisionTreeClassifier
304: (4)                def pretty_format(self, *args, **kwargs):
305: (8)                    """Return a string containing a pretty-printed version of this decision
306: (8)                    tree. Each line in the string corresponds to a single decision tree node
307: (8)                    or leaf, and indentation is used to display the structure of the tree.
308: (8)                    :rtype: str
309: (8)                    """
310: (8)                    return self.classifier.pretty_format(*args, **kwargs)
311: (4)                # Backwards-compat
312: (4)                pprint = pretty_format
313: (4)                def pseudocode(self, *args, **kwargs):
314: (8)                    """Return a string representation of this decision tree that expresses
315: (8)                    the decisions it makes as a nested set of pseudocode if statements.
316: (8)                    :rtype: str
317: (8)                    """
318: (8)                    return self.classifier.pseudocode(*args, **kwargs)
319: (0)            class PositiveNaiveBayesClassifier(NLTKClassifier):
320: (4)                """A variant of the Naive Bayes Classifier that performs binary
321: (4)                classification with partially-labeled training sets, i.e. when only
322: (4)                one class is labeled and the other is not. Assuming a prior distribution
323: (4)                on the two labels, uses the unlabeled set to estimate the frequencies of
324: (4)                the features.
325: (4)                Example usage:
326: (4)                ::
327: (8)                    >>> from text.classifiers import PositiveNaiveBayesClassifier
328: (8)                    >>> sports_sentences = ['The team dominated the game',
329: (8)                    ...                   'They lost the ball',
330: (8)                    ...                   'The game was intense',
331: (8)                    ...                   'The goalkeeper catched the ball',
332: (8)                    ...                   'The other team controlled the ball']
333: (8)                    >>> various_sentences = ['The President did not comment',
334: (8)                    ...                        'I lost the keys',
335: (8)                    ...                        'The team won the game',
336: (8)                    ...                        'Sara has two kids',
337: (8)                    ...                        'The ball went off the court',
338: (8)                    ...                        'They had the ball for the whole game',
339: (8)                    ...                        'The show is over']
340: (8)                    >>> classifier = PositiveNaiveBayesClassifier(positive_set=sports_sentences,
341: (8)                    ...                                           unlabeled_set=various_sentences)
342: (8)                    >>> classifier.classify("My team lost the game")
343: (8)                    True
344: (8)                    >>> classifier.classify("And now for something completely different.")
345: (8)                    False
346: (4)                :param positive_set: A collection of strings that have the positive label.
347: (4)                :param unlabeled_set: A collection of unlabeled strings.
348: (4)                :param feature_extractor: A feature extractor function.
349: (4)                :param positive_prob_prior: A prior estimate of the probability of the
350: (8)                    label ``True``.
351: (4)                .. versionadded:: 0.7.0
352: (4)                """
353: (4)                nltk_class = nltk.classify.PositiveNaiveBayesClassifier
354: (4)                def __init__(
355: (8)                    self,
356: (8)                    positive_set,
357: (8)                    unlabeled_set,
358: (8)                    feature_extractor=contains_extractor,
359: (8)                    positive_prob_prior=0.5,
360: (8)                    **kwargs,
361: (4)                ):
362: (8)                    self.feature_extractor = feature_extractor
363: (8)                    self.positive_set = positive_set
364: (8)                    self.unlabeled_set = unlabeled_set
365: (8)                    self.positive_features = [self.extract_features(d) for d in self.positive_set]
366: (8)                    self.unlabeled_features = [self.extract_features(d) for d in self.unlabeled_set]
367: (8)                    self.positive_prob_prior = positive_prob_prior
368: (4)                def __repr__(self):
369: (8)                    class_name = self.__class__.__name__
370: (8)                    return (
371: (12)                       f"<{class_name} trained on {len(self.positive_set)} labeled "
372: (12)                       f"and {len(self.unlabeled_set)} unlabeled instances>"
373: (8)                    )
374: (4)                # Override
375: (4)                def train(self, *args, **kwargs):
376: (8)                    """Train the classifier with a labeled and unlabeled feature sets and return
377: (8)                    the classifier. Takes the same arguments as the wrapped NLTK class.
378: (8)                    This method is implicitly called when calling ``classify`` or
379: (8)                    ``accuracy`` methods and is included only to allow passing in arguments
380: (8)                    to the ``train`` method of the wrapped NLTK class.
381: (8)                    :rtype: A classifier
382: (8)                    """
383: (8)                    self.classifier = self.nltk_class.train(
384: (12)                       self.positive_features, self.unlabeled_features, self.positive_prob_prior
385: (8)                    )
386: (8)                    return self.classifier
387: (4)                def update(
388: (8)                    self,
389: (8)                    new_positive_data=None,
390: (8)                    new_unlabeled_data=None,
391: (8)                    positive_prob_prior=0.5,
392: (8)                    *args,
393: (8)                    **kwargs,
394: (4)                ):
395: (8)                    """Update the classifier with new data and re-trains the
396: (8)                    classifier.
397: (8)                    :param new_positive_data: List of new, labeled strings.
398: (8)                    :param new_unlabeled_data: List of new, unlabeled strings.
399: (8)                    """
400: (8)                    self.positive_prob_prior = positive_prob_prior
401: (8)                    if new_positive_data:
402: (12)                       self.positive_set += new_positive_data
403: (12)                       self.positive_features += [
404: (16)                           self.extract_features(d) for d in new_positive_data
405: (12)                       ]
406: (8)                    if new_unlabeled_data:
407: (12)                       self.unlabeled_set += new_unlabeled_data
408: (12)                       self.unlabeled_features += [
409: (16)                           self.extract_features(d) for d in new_unlabeled_data
410: (12)                       ]
411: (8)                    self.classifier = self.nltk_class.train(
412: (12)                       self.positive_features,
413: (12)                       self.unlabeled_features,
414: (12)                       self.positive_prob_prior,
415: (12)                       *args,
416: (12)                       **kwargs,
417: (8)                    )
418: (8)                    return True
419: (0)            class MaxEntClassifier(NLTKClassifier):
420: (4)                __doc__ = nltk.classify.maxent.MaxentClassifier.__doc__
421: (4)                nltk_class = nltk.classify.maxent.MaxentClassifier
422: (4)                def prob_classify(self, text):
423: (8)                    """Return the label probability distribution for classifying a string
424: (8)                    of text.
425: (8)                    Example:
426: (8)                    ::
427: (12)                       >>> classifier = MaxEntClassifier(train_data)
428: (12)                       >>> prob_dist = classifier.prob_classify("I feel happy this morning.")
429: (12)                       >>> prob_dist.max()
430: (12)                       'positive'
431: (12)                       >>> prob_dist.prob("positive")
432: (12)                       0.7
433: (8)                    :rtype: nltk.probability.DictionaryProbDist
434: (8)                    """
435: (8)                    feats = self.extract_features(text)
436: (8)                    return self.classifier.prob_classify(feats)

----------------------------------------

File 12 - . \np_extractors.py:

1: (0)              """Default noun phrase extractors are for English to maintain backwards
2: (0)              compatibility, so you can still do
3: (0)              >>> from textblob.np_extractors import ConllExtractor
4: (0)              which is equivalent to
5: (0)              >>> from textblob.en.np_extractors import ConllExtractor
6: (0)              """
7: (0)              from textblob.base import BaseNPExtractor
8: (0)              from textblob.en.np_extractors import ConllExtractor, FastNPExtractor
9: (0)              __all__ = [
10: (4)                 "BaseNPExtractor",
11: (4)                 "ConllExtractor",
12: (4)                 "FastNPExtractor",
13: (0)             ]

----------------------------------------

File 13 - . \download_corpora.py:

1: (0)              #!/usr/bin/env python
2: (0)              """Downloads the necessary NLTK corpora for TextBlob.
3: (0)              Usage: ::
4: (4)                  $ python -m textblob.download_corpora
5: (0)              If you only intend to use TextBlob's default models, you can use the "lite"
6: (0)              option: ::
7: (4)                  $ python -m textblob.download_corpora lite
8: (0)              """
9: (0)              import sys
10: (0)             import nltk
11: (0)             MIN_CORPORA = [
12: (4)                 "brown",  # Required for FastNPExtractor
13: (4)                 "punkt",  # Required for WordTokenizer
14: (4)                 "wordnet",  # Required for lemmatization
15: (4)                 "averaged_perceptron_tagger",  # Required for NLTKTagger
16: (0)             ]
17: (0)             ADDITIONAL_CORPORA = [
18: (4)                 "conll2000",  # Required for ConllExtractor
19: (4)                 "movie_reviews",  # Required for NaiveBayesAnalyzer
20: (0)             ]
21: (0)             ALL_CORPORA = MIN_CORPORA + ADDITIONAL_CORPORA
22: (0)             def download_lite():
23: (4)                 for each in MIN_CORPORA:
24: (8)                     nltk.download(each)
25: (0)             def download_all():
26: (4)                 for each in ALL_CORPORA:
27: (8)                     nltk.download(each)
28: (0)             def main():
29: (4)                 if "lite" in sys.argv:
30: (8)                     download_lite()
31: (4)                 else:
32: (8)                     download_all()
33: (4)                 print("Finished.")
34: (0)             if __name__ == "__main__":
35: (4)                 main()

----------------------------------------

File 14 - . \utils.py:

1: (0)              import re
2: (0)              import string
3: (0)              PUNCTUATION_REGEX = re.compile(f"[{re.escape(string.punctuation)}]")
4: (0)              def strip_punc(s, all=False):
5: (4)                  """Removes punctuation from a string.
6: (4)                  :param s: The string.
7: (4)                  :param all: Remove all punctuation. If False, only removes punctuation from
8: (8)                      the ends of the string.
9: (4)                  """
10: (4)                 if all:
11: (8)                     return PUNCTUATION_REGEX.sub("", s.strip())
12: (4)                 else:
13: (8)                     return s.strip().strip(string.punctuation)
14: (0)             def lowerstrip(s, all=False):
15: (4)                 """Makes text all lowercase and strips punctuation and whitespace.
16: (4)                 :param s: The string.
17: (4)                 :param all: Remove all punctuation. If False, only removes punctuation from
18: (8)                     the ends of the string.
19: (4)                 """
20: (4)                 return strip_punc(s.lower().strip(), all=all)
21: (0)             def tree2str(tree, concat=" "):
22: (4)                 """Convert a nltk.tree.Tree to a string.
23: (4)                 For example:
24: (8)                     (NP a/DT beautiful/JJ new/JJ dashboard/NN) -> "a beautiful dashboard"
25: (4)                 """
26: (4)                 return concat.join([word for (word, tag) in tree])
27: (0)             def filter_insignificant(chunk, tag_suffixes=("DT", "CC", "PRP$", "PRP")):
28: (4)                 """Filter out insignificant (word, tag) tuples from a chunk of text."""
29: (4)                 good = []
30: (4)                 for word, tag in chunk:
31: (8)                     ok = True
32: (8)                     for suffix in tag_suffixes:
33: (12)                        if tag.endswith(suffix):
34: (16)                            ok = False
35: (16)                            break
36: (8)                     if ok:
37: (12)                        good.append((word, tag))
38: (4)                 return good
39: (0)             def is_filelike(obj):
40: (4)                 """Return whether ``obj`` is a file-like object."""
41: (4)                 return hasattr(obj, "read")

----------------------------------------

File 15 - . \taggers.py:

1: (0)              """Default taggers to the English taggers for backwards incompatibility, so you
2: (0)              can still do
3: (0)              >>> from textblob.taggers import NLTKTagger
4: (0)              which is equivalent to
5: (0)              >>> from textblob.en.taggers import NLTKTagger
6: (0)              """
7: (0)              from textblob.base import BaseTagger
8: (0)              from textblob.en.taggers import NLTKTagger, PatternTagger
9: (0)              __all__ = [
10: (4)                 "BaseTagger",
11: (4)                 "PatternTagger",
12: (4)                 "NLTKTagger",
13: (0)             ]

----------------------------------------

File 16 - . \wordnet.py:

1: (0)              """Wordnet interface. Contains classes for creating Synsets and Lemmas
2: (0)              directly.
3: (0)              .. versionadded:: 0.7.0
4: (0)              """
5: (0)              import nltk
6: (0)              #: wordnet module from nltk
7: (0)              wordnet = nltk.corpus.wordnet
8: (0)              #: Synset constructor
9: (0)              Synset = nltk.corpus.wordnet.synset
10: (0)             #: Lemma constructor
11: (0)             Lemma = nltk.corpus.wordnet.lemma
12: (0)             # Part of speech constants
13: (0)             VERB, NOUN, ADJ, ADV = wordnet.VERB, wordnet.NOUN, wordnet.ADJ, wordnet.ADV

----------------------------------------

File 17 - .\en \__init__.py:

1: (0)              """This file is based on pattern.en. See the bundled NOTICE file for
2: (0)              license information.
3: (0)              """
4: (0)              import os
5: (0)              from textblob._text import CHUNK, PENN, PNP, POS, UNIVERSAL, WORD, Lexicon, Spelling
6: (0)              from textblob._text import Parser as _Parser
7: (0)              from textblob._text import Sentiment as _Sentiment
8: (0)              try:
9: (4)                  MODULE = os.path.dirname(os.path.abspath(__file__))
10: (0)             except:
11: (4)                 MODULE = ""
12: (0)             spelling = Spelling(path=os.path.join(MODULE, "en-spelling.txt"))
13: (0)             # --- ENGLISH PARSER --------------------------------------------------------------------------------
14: (0)             def find_lemmata(tokens):
15: (4)                 """Annotates the tokens with lemmata for plural nouns and conjugated verbs,
16: (4)                 where each token is a [word, part-of-speech] list.
17: (4)                 """
18: (4)                 for token in tokens:
19: (8)                     word, pos, lemma = token[0], token[1], token[0]
20: (8)                     # cats => cat
21: (8)                     if pos == "NNS":
22: (12)                        lemma = singularize(word)
23: (8)                     # sat => sit
24: (8)                     if pos.startswith(("VB", "MD")):
25: (12)                        lemma = conjugate(word, INFINITIVE) or word
26: (8)                     token.append(lemma.lower())
27: (4)                 return tokens
28: (0)             class Parser(_Parser):
29: (4)                 def find_lemmata(self, tokens, **kwargs):
30: (8)                     return find_lemmata(tokens)
31: (4)                 def find_tags(self, tokens, **kwargs):
32: (8)                     if kwargs.get("tagset") in (PENN, None):
33: (12)                        kwargs.setdefault("map", lambda token, tag: (token, tag))
34: (8)                     if kwargs.get("tagset") == UNIVERSAL:
35: (12)                        kwargs.setdefault(
36: (16)                            "map", lambda token, tag: penntreebank2universal(token, tag)
37: (12)                        )
38: (8)                     return _Parser.find_tags(self, tokens, **kwargs)
39: (0)             class Sentiment(_Sentiment):
40: (4)                 def load(self, path=None):
41: (8)                     _Sentiment.load(self, path)
42: (8)                     # Map "terrible" to adverb "terribly" (+1% accuracy)
43: (8)                     if not path:
44: (12)                        for w, pos in list(dict.items(self)):
45: (16)                            if "JJ" in pos:
46: (20)                                if w.endswith("y"):
47: (24)                                    w = w[:-1] + "i"
48: (20)                                if w.endswith("le"):
49: (24)                                    w = w[:-2]
50: (20)                                p, s, i = pos["JJ"]
51: (20)                                self.annotate(w + "ly", "RB", p, s, i)
52: (0)             lexicon = Lexicon(
53: (4)                 path=os.path.join(MODULE, "en-lexicon.txt"),
54: (4)                 morphology=os.path.join(MODULE, "en-morphology.txt"),
55: (4)                 context=os.path.join(MODULE, "en-context.txt"),
56: (4)                 entities=os.path.join(MODULE, "en-entities.txt"),
57: (4)                 language="en",
58: (0)             )
59: (0)             parser = Parser(lexicon=lexicon, default=("NN", "NNP", "CD"), language="en")
60: (0)             sentiment = Sentiment(
61: (4)                 path=os.path.join(MODULE, "en-sentiment.xml"),
62: (4)                 synset="wordnet_id",
63: (4)                 negations=("no", "not", "n't", "never"),
64: (4)                 modifiers=("RB",),
65: (4)                 modifier=lambda w: w.endswith("ly"),
66: (4)                 tokenizer=parser.find_tokens,
67: (4)                 language="en",
68: (0)             )
69: (0)             def tokenize(s, *args, **kwargs):
70: (4)                 """Returns a list of sentences, where punctuation marks have been split from words."""
71: (4)                 return parser.find_tokens(str(s), *args, **kwargs)
72: (0)             def parse(s, *args, **kwargs):
73: (4)                 """Returns a tagged str string."""
74: (4)                 return parser.parse(str(s), *args, **kwargs)
75: (0)             def parsetree(s, *args, **kwargs):
76: (4)                 """Returns a parsed Text from the given string."""
77: (4)                 return Text(parse(str(s), *args, **kwargs))
78: (0)             def split(s, token=None):
79: (4)                 """Returns a parsed Text from the given parsed string."""
80: (4)                 if token is None:
81: (8)                     token = [WORD, POS, CHUNK, PNP]
82: (4)                 return Text(str(s), token)
83: (0)             def tag(s, tokenize=True, encoding="utf-8"):
84: (4)                 """Returns a list of (token, tag)-tuples from the given string."""
85: (4)                 tags = []
86: (4)                 for sentence in parse(s, tokenize, True, False, False, False, encoding).split():
87: (8)                     for token in sentence:
88: (12)                        tags.append((token[0], token[1]))
89: (4)                 return tags
90: (0)             def suggest(w):
91: (4)                 """Returns a list of (word, confidence)-tuples of spelling corrections."""
92: (4)                 return spelling.suggest(w)
93: (0)             def polarity(s, **kwargs):
94: (4)                 """Returns the sentence polarity (positive/negative) between -1.0 and 1.0."""
95: (4)                 return sentiment(str(s), **kwargs)[0]
96: (0)             def subjectivity(s, **kwargs):
97: (4)                 """Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0."""
98: (4)                 return sentiment(str(s), **kwargs)[1]
99: (0)             def positive(s, threshold=0.1, **kwargs):
100: (4)                """Returns True if the given sentence has a positive sentiment (polarity >= threshold)."""
101: (4)                return polarity(str(s), **kwargs) >= threshold

----------------------------------------

File 18 - . \sentiments.py:

1: (0)              """Default sentiment analyzers are English for backwards compatibility, so
2: (0)              you can still do
3: (0)              >>> from textblob.sentiments import PatternAnalyzer
4: (0)              which is equivalent to
5: (0)              >>> from textblob.en.sentiments import PatternAnalyzer
6: (0)              """
7: (0)              from textblob.base import BaseSentimentAnalyzer
8: (0)              from textblob.en.sentiments import (
9: (4)                  CONTINUOUS,
10: (4)                 DISCRETE,
11: (4)                 NaiveBayesAnalyzer,
12: (4)                 PatternAnalyzer,
13: (0)             )
14: (0)             __all__ = [
15: (4)                 "BaseSentimentAnalyzer",
16: (4)                 "DISCRETE",
17: (4)                 "CONTINUOUS",
18: (4)                 "PatternAnalyzer",
19: (4)                 "NaiveBayesAnalyzer",
20: (0)             ]

----------------------------------------

File 19 - . \tokenizers.py:

1: (0)              """Various tokenizer implementations.
2: (0)              .. versionadded:: 0.4.0
3: (0)              """
4: (0)              from itertools import chain
5: (0)              import nltk
6: (0)              from textblob.base import BaseTokenizer
7: (0)              from textblob.decorators import requires_nltk_corpus
8: (0)              from textblob.utils import strip_punc
9: (0)              class WordTokenizer(BaseTokenizer):
10: (4)                 """NLTK's recommended word tokenizer (currently the TreeBankTokenizer).
11: (4)                 Uses regular expressions to tokenize text. Assumes text has already been
12: (4)                 segmented into sentences.
13: (4)                 Performs the following steps:
14: (4)                 * split standard contractions, e.g. don't -> do n't
15: (4)                 * split commas and single quotes
16: (4)                 * separate periods that appear at the end of line
17: (4)                 """
18: (4)                 def tokenize(self, text, include_punc=True):
19: (8)                     """Return a list of word tokens.
20: (8)                     :param text: string of text.
21: (8)                     :param include_punc: (optional) whether to
22: (12)                        include punctuation as separate tokens. Default to True.
23: (8)                     """
24: (8)                     tokens = nltk.tokenize.word_tokenize(text)
25: (8)                     if include_punc:
26: (12)                        return tokens
27: (8)                     else:
28: (12)                        # Return each word token
29: (12)                        # Strips punctuation unless the word comes from a contraction
30: (12)                        # e.g. "Let's" => ["Let", "'s"]
31: (12)                        # e.g. "Can't" => ["Ca", "n't"]
32: (12)                        # e.g. "home." => ['home']
33: (12)                        return [
34: (16)                            word if word.startswith("'") else strip_punc(word, all=False)
35: (16)                            for word in tokens
36: (16)                            if strip_punc(word, all=False)
37: (12)                        ]
38: (0)             class SentenceTokenizer(BaseTokenizer):
39: (4)                 """NLTK's sentence tokenizer (currently PunktSentenceTokenizer).
40: (4)                 Uses an unsupervised algorithm to build a model for abbreviation words,
41: (4)                 collocations, and words that start sentences,
42: (4)                 then uses that to find sentence boundaries.
43: (4)                 """
44: (4)                 @requires_nltk_corpus
45: (4)                 def tokenize(self, text):
46: (8)                     """Return a list of sentences."""
47: (8)                     return nltk.tokenize.sent_tokenize(text)
48: (0)             #: Convenience function for tokenizing sentences
49: (0)             sent_tokenize = SentenceTokenizer().itokenize
50: (0)             _word_tokenizer = WordTokenizer()  # Singleton word tokenizer
51: (0)             def word_tokenize(text, include_punc=True, *args, **kwargs):
52: (4)                 """Convenience function for tokenizing text into words.
53: (4)                 NOTE: NLTK's word tokenizer expects sentences as input, so the text will be
54: (4)                 tokenized to sentences before being tokenized to words.
55: (4)                 """
56: (4)                 words = chain.from_iterable(
57: (8)                     _word_tokenizer.itokenize(sentence, include_punc, *args, **kwargs)
58: (8)                     for sentence in sent_tokenize(text)
59: (4)                 )
60: (4)                 return words

----------------------------------------

File 20 - .\en \inflect.py:

1: (0)              """The pluralize and singular methods from the pattern library.
2: (0)              Licenced under the BSD.
3: (0)              See here https://github.com/clips/pattern/blob/master/LICENSE.txt for
4: (0)              complete license information.
5: (0)              """
6: (0)              import re
7: (0)              VERB, NOUN, ADJECTIVE, ADVERB = "VB", "NN", "JJ", "RB"
8: (0)              #### PLURALIZE #####################################################################################
9: (0)              # Based on "An Algorithmic Approach to English Pluralization" by Damian Conway:
10: (0)             # http://www.csse.monash.edu.au/~damian/papers/HTML/Plurals.html
11: (0)             # Prepositions are used to solve things like
12: (0)             # "mother-in-law" or "man at arms"
13: (0)             plural_prepositions = [
14: (4)                 "about",
15: (4)                 "above",
16: (4)                 "across",
17: (4)                 "after",
18: (4)                 "among",
19: (4)                 "around",
20: (4)                 "at",
21: (4)                 "athwart",
22: (4)                 "before",
23: (4)                 "behind",
24: (4)                 "below",
25: (4)                 "beneath",
26: (4)                 "beside",
27: (4)                 "besides",
28: (4)                 "between",
29: (4)                 "betwixt",
30: (4)                 "beyond",
31: (4)                 "but",
32: (4)                 "by",
33: (4)                 "during",
34: (4)                 "except",
35: (4)                 "for",
36: (4)                 "from",
37: (4)                 "in",
38: (4)                 "into",
39: (4)                 "near",
40: (4)                 "of",
41: (4)                 "off",
42: (4)                 "on",
43: (4)                 "onto",
44: (4)                 "out",
45: (4)                 "over",
46: (4)                 "since",
47: (4)                 "till",
48: (4)                 "to",
49: (4)                 "under",
50: (4)                 "until",
51: (4)                 "unto",
52: (4)                 "upon",
53: (4)                 "with",
54: (0)             ]
55: (0)             # Inflection rules that are either general,
56: (0)             # or apply to a certain category of words,
57: (0)             # or apply to a certain category of words only in classical mode,
58: (0)             # or apply only in classical mode.
59: (0)             # Each rule consists of:
60: (0)             # suffix, inflection, category and classic flag.
61: (0)             plural_rules = [
62: (4)                 # 0) Indefinite articles and demonstratives.
63: (4)                 [
64: (8)                     ["^a$|^an$", "some", None, False],
65: (8)                     ["^this$", "these", None, False],
66: (8)                     ["^that$", "those", None, False],
67: (8)                     ["^any$", "all", None, False],
68: (4)                 ],
69: (4)                 # 1) Possessive adjectives.
70: (4)                 # Overlaps with 1/ for "his" and "its".
71: (4)                 # Overlaps with 2/ for "her".
72: (4)                 [
73: (8)                     ["^my$", "our", None, False],
74: (8)                     ["^your$|^thy$", "your", None, False],
75: (8)                     ["^her$|^his$|^its$|^their$", "their", None, False],
76: (4)                 ],
77: (4)                 # 2) Possessive pronouns.
78: (4)                 [
79: (8)                     ["^mine$", "ours", None, False],
80: (8)                     ["^yours$|^thine$", "yours", None, False],
81: (8)                     ["^hers$|^his$|^its$|^theirs$", "theirs", None, False],
82: (4)                 ],
83: (4)                 # 3) Personal pronouns.
84: (4)                 [
85: (8)                     ["^I$", "we", None, False],
86: (8)                     ["^me$", "us", None, False],
87: (8)                     ["^myself$", "ourselves", None, False],
88: (8)                     ["^you$", "you", None, False],
89: (8)                     ["^thou$|^thee$", "ye", None, False],
90: (8)                     ["^yourself$|^thyself$", "yourself", None, False],
91: (8)                     ["^she$|^he$|^it$|^they$", "they", None, False],
92: (8)                     ["^her$|^him$|^it$|^them$", "them", None, False],
93: (8)                     ["^herself$|^himself$|^itself$|^themself$", "themselves", None, False],
94: (8)                     ["^oneself$", "oneselves", None, False],
95: (4)                 ],
96: (4)                 # 4) Words that do not inflect.
97: (4)                 [
98: (8)                     ["$", "", "uninflected", False],
99: (8)                     ["$", "", "uncountable", False],
100: (8)                    ["fish$", "fish", None, False],
101: (8)                    ["([- ])bass$", "\\1bass", None, False],
102: (8)                    ["ois$", "ois", None, False],
103: (8)                    ["sheep$", "sheep", None, False],
104: (8)                    ["deer$", "deer", None, False],
105: (8)                    ["pox$", "pox", None, False],
106: (8)                    ["([A-Z].*)ese$", "\\1ese", None, False],
107: (8)                    ["itis$", "itis", None, False],
108: (8)                    [
109: (12)                       "(fruct|gluc|galact|lact|ket|malt|rib|sacchar|cellul)ose$",
110: (12)                       "\\1ose",
111: (12)                       None,
112: (12)                       False,
113: (8)                    ],
114: (4)                ],
115: (4)                # 5) Irregular plurals (mongoose, oxen).
116: (4)                [
117: (8)                    ["atlas$", "atlantes", None, True],
118: (8)                    ["atlas$", "atlases", None, False],
119: (8)                    ["beef$", "beeves", None, True],
120: (8)                    ["brother$", "brethren", None, True],
121: (8)                    ["child$", "children", None, False],
122: (8)                    ["corpus$", "corpora", None, True],
123: (8)                    ["corpus$", "corpuses", None, False],
124: (8)                    ["^cow$", "kine", None, True],
125: (8)                    ["ephemeris$", "ephemerides", None, False],
126: (8)                    ["ganglion$", "ganglia", None, True],
127: (8)                    ["genie$", "genii", None, True],
128: (8)                    ["genus$", "genera", None, False],
129: (8)                    ["graffito$", "graffiti", None, False],
130: (8)                    ["loaf$", "loaves", None, False],
131: (8)                    ["money$", "monies", None, True],
132: (8)                    ["mongoose$", "mongooses", None, False],
133: (8)                    ["mythos$", "mythoi", None, False],
134: (8)                    ["octopus$", "octopodes", None, True],
135: (8)                    ["opus$", "opera", None, True],
136: (8)                    ["opus$", "opuses", None, False],
137: (8)                    ["^ox$", "oxen", None, False],
138: (8)                    ["penis$", "penes", None, True],
139: (8)                    ["penis$", "penises", None, False],
140: (8)                    ["soliloquy$", "soliloquies", None, False],
141: (8)                    ["testis$", "testes", None, False],
142: (8)                    ["trilby$", "trilbys", None, False],
143: (8)                    ["turf$", "turves", None, True],
144: (8)                    ["numen$", "numena", None, False],
145: (8)                    ["occiput$", "occipita", None, True],
146: (4)                ],
147: (4)                # 6) Irregular inflections for common suffixes (synopses, mice, men).
148: (4)                [
149: (8)                    ["man$", "men", None, False],
150: (8)                    ["person$", "people", None, False],
151: (8)                    ["([lm])ouse$", "\\1ice", None, False],
152: (8)                    ["tooth$", "teeth", None, False],
153: (8)                    ["goose$", "geese", None, False],
154: (8)                    ["foot$", "feet", None, False],
155: (8)                    ["zoon$", "zoa", None, False],
156: (8)                    ["([csx])is$", "\\1es", None, False],
157: (4)                ],
158: (4)                # 7) Fully assimilated classical inflections (vertebrae, codices).
159: (4)                [
160: (8)                    ["ex$", "ices", "ex-ices", False],
161: (8)                    ["ex$", "ices", "ex-ices-classical", True],
162: (8)                    ["um$", "a", "um-a", False],
163: (8)                    ["um$", "a", "um-a-classical", True],
164: (8)                    ["on$", "a", "on-a", False],
165: (8)                    ["a$", "ae", "a-ae", False],
166: (8)                    ["a$", "ae", "a-ae-classical", True],
167: (4)                ],
168: (4)                # 8) Classical variants of modern inflections (stigmata, soprani).
169: (4)                [
170: (8)                    ["trix$", "trices", None, True],
171: (8)                    ["eau$", "eaux", None, True],
172: (8)                    ["ieu$", "ieu", None, True],
173: (8)                    ["([iay])nx$", "\\1nges", None, True],
174: (8)                    ["en$", "ina", "en-ina-classical", True],
175: (8)                    ["a$", "ata", "a-ata-classical", True],
176: (8)                    ["is$", "ides", "is-ides-classical", True],
177: (8)                    ["us$", "i", "us-i-classical", True],
178: (8)                    ["us$", "us", "us-us-classical", True],
179: (8)                    ["o$", "i", "o-i-classical", True],
180: (8)                    ["$", "i", "-i-classical", True],
181: (8)                    ["$", "im", "-im-classical", True],
182: (4)                ],
183: (4)                # 9) -ch, -sh and -ss and the s-singular group take -es in the plural (churches, classes, lenses).
184: (4)                [
185: (8)                    ["([cs])h$", "\\1hes", None, False],
186: (8)                    ["ss$", "sses", None, False],
187: (8)                    ["x$", "xes", None, False],
188: (8)                    ["s$", "ses", "s-singular", False],
189: (4)                ],
190: (4)                # 10) Certain words ending in -f or -fe take -ves in the plural (lives, wolves).
191: (4)                [
192: (8)                    ["([aeo]l)f$", "\\1ves", None, False],
193: (8)                    ["([^d]ea)f$", "\\1ves", None, False],
194: (8)                    ["arf$", "arves", None, False],
195: (8)                    ["([nlw]i)fe$", "\\1ves", None, False],
196: (4)                ],
197: (4)                # 11) -y takes -ys if preceded by a vowel or when a proper noun,
198: (4)                # but -ies if preceded by a consonant (storeys, Marys, stories).
199: (4)                [
200: (8)                    ["([aeiou])y$", "\\1ys", None, False],
201: (8)                    ["([A-Z].*)y$", "\\1ys", None, False],
202: (8)                    ["y$", "ies", None, False],
203: (4)                ],
204: (4)                # 12) Some words ending in -o take -os, the rest take -oes.
205: (4)                # Words in which the -o is preceded by a vowel always take -os (lassos, potatoes, bamboos).
206: (4)                [
207: (8)                    ["o$", "os", "o-os", False],
208: (8)                    ["([aeiou])o$", "\\1os", None, False],
209: (8)                    ["o$", "oes", None, False],
210: (4)                ],
211: (4)                # 13) Miltary stuff (Major Generals).
212: (4)                [["l$", "ls", "general-generals", False]],
213: (4)                # 14) Otherwise, assume that the plural just adds -s (cats, programmes).
214: (4)                [["$", "s", None, False]],
215: (0)            ]
216: (0)            # For performance, compile the regular expressions only once:
217: (0)            for ruleset in plural_rules:
218: (4)                for rule in ruleset:
219: (8)                    rule[0] = re.compile(rule[0])
220: (0)            # Suffix categories.
221: (0)            plural_categories = {
222: (4)                "uninflected": [
223: (8)                    "aircraft",
224: (8)                    "antelope",
225: (8)                    "bison",
226: (8)                    "bream",
227: (8)                    "breeches",
228: (8)                    "britches",
229: (8)                    "carp",
230: (8)                    "cattle",
231: (8)                    "chassis",
232: (8)                    "clippers",
233: (8)                    "cod",
234: (8)                    "contretemps",
235: (8)                    "corps",
236: (8)                    "debris",
237: (8)                    "diabetes",
238: (8)                    "djinn",
239: (8)                    "eland",
240: (8)                    "elk",
241: (8)                    "flounder",
242: (8)                    "gallows",
243: (8)                    "graffiti",
244: (8)                    "headquarters",
245: (8)                    "herpes",
246: (8)                    "high-jinks",
247: (8)                    "homework",
248: (8)                    "innings",
249: (8)                    "jackanapes",
250: (8)                    "mackerel",
251: (8)                    "measles",
252: (8)                    "mews",
253: (8)                    "moose",
254: (8)                    "mumps",
255: (8)                    "offspring",
256: (8)                    "news",
257: (8)                    "pincers",
258: (8)                    "pliers",
259: (8)                    "proceedings",
260: (8)                    "rabies",
261: (8)                    "salmon",
262: (8)                    "scissors",
263: (8)                    "series",
264: (8)                    "shears",
265: (8)                    "species",
266: (8)                    "swine",
267: (8)                    "trout",
268: (8)                    "tuna",
269: (8)                    "whiting",
270: (8)                    "wildebeest",
271: (4)                ],
272: (4)                "uncountable": [
273: (8)                    "advice",
274: (8)                    "bread",
275: (8)                    "butter",
276: (8)                    "cannabis",
277: (8)                    "cheese",
278: (8)                    "electricity",
279: (8)                    "equipment",
280: (8)                    "fruit",
281: (8)                    "furniture",
282: (8)                    "garbage",
283: (8)                    "gravel",
284: (8)                    "happiness",
285: (8)                    "information",
286: (8)                    "ketchup",
287: (8)                    "knowledge",
288: (8)                    "love",
289: (8)                    "luggage",
290: (8)                    "mathematics",
291: (8)                    "mayonnaise",
292: (8)                    "meat",
293: (8)                    "mustard",
294: (8)                    "news",
295: (8)                    "progress",
296: (8)                    "research",
297: (8)                    "rice",
298: (8)                    "sand",
299: (8)                    "software",
300: (8)                    "understanding",
301: (8)                    "water",
302: (4)                ],
303: (4)                "s-singular": [
304: (8)                    "acropolis",
305: (8)                    "aegis",
306: (8)                    "alias",
307: (8)                    "asbestos",
308: (8)                    "bathos",
309: (8)                    "bias",
310: (8)                    "bus",
311: (8)                    "caddis",
312: (8)                    "canvas",
313: (8)                    "chaos",
314: (8)                    "christmas",
315: (8)                    "cosmos",
316: (8)                    "dais",
317: (8)                    "digitalis",
318: (8)                    "epidermis",
319: (8)                    "ethos",
320: (8)                    "gas",
321: (8)                    "glottis",
322: (8)                    "ibis",
323: (8)                    "lens",
324: (8)                    "mantis",
325: (8)                    "marquis",
326: (8)                    "metropolis",
327: (8)                    "pathos",
328: (8)                    "pelvis",
329: (8)                    "polis",
330: (8)                    "rhinoceros",
331: (8)                    "sassafras",
332: (8)                    "trellis",
333: (4)                ],
334: (4)                "ex-ices": ["codex", "murex", "silex"],
335: (4)                "ex-ices-classical": [
336: (8)                    "apex",
337: (8)                    "cortex",
338: (8)                    "index",
339: (8)                    "latex",
340: (8)                    "pontifex",
341: (8)                    "simplex",
342: (8)                    "vertex",
343: (8)                    "vortex",
344: (4)                ],
345: (4)                "um-a": [
346: (8)                    "agendum",
347: (8)                    "bacterium",
348: (8)                    "candelabrum",
349: (8)                    "datum",
350: (8)                    "desideratum",
351: (8)                    "erratum",
352: (8)                    "extremum",
353: (8)                    "ovum",
354: (8)                    "stratum",
355: (4)                ],
356: (4)                "um-a-classical": [
357: (8)                    "aquarium",
358: (8)                    "compendium",
359: (8)                    "consortium",
360: (8)                    "cranium",
361: (8)                    "curriculum",
362: (8)                    "dictum",
363: (8)                    "emporium",
364: (8)                    "enconium",
365: (8)                    "gymnasium",
366: (8)                    "honorarium",
367: (8)                    "interregnum",
368: (8)                    "lustrum",
369: (8)                    "maximum",
370: (8)                    "medium",
371: (8)                    "memorandum",
372: (8)                    "millenium",
373: (8)                    "minimum",
374: (8)                    "momentum",
375: (8)                    "optimum",
376: (8)                    "phylum",
377: (8)                    "quantum",
378: (8)                    "rostrum",
379: (8)                    "spectrum",
380: (8)                    "speculum",
381: (8)                    "stadium",
382: (8)                    "trapezium",
383: (8)                    "ultimatum",
384: (8)                    "vacuum",
385: (8)                    "velum",
386: (4)                ],
387: (4)                "on-a": [
388: (8)                    "aphelion",
389: (8)                    "asyndeton",
390: (8)                    "criterion",
391: (8)                    "hyperbaton",
392: (8)                    "noumenon",
393: (8)                    "organon",
394: (8)                    "perihelion",
395: (8)                    "phenomenon",
396: (8)                    "prolegomenon",
397: (4)                ],
398: (4)                "a-ae": ["alga", "alumna", "vertebra"],
399: (4)                "a-ae-classical": [
400: (8)                    "abscissa",
401: (8)                    "amoeba",
402: (8)                    "antenna",
403: (8)                    "aurora",
404: (8)                    "formula",
405: (8)                    "hydra",
406: (8)                    "hyperbola",
407: (8)                    "lacuna",
408: (8)                    "medusa",
409: (8)                    "nebula",
410: (8)                    "nova",
411: (8)                    "parabola",
412: (4)                ],
413: (4)                "en-ina-classical": ["foramen", "lumen", "stamen"],
414: (4)                "a-ata-classical": [
415: (8)                    "anathema",
416: (8)                    "bema",
417: (8)                    "carcinoma",
418: (8)                    "charisma",
419: (8)                    "diploma",
420: (8)                    "dogma",
421: (8)                    "drama",
422: (8)                    "edema",
423: (8)                    "enema",
424: (8)                    "enigma",
425: (8)                    "gumma",
426: (8)                    "lemma",
427: (8)                    "lymphoma",
428: (8)                    "magma",
429: (8)                    "melisma",
430: (8)                    "miasma",
431: (8)                    "oedema",
432: (8)                    "sarcoma",
433: (8)                    "schema",
434: (8)                    "soma",
435: (8)                    "stigma",
436: (8)                    "stoma",
437: (8)                    "trauma",
438: (4)                ],
439: (4)                "is-ides-classical": ["clitoris", "iris"],
440: (4)                "us-i-classical": [
441: (8)                    "focus",
442: (8)                    "fungus",
443: (8)                    "genius",
444: (8)                    "incubus",
445: (8)                    "nimbus",
446: (8)                    "nucleolus",
447: (8)                    "radius",
448: (8)                    "stylus",
449: (8)                    "succubus",
450: (8)                    "torus",
451: (8)                    "umbilicus",
452: (8)                    "uterus",
453: (4)                ],
454: (4)                "us-us-classical": [
455: (8)                    "apparatus",
456: (8)                    "cantus",
457: (8)                    "coitus",
458: (8)                    "hiatus",
459: (8)                    "impetus",
460: (8)                    "nexus",
461: (8)                    "plexus",
462: (8)                    "prospectus",
463: (8)                    "sinus",
464: (8)                    "status",
465: (4)                ],
466: (4)                "o-i-classical": [
467: (8)                    "alto",
468: (8)                    "basso",
469: (8)                    "canto",
470: (8)                    "contralto",
471: (8)                    "crescendo",
472: (8)                    "solo",
473: (8)                    "soprano",
474: (8)                    "tempo",
475: (4)                ],
476: (4)                "-i-classical": ["afreet", "afrit", "efreet"],
477: (4)                "-im-classical": ["cherub", "goy", "seraph"],
478: (4)                "o-os": [
479: (8)                    "albino",
480: (8)                    "archipelago",
481: (8)                    "armadillo",
482: (8)                    "commando",
483: (8)                    "ditto",
484: (8)                    "dynamo",
485: (8)                    "embryo",
486: (8)                    "fiasco",
487: (8)                    "generalissimo",
488: (8)                    "ghetto",
489: (8)                    "guano",
490: (8)                    "inferno",
491: (8)                    "jumbo",
492: (8)                    "lingo",
493: (8)                    "lumbago",
494: (8)                    "magneto",
495: (8)                    "manifesto",
496: (8)                    "medico",
497: (8)                    "octavo",
498: (8)                    "photo",
499: (8)                    "pro",
500: (8)                    "quarto",
501: (8)                    "rhino",
502: (8)                    "stylo",
503: (4)                ],
504: (4)                "general-generals": [
505: (8)                    "Adjutant",
506: (8)                    "Brigadier",
507: (8)                    "Lieutenant",
508: (8)                    "Major",
509: (8)                    "Quartermaster",
510: (8)                    "adjutant",
511: (8)                    "brigadier",
512: (8)                    "lieutenant",
513: (8)                    "major",
514: (8)                    "quartermaster",
515: (4)                ],
516: (0)            }
517: (0)            def pluralize(word, pos=NOUN, custom=None, classical=True):
518: (4)                """Returns the plural of a given word.
519: (4)                For example: child -> children.
520: (4)                Handles nouns and adjectives, using classical inflection by default
521: (4)                (e.g. where "matrix" pluralizes to "matrices" instead of "matrixes").
522: (4)                The custom dictionary is for user-defined replacements.
523: (4)                """
524: (4)                if custom is None:
525: (8)                    custom = {}
526: (4)                if word in custom:
527: (8)                    return custom[word]
528: (4)                # Recursion of genitives.
529: (4)                # Remove the apostrophe and any trailing -s,
530: (4)                # form the plural of the resultant noun, and then append an apostrophe (dog's -> dogs').
531: (4)                if word.endswith("'") or word.endswith("'s"):
532: (8)                    owner = word.rstrip("'s")
533: (8)                    owners = pluralize(owner, pos, custom, classical)
534: (8)                    if owners.endswith("s"):
535: (12)                       return owners + "'"
536: (8)                    else:
537: (12)                       return owners + "'s"
538: (4)                # Recursion of compound words
539: (4)                # (Postmasters General, mothers-in-law, Roman deities).
540: (4)                words = word.replace("-", " ").split(" ")
541: (4)                if len(words) > 1:
542: (8)                    if (
543: (12)                       words[1] == "general"
544: (12)                       or words[1] == "General"
545: (12)                       and words[0] not in plural_categories["general-generals"]
546: (8)                    ):
547: (12)                       return word.replace(words[0], pluralize(words[0], pos, custom, classical))
548: (8)                    elif words[1] in plural_prepositions:
549: (12)                       return word.replace(words[0], pluralize(words[0], pos, custom, classical))
550: (8)                    else:
551: (12)                       return word.replace(words[-1], pluralize(words[-1], pos, custom, classical))
552: (4)                # Only a very few number of adjectives inflect.
553: (4)                n = list(range(len(plural_rules)))
554: (4)                if pos.startswith(ADJECTIVE):
555: (8)                    n = [0, 1]
556: (4)                # Apply pluralization rules.
557: (4)                for i in n:
558: (8)                    ruleset = plural_rules[i]
559: (8)                    for rule in ruleset:
560: (12)                       suffix, inflection, category, classic = rule
561: (12)                       # A general rule, or a classic rule in classical mode.
562: (12)                       if category is None:
563: (16)                           if not classic or (classic and classical):
564: (20)                               if suffix.search(word) is not None:
565: (24)                                   return suffix.sub(inflection, word)
566: (12)                       # A rule relating to a specific category of words.
567: (12)                       if category is not None:
568: (16)                           if word in plural_categories[category] and (
569: (20)                               not classic or (classic and classical)
570: (16)                           ):
571: (20)                               if suffix.search(word) is not None:
572: (24)                                   return suffix.sub(inflection, word)
573: (0)            #### SINGULARIZE ###################################################################################
574: (0)            # Adapted from Bermi Ferrer's Inflector for Python:
575: (0)            # http://www.bermi.org/inflector/
576: (0)            # Copyright (c) 2006 Bermi Ferrer Martinez
577: (0)            # Permission is hereby granted, free of charge, to any person obtaining a copy
578: (0)            # of this software to deal in this software without restriction, including
579: (0)            # without limitation the rights to use, copy, modify, merge, publish,
580: (0)            # distribute, sublicense, and/or sell copies of this software, and to permit
581: (0)            # persons to whom this software is furnished to do so, subject to the following
582: (0)            # condition:
583: (0)            #
584: (0)            # THIS SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
585: (0)            # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
586: (0)            # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
587: (0)            # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
588: (0)            # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
589: (0)            # OUT OF OR IN CONNECTION WITH THIS SOFTWARE OR THE USE OR OTHER DEALINGS IN
590: (0)            # THIS SOFTWARE.
591: (0)            singular_rules = [
592: (4)                ["(?i)(.)ae$", "\\1a"],
593: (4)                ["(?i)(.)itis$", "\\1itis"],
594: (4)                ["(?i)(.)eaux$", "\\1eau"],
595: (4)                ["(?i)(quiz)zes$", "\\1"],
596: (4)                ["(?i)(matr)ices$", "\\1ix"],
597: (4)                ["(?i)(ap|vert|ind)ices$", "\\1ex"],
598: (4)                ["(?i)^(ox)en", "\\1"],
599: (4)                ["(?i)(alias|status)es$", "\\1"],
600: (4)                ["(?i)([octop|vir])i$", "\\1us"],
601: (4)                ["(?i)(cris|ax|test)es$", "\\1is"],
602: (4)                ["(?i)(shoe)s$", "\\1"],
603: (4)                ["(?i)(o)es$", "\\1"],
604: (4)                ["(?i)(bus)es$", "\\1"],
605: (4)                ["(?i)([m|l])ice$", "\\1ouse"],
606: (4)                ["(?i)(x|ch|ss|sh)es$", "\\1"],
607: (4)                ["(?i)(m)ovies$", "\\1ovie"],
608: (4)                ["(?i)(.)ombies$", "\\1ombie"],
609: (4)                ["(?i)(s)eries$", "\\1eries"],
610: (4)                ["(?i)([^aeiouy]|qu)ies$", "\\1y"],
611: (4)                # Certain words ending in -f or -fe take -ves in the plural (lives, wolves).
612: (4)                ["([aeo]l)ves$", "\\1f"],
613: (4)                ["([^d]ea)ves$", "\\1f"],
614: (4)                ["arves$", "arf"],
615: (4)                ["erves$", "erve"],
616: (4)                ["([nlw]i)ves$", "\\1fe"],
617: (4)                ["(?i)([lr])ves$", "\\1f"],
618: (4)                ["([aeo])ves$", "\\1ve"],
619: (4)                ["(?i)(sive)s$", "\\1"],
620: (4)                ["(?i)(tive)s$", "\\1"],
621: (4)                ["(?i)(hive)s$", "\\1"],
622: (4)                ["(?i)([^f])ves$", "\\1fe"],
623: (4)                # -es suffix.
624: (4)                ["(?i)(^analy)ses$", "\\1sis"],
625: (4)                ["(?i)((a)naly|(b)a|(d)iagno|(p)arenthe|(p)rogno|(s)ynop|(t)he)ses$", "\\1\\2sis"],
626: (4)                ["(?i)(.)opses$", "\\1opsis"],
627: (4)                ["(?i)(.)yses$", "\\1ysis"],
628: (4)                ["(?i)(h|d|r|o|n|b|cl|p)oses$", "\\1ose"],
629: (4)                ["(?i)(fruct|gluc|galact|lact|ket|malt|rib|sacchar|cellul)ose$", "\\1ose"],
630: (4)                ["(?i)(.)oses$", "\\1osis"],
631: (4)                # -a
632: (4)                ["(?i)([ti])a$", "\\1um"],
633: (4)                ["(?i)(n)ews$", "\\1ews"],
634: (4)                ["(?i)s$", ""],
635: (0)            ]
636: (0)            # For performance, compile the regular expressions only once:
637: (0)            for rule in singular_rules:
638: (4)                rule[0] = re.compile(rule[0])
639: (0)            singular_uninflected = [
640: (4)                "aircraft",
641: (4)                "antelope",
642: (4)                "bison",
643: (4)                "bream",
644: (4)                "breeches",
645: (4)                "britches",
646: (4)                "carp",
647: (4)                "cattle",
648: (4)                "chassis",
649: (4)                "clippers",
650: (4)                "cod",
651: (4)                "contretemps",
652: (4)                "corps",
653: (4)                "debris",
654: (4)                "diabetes",
655: (4)                "djinn",
656: (4)                "eland",
657: (4)                "elk",
658: (4)                "flounder",
659: (4)                "gallows",
660: (4)                "georgia",
661: (4)                "graffiti",
662: (4)                "headquarters",
663: (4)                "herpes",
664: (4)                "high-jinks",
665: (4)                "homework",
666: (4)                "innings",
667: (4)                "jackanapes",
668: (4)                "mackerel",
669: (4)                "measles",
670: (4)                "mews",
671: (4)                "moose",
672: (4)                "mumps",
673: (4)                "news",
674: (4)                "offspring",
675: (4)                "pincers",
676: (4)                "pliers",
677: (4)                "proceedings",
678: (4)                "rabies",
679: (4)                "salmon",
680: (4)                "scissors",
681: (4)                "series",
682: (4)                "shears",
683: (4)                "species",
684: (4)                "swine",
685: (4)                "swiss",
686: (4)                "trout",
687: (4)                "tuna",
688: (4)                "whiting",
689: (4)                "wildebeest",
690: (0)            ]
691: (0)            singular_uncountable = [
692: (4)                "advice",
693: (4)                "bread",
694: (4)                "butter",
695: (4)                "cannabis",
696: (4)                "cheese",
697: (4)                "electricity",
698: (4)                "equipment",
699: (4)                "fruit",
700: (4)                "furniture",
701: (4)                "garbage",
702: (4)                "gravel",
703: (4)                "happiness",
704: (4)                "information",
705: (4)                "ketchup",
706: (4)                "knowledge",
707: (4)                "love",
708: (4)                "luggage",
709: (4)                "mathematics",
710: (4)                "mayonnaise",
711: (4)                "meat",
712: (4)                "mustard",
713: (4)                "news",
714: (4)                "progress",
715: (4)                "research",
716: (4)                "rice",
717: (4)                "sand",
718: (4)                "software",
719: (4)                "understanding",
720: (4)                "water",
721: (0)            ]
722: (0)            singular_ie = [
723: (4)                "algerie",
724: (4)                "auntie",
725: (4)                "beanie",
726: (4)                "birdie",
727: (4)                "bogie",
728: (4)                "bombie",
729: (4)                "bookie",
730: (4)                "collie",
731: (4)                "cookie",
732: (4)                "cutie",
733: (4)                "doggie",
734: (4)                "eyrie",
735: (4)                "freebie",
736: (4)                "goonie",
737: (4)                "groupie",
738: (4)                "hankie",
739: (4)                "hippie",
740: (4)                "hoagie",
741: (4)                "hottie",
742: (4)                "indie",
743: (4)                "junkie",
744: (4)                "laddie",
745: (4)                "laramie",
746: (4)                "lingerie",
747: (4)                "meanie",
748: (4)                "nightie",
749: (4)                "oldie",
750: (4)                "^pie",
751: (4)                "pixie",
752: (4)                "quickie",
753: (4)                "reverie",
754: (4)                "rookie",
755: (4)                "softie",
756: (4)                "sortie",
757: (4)                "stoolie",
758: (4)                "sweetie",
759: (4)                "techie",
760: (4)                "^tie",
761: (4)                "toughie",
762: (4)                "valkyrie",
763: (4)                "veggie",
764: (4)                "weenie",
765: (4)                "yuppie",
766: (4)                "zombie",
767: (0)            ]
768: (0)            singular_s = plural_categories["s-singular"]
769: (0)            # key plural, value singular
770: (0)            singular_irregular = {
771: (4)                "men": "man",
772: (4)                "people": "person",
773: (4)                "children": "child",
774: (4)                "sexes": "sex",
775: (4)                "axes": "axe",
776: (4)                "moves": "move",
777: (4)                "teeth": "tooth",
778: (4)                "geese": "goose",
779: (4)                "feet": "foot",
780: (4)                "zoa": "zoon",
781: (4)                "atlantes": "atlas",
782: (4)                "atlases": "atlas",
783: (4)                "beeves": "beef",
784: (4)                "brethren": "brother",
785: (4)                "corpora": "corpus",
786: (4)                "corpuses": "corpus",
787: (4)                "kine": "cow",
788: (4)                "ephemerides": "ephemeris",
789: (4)                "ganglia": "ganglion",
790: (4)                "genii": "genie",
791: (4)                "genera": "genus",
792: (4)                "graffiti": "graffito",
793: (4)                "helves": "helve",
794: (4)                "leaves": "leaf",
795: (4)                "loaves": "loaf",
796: (4)                "monies": "money",
797: (4)                "mongooses": "mongoose",
798: (4)                "mythoi": "mythos",
799: (4)                "octopodes": "octopus",
800: (4)                "opera": "opus",
801: (4)                "opuses": "opus",
802: (4)                "oxen": "ox",
803: (4)                "penes": "penis",
804: (4)                "penises": "penis",
805: (4)                "soliloquies": "soliloquy",
806: (4)                "testes": "testis",
807: (4)                "trilbys": "trilby",
808: (4)                "turves": "turf",
809: (4)                "numena": "numen",
810: (4)                "occipita": "occiput",
811: (4)                "our": "my",
812: (0)            }
813: (0)            def singularize(word, pos=NOUN, custom=None):
814: (4)                if custom is None:
815: (8)                    custom = {}
816: (4)                if word in list(custom.keys()):
817: (8)                    return custom[word]
818: (4)                # Recursion of compound words (e.g. mothers-in-law).
819: (4)                if "-" in word:
820: (8)                    words = word.split("-")
821: (8)                    if len(words) > 1 and words[1] in plural_prepositions:
822: (12)                       return singularize(words[0], pos, custom) + "-" + "-".join(words[1:])
823: (4)                # dogs' => dog's
824: (4)                if word.endswith("'"):
825: (8)                    return singularize(word[:-1]) + "'s"
826: (4)                lower = word.lower()
827: (4)                for w in singular_uninflected:
828: (8)                    if w.endswith(lower):
829: (12)                       return word
830: (4)                for w in singular_uncountable:
831: (8)                    if w.endswith(lower):
832: (12)                       return word
833: (4)                for w in singular_ie:
834: (8)                    if lower.endswith(w + "s"):
835: (12)                       return w
836: (4)                for w in singular_s:
837: (8)                    if lower.endswith(w + "es"):
838: (12)                       return w
839: (4)                for w in list(singular_irregular.keys()):
840: (8)                    if lower.endswith(w):
841: (12)                       return re.sub("(?i)" + w + "$", singular_irregular[w], word)
842: (4)                for rule in singular_rules:
843: (8)                    suffix, inflection = rule
844: (8)                    match = suffix.search(word)
845: (8)                    if match:
846: (12)                       groups = match.groups()
847: (12)                       for k in range(0, len(groups)):
848: (16)                           if groups[k] is None:
849: (20)                               inflection = inflection.replace("\\" + str(k + 1), "")
850: (12)                       return suffix.sub(inflection, word)
851: (4)                return word

----------------------------------------

File 21 - .\en \np_extractors.py:

1: (0)              """Various noun phrase extractors."""
2: (0)              import nltk
3: (0)              from textblob.base import BaseNPExtractor
4: (0)              from textblob.decorators import requires_nltk_corpus
5: (0)              from textblob.taggers import PatternTagger
6: (0)              from textblob.utils import filter_insignificant, tree2str
7: (0)              class ChunkParser(nltk.ChunkParserI):
8: (4)                  def __init__(self):
9: (8)                      self._trained = False
10: (4)                 @requires_nltk_corpus
11: (4)                 def train(self):
12: (8)                     """Train the Chunker on the ConLL-2000 corpus."""
13: (8)                     train_data = [
14: (12)                        [(t, c) for _, t, c in nltk.chunk.tree2conlltags(sent)]
15: (12)                        for sent in nltk.corpus.conll2000.chunked_sents(
16: (16)                            "train.txt", chunk_types=["NP"]
17: (12)                        )
18: (8)                     ]
19: (8)                     unigram_tagger = nltk.UnigramTagger(train_data)
20: (8)                     self.tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)
21: (8)                     self._trained = True
22: (4)                 def parse(self, sentence):
23: (8)                     """Return the parse tree for the sentence."""
24: (8)                     if not self._trained:
25: (12)                        self.train()
26: (8)                     pos_tags = [pos for (word, pos) in sentence]
27: (8)                     tagged_pos_tags = self.tagger.tag(pos_tags)
28: (8)                     chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]
29: (8)                     conlltags = [
30: (12)                        (word, pos, chunktag)
31: (12)                        for ((word, pos), chunktag) in zip(sentence, chunktags)
32: (8)                     ]
33: (8)                     return nltk.chunk.util.conlltags2tree(conlltags)
34: (0)             class ConllExtractor(BaseNPExtractor):
35: (4)                 """A noun phrase extractor that uses chunk parsing trained with the
36: (4)                 ConLL-2000 training corpus.
37: (4)                 """
38: (4)                 POS_TAGGER = PatternTagger()
39: (4)                 # The context-free grammar with which to filter the noun phrases
40: (4)                 CFG = {
41: (8)                     ("NNP", "NNP"): "NNP",
42: (8)                     ("NN", "NN"): "NNI",
43: (8)                     ("NNI", "NN"): "NNI",
44: (8)                     ("JJ", "JJ"): "JJ",
45: (8)                     ("JJ", "NN"): "NNI",
46: (4)                 }
47: (4)                 # POS suffixes that will be ignored
48: (4)                 INSIGNIFICANT_SUFFIXES = ["DT", "CC", "PRP$", "PRP"]
49: (4)                 def __init__(self, parser=None):
50: (8)                     self.parser = ChunkParser() if not parser else parser
51: (4)                 def extract(self, text):
52: (8)                     """Return a list of noun phrases (strings) for body of text."""
53: (8)                     sentences = nltk.tokenize.sent_tokenize(text)
54: (8)                     noun_phrases = []
55: (8)                     for sentence in sentences:
56: (12)                        parsed = self._parse_sentence(sentence)
57: (12)                        # Get the string representation of each subtree that is a
58: (12)                        # noun phrase tree
59: (12)                        phrases = [
60: (16)                            _normalize_tags(filter_insignificant(each, self.INSIGNIFICANT_SUFFIXES))
61: (16)                            for each in parsed
62: (16)                            if isinstance(each, nltk.tree.Tree)
63: (16)                            and each.label() == "NP"
64: (16)                            and len(filter_insignificant(each)) >= 1
65: (16)                            and _is_match(each, cfg=self.CFG)
66: (12)                        ]
67: (12)                        nps = [tree2str(phrase) for phrase in phrases]
68: (12)                        noun_phrases.extend(nps)
69: (8)                     return noun_phrases
70: (4)                 def _parse_sentence(self, sentence):
71: (8)                     """Tag and parse a sentence (a plain, untagged string)."""
72: (8)                     tagged = self.POS_TAGGER.tag(sentence)
73: (8)                     return self.parser.parse(tagged)
74: (0)             class FastNPExtractor(BaseNPExtractor):
75: (4)                 """A fast and simple noun phrase extractor.
76: (4)                 Credit to Shlomi Babluk. Link to original blog post:
77: (8)                     http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/
78: (4)                 """
79: (4)                 CFG = {
80: (8)                     ("NNP", "NNP"): "NNP",
81: (8)                     ("NN", "NN"): "NNI",
82: (8)                     ("NNI", "NN"): "NNI",
83: (8)                     ("JJ", "JJ"): "JJ",
84: (8)                     ("JJ", "NN"): "NNI",
85: (4)                 }
86: (4)                 def __init__(self):
87: (8)                     self._trained = False
88: (4)                 @requires_nltk_corpus
89: (4)                 def train(self):
90: (8)                     train_data = nltk.corpus.brown.tagged_sents(categories="news")
91: (8)                     regexp_tagger = nltk.RegexpTagger(
92: (12)                        [
93: (16)                            (r"^-?[0-9]+(.[0-9]+)?$", "CD"),
94: (16)                            (r"(-|:|;)$", ":"),
95: (16)                            (r"\'*$", "MD"),
96: (16)                            (r"(The|the|A|a|An|an)$", "AT"),
97: (16)                            (r".*able$", "JJ"),
98: (16)                            (r"^[A-Z].*$", "NNP"),
99: (16)                            (r".*ness$", "NN"),
100: (16)                           (r".*ly$", "RB"),
101: (16)                           (r".*s$", "NNS"),
102: (16)                           (r".*ing$", "VBG"),
103: (16)                           (r".*ed$", "VBD"),
104: (16)                           (r".*", "NN"),
105: (12)                       ]
106: (8)                    )
107: (8)                    unigram_tagger = nltk.UnigramTagger(train_data, backoff=regexp_tagger)
108: (8)                    self.tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)
109: (8)                    self._trained = True
110: (8)                    return None
111: (4)                def _tokenize_sentence(self, sentence):
112: (8)                    """Split the sentence into single words/tokens"""
113: (8)                    tokens = nltk.word_tokenize(sentence)
114: (8)                    return tokens
115: (4)                def extract(self, sentence):
116: (8)                    """Return a list of noun phrases (strings) for body of text."""
117: (8)                    if not self._trained:
118: (12)                       self.train()
119: (8)                    tokens = self._tokenize_sentence(sentence)
120: (8)                    tagged = self.tagger.tag(tokens)
121: (8)                    tags = _normalize_tags(tagged)
122: (8)                    merge = True
123: (8)                    while merge:
124: (12)                       merge = False
125: (12)                       for x in range(0, len(tags) - 1):
126: (16)                           t1 = tags[x]
127: (16)                           t2 = tags[x + 1]
128: (16)                           key = t1[1], t2[1]
129: (16)                           value = self.CFG.get(key, "")
130: (16)                           if value:
131: (20)                               merge = True
132: (20)                               tags.pop(x)
133: (20)                               tags.pop(x)
134: (20)                               match = f"{t1[0]} {t2[0]}"
135: (20)                               pos = value
136: (20)                               tags.insert(x, (match, pos))
137: (20)                               break
138: (8)                    matches = [t[0] for t in tags if t[1] in ["NNP", "NNI"]]
139: (8)                    return matches
140: (0)            ### Utility methods ###
141: (0)            def _normalize_tags(chunk):
142: (4)                """Normalize the corpus tags.
143: (4)                ("NN", "NN-PL", "NNS") -> "NN"
144: (4)                """
145: (4)                ret = []
146: (4)                for word, tag in chunk:
147: (8)                    if tag == "NP-TL" or tag == "NP":
148: (12)                       ret.append((word, "NNP"))
149: (12)                       continue
150: (8)                    if tag.endswith("-TL"):
151: (12)                       ret.append((word, tag[:-3]))
152: (12)                       continue
153: (8)                    if tag.endswith("S"):
154: (12)                       ret.append((word, tag[:-1]))
155: (12)                       continue
156: (8)                    ret.append((word, tag))
157: (4)                return ret
158: (0)            def _is_match(tagged_phrase, cfg):
159: (4)                """Return whether or not a tagged phrases matches a context-free grammar."""
160: (4)                copy = list(tagged_phrase)  # A copy of the list
161: (4)                merge = True
162: (4)                while merge:
163: (8)                    merge = False
164: (8)                    for i in range(len(copy) - 1):
165: (12)                       first, second = copy[i], copy[i + 1]
166: (12)                       key = first[1], second[1]  # Tuple of tags e.g. ('NN', 'JJ')
167: (12)                       value = cfg.get(key, None)
168: (12)                       if value:
169: (16)                           merge = True
170: (16)                           copy.pop(i)
171: (16)                           copy.pop(i)
172: (16)                           match = f"{first[0]} {second[0]}"
173: (16)                           pos = value
174: (16)                           copy.insert(i, (match, pos))
175: (16)                           break
176: (4)                match = any([t[1] in ("NNP", "NNI") for t in copy])
177: (4)                return match

----------------------------------------

File 22 - .\en \parsers.py:

1: (0)              """Various parser implementations.
2: (0)              .. versionadded:: 0.6.0
3: (0)              """
4: (0)              from textblob.base import BaseParser
5: (0)              from textblob.en import parse as pattern_parse
6: (0)              class PatternParser(BaseParser):
7: (4)                  """Parser that uses the implementation in Tom de Smedt's pattern library.
8: (4)                  http://www.clips.ua.ac.be/pages/pattern-en#parser
9: (4)                  """
10: (4)                 def parse(self, text):
11: (8)                     """Parses the text."""
12: (8)                     return pattern_parse(text)

----------------------------------------

File 23 - .\en \taggers.py:

1: (0)              """Parts-of-speech tagger implementations."""
2: (0)              import nltk
3: (0)              import textblob as tb
4: (0)              from textblob.base import BaseTagger
5: (0)              from textblob.decorators import requires_nltk_corpus
6: (0)              from textblob.en import tag as pattern_tag
7: (0)              class PatternTagger(BaseTagger):
8: (4)                  """Tagger that uses the implementation in
9: (4)                  Tom de Smedt's pattern library
10: (4)                 (http://www.clips.ua.ac.be/pattern).
11: (4)                 """
12: (4)                 def tag(self, text, tokenize=True):
13: (8)                     """Tag a string or BaseBlob."""
14: (8)                     if not isinstance(text, str):
15: (12)                        text = text.raw
16: (8)                     return pattern_tag(text, tokenize)
17: (0)             class NLTKTagger(BaseTagger):
18: (4)                 """Tagger that uses NLTK's standard TreeBank tagger.
19: (4)                 NOTE: Requires numpy. Not yet supported with PyPy.
20: (4)                 """
21: (4)                 @requires_nltk_corpus
22: (4)                 def tag(self, text):
23: (8)                     """Tag a string or BaseBlob."""
24: (8)                     if isinstance(text, str):
25: (12)                        text = tb.TextBlob(text)
26: (8)                     return nltk.tag.pos_tag(text.tokens)

----------------------------------------

File 24 - .\unicodecsv \__init__.py:

1: (0)              import csv
2: (0)              # http://semver.org/
3: (0)              VERSION = (0, 9, 4)
4: (0)              __version__ = ".".join(map(str, VERSION))
5: (0)              pass_throughs = [
6: (4)                  "register_dialect",
7: (4)                  "unregister_dialect",
8: (4)                  "get_dialect",
9: (4)                  "list_dialects",
10: (4)                 "field_size_limit",
11: (4)                 "Dialect",
12: (4)                 "excel",
13: (4)                 "excel_tab",
14: (4)                 "Sniffer",
15: (4)                 "QUOTE_ALL",
16: (4)                 "QUOTE_MINIMAL",
17: (4)                 "QUOTE_NONNUMERIC",
18: (4)                 "QUOTE_NONE",
19: (4)                 "Error",
20: (0)             ]
21: (0)             __all__ = [
22: (4)                 "reader",
23: (4)                 "writer",
24: (4)                 "DictReader",
25: (4)                 "DictWriter",
26: (0)             ] + pass_throughs
27: (0)             for prop in pass_throughs:
28: (4)                 globals()[prop] = getattr(csv, prop)
29: (0)             def _stringify(s, encoding, errors):
30: (4)                 if s is None:
31: (8)                     return ""
32: (4)                 if isinstance(s, unicode):
33: (8)                     return s.encode(encoding, errors)
34: (4)                 elif isinstance(s, (int, float)):
35: (8)                     pass  # let csv.QUOTE_NONNUMERIC do its thing.
36: (4)                 elif not isinstance(s, str):
37: (8)                     s = str(s)
38: (4)                 return s
39: (0)             def _stringify_list(l, encoding, errors="strict"):
40: (4)                 try:
41: (8)                     return [_stringify(s, encoding, errors) for s in iter(l)]
42: (4)                 except TypeError as e:
43: (8)                     raise csv.Error(str(e))
44: (0)             def _unicodify(s, encoding):
45: (4)                 if s is None:
46: (8)                     return None
47: (4)                 if isinstance(s, (unicode, int, float)):
48: (8)                     return s
49: (4)                 elif isinstance(s, str):
50: (8)                     return s.decode(encoding)
51: (4)                 return s
52: (0)             class UnicodeWriter:
53: (4)                 """
54: (4)                 >>> import unicodecsv
55: (4)                 >>> from cStringIO import StringIO
56: (4)                 >>> f = StringIO()
57: (4)                 >>> w = unicodecsv.writer(f, encoding='utf-8')
58: (4)                 >>> w.writerow((u'é', u'ñ'))
59: (4)                 >>> f.seek(0)
60: (4)                 >>> r = unicodecsv.reader(f, encoding='utf-8')
61: (4)                 >>> row = r.next()
62: (4)                 >>> row[0] == u'é'
63: (4)                 True
64: (4)                 >>> row[1] == u'ñ'
65: (4)                 True
66: (4)                 """
67: (4)                 def __init__(
68: (8)                     self, f, dialect=csv.excel, encoding="utf-8", errors="strict", *args, **kwds
69: (4)                 ):
70: (8)                     self.encoding = encoding
71: (8)                     self.writer = csv.writer(f, dialect, *args, **kwds)
72: (8)                     self.encoding_errors = errors
73: (4)                 def writerow(self, row):
74: (8)                     self.writer.writerow(_stringify_list(row, self.encoding, self.encoding_errors))
75: (4)                 def writerows(self, rows):
76: (8)                     for row in rows:
77: (12)                        self.writerow(row)
78: (4)                 @property
79: (4)                 def dialect(self):
80: (8)                     return self.writer.dialect
81: (0)             writer = UnicodeWriter
82: (0)             class UnicodeReader:
83: (4)                 def __init__(self, f, dialect=None, encoding="utf-8", errors="strict", **kwds):
84: (8)                     format_params = [
85: (12)                        "delimiter",
86: (12)                        "doublequote",
87: (12)                        "escapechar",
88: (12)                        "lineterminator",
89: (12)                        "quotechar",
90: (12)                        "quoting",
91: (12)                        "skipinitialspace",
92: (8)                     ]
93: (8)                     if dialect is None:
94: (12)                        if not any([kwd_name in format_params for kwd_name in kwds.keys()]):
95: (16)                            dialect = csv.excel
96: (8)                     self.reader = csv.reader(f, dialect, **kwds)
97: (8)                     self.encoding = encoding
98: (8)                     self.encoding_errors = errors
99: (4)                 def next(self):
100: (8)                    row = self.reader.next()
101: (8)                    encoding = self.encoding
102: (8)                    encoding_errors = self.encoding_errors
103: (8)                    float_ = float
104: (8)                    unicode_ = unicode
105: (8)                    return [
106: (12)                       (
107: (16)                           value
108: (16)                           if isinstance(value, float_)
109: (16)                           else unicode_(value, encoding, encoding_errors)
110: (12)                       )
111: (12)                       for value in row
112: (8)                    ]
113: (4)                def __iter__(self):
114: (8)                    return self
115: (4)                @property
116: (4)                def dialect(self):
117: (8)                    return self.reader.dialect
118: (4)                @property
119: (4)                def line_num(self):
120: (8)                    return self.reader.line_num
121: (0)            reader = UnicodeReader
122: (0)            class DictWriter(csv.DictWriter):
123: (4)                """
124: (4)                >>> from cStringIO import StringIO
125: (4)                >>> f = StringIO()
126: (4)                >>> w = DictWriter(f, ['a', u'ñ', 'b'], restval=u'î')
127: (4)                >>> w.writerow({'a':'1', u'ñ':'2'})
128: (4)                >>> w.writerow({'a':'1', u'ñ':'2', 'b':u'ø'})
129: (4)                >>> w.writerow({'a':u'é', u'ñ':'2'})
130: (4)                >>> f.seek(0)
131: (4)                >>> r = DictReader(f, fieldnames=['a', u'ñ'], restkey='r')
132: (4)                >>> r.next() == {'a': u'1', u'ñ':'2', 'r': [u'î']}
133: (4)                True
134: (4)                >>> r.next() == {'a': u'1', u'ñ':'2', 'r': [u'\xc3\xb8']}
135: (4)                True
136: (4)                >>> r.next() == {'a': u'\xc3\xa9', u'ñ':'2', 'r': [u'\xc3\xae']}
137: (4)                True
138: (4)                """
139: (4)                def __init__(
140: (8)                    self,
141: (8)                    csvfile,
142: (8)                    fieldnames,
143: (8)                    restval="",
144: (8)                    extrasaction="raise",
145: (8)                    dialect="excel",
146: (8)                    encoding="utf-8",
147: (8)                    errors="strict",
148: (8)                    *args,
149: (8)                    **kwds,
150: (4)                ):
151: (8)                    self.encoding = encoding
152: (8)                    csv.DictWriter.__init__(
153: (12)                       self, csvfile, fieldnames, restval, extrasaction, dialect, *args, **kwds
154: (8)                    )
155: (8)                    self.writer = UnicodeWriter(
156: (12)                       csvfile, dialect, encoding=encoding, errors=errors, *args, **kwds
157: (8)                    )
158: (8)                    self.encoding_errors = errors
159: (4)                def writeheader(self):
160: (8)                    _stringify_list(self.fieldnames, self.encoding, self.encoding_errors)
161: (8)                    header = dict(zip(self.fieldnames, self.fieldnames))
162: (8)                    self.writerow(header)
163: (0)            class DictReader(csv.DictReader):
164: (4)                """
165: (4)                >>> from cStringIO import StringIO
166: (4)                >>> f = StringIO()
167: (4)                >>> w = DictWriter(f, fieldnames=['name', 'place'])
168: (4)                >>> w.writerow({'name': 'Cary Grant', 'place': 'hollywood'})
169: (4)                >>> w.writerow({'name': 'Nathan Brillstone', 'place': u'øLand'})
170: (4)                >>> w.writerow({'name': u'Willam ø. Unicoder', 'place': u'éSpandland'})
171: (4)                >>> f.seek(0)
172: (4)                >>> r = DictReader(f, fieldnames=['name', 'place'])
173: (4)                >>> print r.next() == {'name': 'Cary Grant', 'place': 'hollywood'}
174: (4)                True
175: (4)                >>> print r.next() == {'name': 'Nathan Brillstone', 'place': u'øLand'}
176: (4)                True
177: (4)                >>> print r.next() == {'name': u'Willam ø. Unicoder', 'place': u'éSpandland'}
178: (4)                True
179: (4)                """
180: (4)                def __init__(
181: (8)                    self,
182: (8)                    csvfile,
183: (8)                    fieldnames=None,
184: (8)                    restkey=None,
185: (8)                    restval=None,
186: (8)                    dialect="excel",
187: (8)                    encoding="utf-8",
188: (8)                    errors="strict",
189: (8)                    *args,
190: (8)                    **kwds,
191: (4)                ):
192: (8)                    if fieldnames is not None:
193: (12)                       fieldnames = _stringify_list(fieldnames, encoding)
194: (8)                    csv.DictReader.__init__(
195: (12)                       self, csvfile, fieldnames, restkey, restval, dialect, *args, **kwds
196: (8)                    )
197: (8)                    self.reader = UnicodeReader(
198: (12)                       csvfile, dialect, encoding=encoding, errors=errors, *args, **kwds
199: (8)                    )
200: (8)                    if fieldnames is None and not hasattr(csv.DictReader, "fieldnames"):
201: (12)                       # Python 2.5 fieldnames workaround. (http://bugs.python.org/issue3436)
202: (12)                       reader = UnicodeReader(csvfile, dialect, encoding=encoding, *args, **kwds)
203: (12)                       self.fieldnames = _stringify_list(reader.next(), reader.encoding)
204: (8)                    self.unicode_fieldnames = [_unicodify(f, encoding) for f in self.fieldnames]
205: (8)                    self.unicode_restkey = _unicodify(restkey, encoding)
206: (4)                def next(self):
207: (8)                    row = csv.DictReader.next(self)
208: (8)                    result = dict(
209: (12)                       (uni_key, row[str_key])
210: (12)                       for (str_key, uni_key) in zip(self.fieldnames, self.unicode_fieldnames)
211: (8)                    )
212: (8)                    rest = row.get(self.restkey)
213: (8)                    if rest:
214: (12)                       result[self.unicode_restkey] = rest
215: (8)                    return result

----------------------------------------

File 25 - .\en \sentiments.py:

1: (0)              """Sentiment analysis implementations.
2: (0)              .. versionadded:: 0.5.0
3: (0)              """
4: (0)              from collections import namedtuple
5: (0)              import nltk
6: (0)              from textblob.base import CONTINUOUS, DISCRETE, BaseSentimentAnalyzer
7: (0)              from textblob.decorators import requires_nltk_corpus
8: (0)              from textblob.en import sentiment as pattern_sentiment
9: (0)              from textblob.tokenizers import word_tokenize
10: (0)             class PatternAnalyzer(BaseSentimentAnalyzer):
11: (4)                 """Sentiment analyzer that uses the same implementation as the
12: (4)                 pattern library. Returns results as a named tuple of the form:
13: (4)                 ``Sentiment(polarity, subjectivity, [assessments])``
14: (4)                 where [assessments] is a list of the assessed tokens and their
15: (4)                 polarity and subjectivity scores
16: (4)                 """
17: (4)                 kind = CONTINUOUS
18: (4)                 # This is only here for backwards-compatibility.
19: (4)                 # The return type is actually determined upon calling analyze()
20: (4)                 RETURN_TYPE = namedtuple("Sentiment", ["polarity", "subjectivity"])
21: (4)                 def analyze(self, text, keep_assessments=False):
22: (8)                     """Return the sentiment as a named tuple of the form:
23: (8)                     ``Sentiment(polarity, subjectivity, [assessments])``.
24: (8)                     """
25: (8)                     #: Return type declaration
26: (8)                     if keep_assessments:
27: (12)                        Sentiment = namedtuple(
28: (16)                            "Sentiment", ["polarity", "subjectivity", "assessments"]
29: (12)                        )
30: (12)                        assessments = pattern_sentiment(text).assessments
31: (12)                        polarity, subjectivity = pattern_sentiment(text)
32: (12)                        return Sentiment(polarity, subjectivity, assessments)
33: (8)                     else:
34: (12)                        Sentiment = namedtuple("Sentiment", ["polarity", "subjectivity"])
35: (12)                        return Sentiment(*pattern_sentiment(text))
36: (0)             def _default_feature_extractor(words):
37: (4)                 """Default feature extractor for the NaiveBayesAnalyzer."""
38: (4)                 return dict((word, True) for word in words)
39: (0)             class NaiveBayesAnalyzer(BaseSentimentAnalyzer):
40: (4)                 """Naive Bayes analyzer that is trained on a dataset of movie reviews.
41: (4)                 Returns results as a named tuple of the form:
42: (4)                 ``Sentiment(classification, p_pos, p_neg)``
43: (4)                 :param callable feature_extractor: Function that returns a dictionary of
44: (8)                     features, given a list of words.
45: (4)                 """
46: (4)                 kind = DISCRETE
47: (4)                 #: Return type declaration
48: (4)                 RETURN_TYPE = namedtuple("Sentiment", ["classification", "p_pos", "p_neg"])
49: (4)                 def __init__(self, feature_extractor=_default_feature_extractor):
50: (8)                     super().__init__()
51: (8)                     self._classifier = None
52: (8)                     self.feature_extractor = feature_extractor
53: (4)                 @requires_nltk_corpus
54: (4)                 def train(self):
55: (8)                     """Train the Naive Bayes classifier on the movie review corpus."""
56: (8)                     super().train()
57: (8)                     neg_ids = nltk.corpus.movie_reviews.fileids("neg")
58: (8)                     pos_ids = nltk.corpus.movie_reviews.fileids("pos")
59: (8)                     neg_feats = [
60: (12)                        (
61: (16)                            self.feature_extractor(nltk.corpus.movie_reviews.words(fileids=[f])),
62: (16)                            "neg",
63: (12)                        )
64: (12)                        for f in neg_ids
65: (8)                     ]
66: (8)                     pos_feats = [
67: (12)                        (
68: (16)                            self.feature_extractor(nltk.corpus.movie_reviews.words(fileids=[f])),
69: (16)                            "pos",
70: (12)                        )
71: (12)                        for f in pos_ids
72: (8)                     ]
73: (8)                     train_data = neg_feats + pos_feats
74: (8)                     self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)
75: (4)                 def analyze(self, text):
76: (8)                     """Return the sentiment as a named tuple of the form:
77: (8)                     ``Sentiment(classification, p_pos, p_neg)``
78: (8)                     """
79: (8)                     # Lazily train the classifier
80: (8)                     super().analyze(text)
81: (8)                     tokens = word_tokenize(text, include_punc=False)
82: (8)                     filtered = (t.lower() for t in tokens if len(t) >= 3)
83: (8)                     feats = self.feature_extractor(filtered)
84: (8)                     prob_dist = self._classifier.prob_classify(feats)
85: (8)                     return self.RETURN_TYPE(
86: (12)                        classification=prob_dist.max(),
87: (12)                        p_pos=prob_dist.prob("pos"),
88: (12)                        p_neg=prob_dist.prob("neg"),
89: (8)                     )

----------------------------------------

File 26 - . \SANJOYNATHQHENOMENOLOGYGEOMETRIFYINGTRIGONOMETRYCOMBINER_aligner_20_characters_for_pythons_codes.py:

1: (0)              import os
2: (0)              from datetime import datetime
3: (0)              def get_file_info(root_folder):
4: (4)                  file_info_list = []
5: (4)                  for root, dirs, files in os.walk(root_folder):
6: (8)                      for file in files:
7: (12)                         try:
8: (16)                             # Check if the file is a Python file
9: (16)                             if file.endswith('.py'):
10: (20)                                file_path = os.path.join(root, file)
11: (20)                                # Get file times
12: (20)                                creation_time = datetime.fromtimestamp(os.path.getctime(file_path))
13: (20)                                modified_time = datetime.fromtimestamp(os.path.getmtime(file_path))
14: (20)                                # Get file extension
15: (20)                                file_extension = os.path.splitext(file)[1].lower()
16: (20)                                # Append file info to list
17: (20)                                file_info_list.append([file, file_path, creation_time, modified_time, file_extension, root])
18: (12)                        except Exception as e:
19: (16)                            print(f"Error processing file {file}: {e}")
20: (4)                 # Sort the files by multiple criteria
21: (4)                 file_info_list.sort(key=lambda x: (x[2], x[3], len(x[0]), x[4]))  # Sort by creation, modification time, name length, extension
22: (4)                 return file_info_list
23: (0)             def process_file(file_info_list):
24: (4)                 combined_output = []
25: (4)                 for idx, (file_name, file_path, creation_time, modified_time, file_extension, root) in enumerate(file_info_list):
26: (8)                     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
27: (12)                        content = f.read()
28: (12)                        # Remove Python comments and blank lines
29: (10)                      ###  content = "\n".join([line for line in content.split('\n') if line.strip() and not line.strip().startswith("#")])
30: (12)                        content = "\n".join([line for line in content.split('\n') if line.strip() ])###and not line.strip().startswith("#")
31: (12)                        # Replace tabs with spaces
32: (12)                        content = content.replace('\t', '    ')
33: (12)                        # Process each line
34: (12)                        processed_lines = []
35: (12)                        for i, line in enumerate(content.split('\n')):
36: (16)                            # Count the number of starting blank spaces
37: (16)                            leading_spaces = len(line) - len(line.lstrip(' '))
38: (16)                            # Create the line with line number and leading spaces count
39: (16)                            line_number_str = f"{i+1}: ({leading_spaces})"
40: (16)                            # Calculate padding to align the original code at the 61st character
41: (16)                            padding = ' ' * (20 - len(line_number_str))
42: (16)                            processed_line = f"{line_number_str}{padding}{line}"
43: (16)                            processed_lines.append(processed_line)
44: (12)                        content_with_line_numbers = "\n".join(processed_lines)
45: (12)                        # Add file listing order and line number
46: (12)                        combined_output.append(f"File {idx + 1} - {root} \\{file_name}:\n")
47: (12)                        combined_output.append(content_with_line_numbers)
48: (12)                        combined_output.append("\n" + "-"*40 + "\n")
49: (4)                 return combined_output
50: (0)             # Set the root folder path
51: (0)             root_folder_path = '.'  # Set this to the desired folder
52: (0)             # Get file information and process files
53: (0)             file_info_list = get_file_info(root_folder_path)
54: (0)             combined_output = process_file(file_info_list)
55: (0)             # Save the processed data to an output file
56: (0)             output_file = 'WITHRELPATH_COMMENTSKEPT_SANJOYNATHQHENOMENOLOGYGEOMETRIFYINGTRIGONOMETRY_combined_python_files_20_chars.txt'
57: (0)             with open(output_file, 'w', encoding='utf-8') as logfile:
58: (4)                 logfile.write("\n".join(combined_output))
59: (0)             print(f"Processed file info logged to {output_file}")

----------------------------------------
