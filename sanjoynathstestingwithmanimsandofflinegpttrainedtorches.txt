









Best Pretrained Systems for Generating Parse Trees and Semantic Analysis
To generate parse trees and perform semantic analysis, several state-of-the-art packages and pretrained systems can be employed. Here are some of the best systems you can explore:

1. spaCy:
Overview: spaCy is one of the most popular NLP libraries for syntactic parsing, named entity recognition (NER), and semantic analysis. It is efficient and easy to use for various NLP tasks, including dependency parsing (parse trees).
Pretrained Models: spaCy offers a variety of pretrained models for multiple languages (e.g., en_core_web_sm, en_core_web_md, and en_core_web_lg for English).
Features:
Dependency parsing for generating parse trees
Named entity recognition (NER)
Part-of-speech tagging (POS)
Text classification
Pretrained transformers via spacy-transformers for semantic analysis and more advanced tasks.
Installation:
bash

pip install spacy
python -m spacy download en_core_web_sm
2. Stanford NLP:
Overview: Developed by Stanford University, this library provides a suite of NLP tools, including syntactic parsing, part-of-speech tagging, named entity recognition, and more.
Pretrained Models: Stanford NLP provides several pretrained models, including dependency parsers and constituency parsers (which can generate detailed parse trees).
Features:
Deep syntactic parsing (dependency and constituency parsing)
Named entity recognition
Sentiment analysis
Installation:
bash
pip install stanfordnlp

3. BERT-based Models for Semantic Analysis:
Overview: BERT (Bidirectional Encoder Representations from Transformers) and its derivatives (RoBERTa, DistilBERT) have revolutionized semantic analysis in NLP. These models are primarily used for tasks such as sentence classification, question answering, and extracting meaning from text.
Pretrained Models: Models like bert-base-uncased or bert-large-cased can be fine-tuned for specific semantic tasks.
Semantic Analysis Tasks:
Text classification
Named entity recognition
Question answering (semantic search)
Sentence similarity and entailment
Installation:
bash
pip install transformers

4. AllenNLP:
Overview: AllenNLP is built on top of PyTorch and provides state-of-the-art models for various NLP tasks. It is highly modular and extensible, designed for academic and research purposes.
Pretrained Models: AllenNLP offers various models for parse tree generation, semantic role labeling (SRL), and textual entailment.
Features:
Dependency parsing
Semantic role labeling
Coreference resolution
Named entity recognition
Installation:
bash

pip install allennlp
5. Transformers Library by Hugging Face:
Overview: Hugging Face’s Transformers library provides easy access to hundreds of pretrained models for tasks like text generation, semantic analysis, parsing, etc.
Pretrained Models: Models like T5, BART, and GPT-3 can generate parse trees and perform semantic analysis by fine-tuning them on specific tasks.
Semantic Tasks:
Text generation (transformations and semantic meaning extraction)
Text classification
Question answering
Sentence similarity and entailment
Installation:
bash

pip install transformers
Reference Books for Parse Trees and Semantic Analysis
"Speech and Language Processing" by Daniel Jurafsky and James H. Martin:

Overview: This is one of the most comprehensive and widely referenced books for NLP tasks. It covers everything from syntactic parsing to semantic analysis and deep learning-based models.
Why Recommended: It’s great for both beginners and experts and covers theoretical foundations along with practical NLP techniques.
"Foundations of Statistical Natural Language Processing" by Christopher D. Manning and Hinrich Schütze:

Overview: This book focuses on statistical methods for NLP, including parsing and semantic analysis.
Why Recommended: It’s a solid reference for understanding the foundational principles behind dependency parsing, part-of-speech tagging, and semantic interpretation.
"Neural Network Methods in Natural Language Processing" by Yoav Goldberg:

Overview: This book provides a detailed look at neural network approaches to NLP, including semantic analysis, sequence modeling, and parsing using deep learning techniques.
Why Recommended: It’s perfect for those interested in using modern neural network methods for NLP tasks.
"Deep Learning for Natural Language Processing" by Palash Goyal, Sumit Pandey, Karan Jain:

Overview: This book provides practical deep learning techniques for NLP tasks, including semantic analysis and syntactic parsing.
Why Recommended: It focuses on practical implementations using popular frameworks like PyTorch and TensorFlow.
"Practical Natural Language Processing" by Sowmya Vajjala, Bodhisattwa Prasad Majumder, Anuj Gupta, Harshit Surana:

Overview: This book focuses on real-world NLP applications, including semantic analysis, and discusses the deployment of NLP models.
Why Recommended: It’s ideal for practitioners looking to understand how to build NLP solutions in production environments.
Industry Uses for Research Paper Peer Review Automation
In the research domain, particularly in peer-reviewed journals, automating the peer review process using NLP techniques is becoming increasingly important. Some potential uses include:

Automated Review Assignment:

Use Case: Using semantic analysis and NLP models to assign relevant reviewers to papers by analyzing the content of both the papers and the reviewers' expertise areas.
Tools: BERT or other transformer-based models can match paper abstracts with reviewer profiles.
Plagiarism Detection:

Use Case: Automatically checking for plagiarism by comparing the submitted paper with a large database of research papers. This can be done using semantic similarity techniques.
Tools: Sentence transformers or other similarity models can calculate the semantic similarity between papers.
Automated Feedback Generation:

Use Case: Automatically generating comments or feedback based on the content of the research paper. This could include suggestions for improving methodology, grammar, or relevance to the field.
Tools: Pretrained models like GPT-3 can be used to generate summaries or feedback based on a research paper.
Opinion Mining for Peer Reviews:

Use Case: Analyzing the sentiment and opinions expressed in peer review comments to identify constructive vs. destructive feedback.
Tools: Sentiment analysis models like RoBERTa or BERT fine-tuned for sentiment classification.
Recommendation of Relevant Literature:

Use Case: Recommending relevant research papers to authors or reviewers based on the content of the manuscript. This could be done through semantic search models that match the paper to relevant publications.
Tools: Semantic search models like BERT, DPR, or Sentence-BERT.
By leveraging these technologies, the peer review process can become more efficient, accurate, and scalable, ultimately saving time and improving the quality of research dissemination.



Microsoft Windows [Version 10.0.22631.4602]

(c) Microsoft Corporation. All rights reserved.



C:\Users\Sanjoy Nath>D:



D:\>CD D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS



D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS>PY SENTENCES_SIMPLIFIERS.PY

tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.32k/2.32k [00:00<?, ?B/s]

C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\huggingface_hub\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Sanjoy Nath\.cache\huggingface\hub\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.

To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development

  warnings.warn(message)

spiece.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 1.53MB/s]

tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.39M/1.39M [00:00<00:00, 2.53MB/s]

config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.21k/1.21k [00:00<?, ?B/s]

model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 242M/242M [01:45<00:00, 2.29MB/s]

generation_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:00<?, ?B/s]















 To read a PDF file, simplify its sentences, and integrate this functionality with a graphical user interface (GUI) using Tkinter, we can follow these steps:

Steps:
Use PyPDF2 or PyMuPDF (fitz) to extract text from the PDF.
Simplify sentences using natural language processing (NLP) techniques or rule-based methods.
Create a Tkinter GUI that:
Lets users select a PDF file.
Displays simplified text in a text widget.
Code Implementation:
Here’s the complete Python code for the task:

python

import tkinter as tk
from tkinter import filedialog, messagebox
import fitz  # PyMuPDF for PDF extraction
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from nltk import download

# Download NLTK resources
download("punkt")
download("stopwords")

def simplify_sentence(sentence):
    """Simplify a given sentence by removing stop words."""
    stop_words = set(stopwords.words('english'))
    words = sentence.split()
    simplified = " ".join([word for word in words if word.lower() not in stop_words])
    return simplified

def extract_text_from_pdf(file_path):
    """Extract text from a PDF file."""
    try:
        document = fitz.open(file_path)
        text = ""
        for page in document:
            text += page.get_text()
        return text
    except Exception as e:
        messagebox.showerror("Error", f"Failed to extract text: {e}")
        return None

def simplify_pdf_text(file_path):
    """Simplify all sentences in the selected PDF file."""
    text = extract_text_from_pdf(file_path)
    if text:
        sentences = sent_tokenize(text)
        simplified_sentences = [simplify_sentence(sentence) for sentence in sentences]
        return "\n".join(simplified_sentences)
    return ""

def open_pdf():
    """Handle file dialog to open a PDF and display simplified text."""
    file_path = filedialog.askopenfilename(filetypes=[("PDF files", "*.pdf")])
    if file_path:
        simplified_text = simplify_pdf_text(file_path)
        if simplified_text:
            text_widget.delete("1.0", tk.END)
            text_widget.insert(tk.END, simplified_text)

# Create Tkinter GUI
root = tk.Tk()
root.title("Simplify PDF Sentences")

frame = tk.Frame(root)
frame.pack(padx=10, pady=10)

open_button = tk.Button(frame, text="Open PDF", command=open_pdf)
open_button.pack(pady=5)

text_widget = tk.Text(frame, wrap=tk.WORD, height=20, width=80)
text_widget.pack(pady=5)

root.mainloop()
How It Works:
PDF Selection: Users click "Open PDF" to select a file.
Text Extraction: The text is extracted using PyMuPDF.
Sentence Simplification: Each sentence is processed to remove stop words.
Text Display: The simplified text is displayed in a Tkinter Text widget.
Requirements:
Install dependencies:
bash

pip install pymupdf nltk
Ensure the NLTK stopwords dataset is downloaded (already included in the code).
Output:
A Tkinter-based application where users can simplify sentences from a PDF and view them in a scrollable text area.
read the document thoroughly thoroughly thoroughly and please guide me step wise to do semantic analysis on every sentences in the document ,dump the texts with simplified sentences for each of the sentences and dump the text files (.txt selected with tkinter or with .pdf selected with tkinter ) and then dump the text file with each sentence in new line and for n th sentence write new line the n.0 original sentence , newline n.1 for first simplified forms ,new line n.2 for second simplified forms and continue in these ways ...... and then write the comprehensibility complicatedness number for each sentences at right side of the sentences (i dont know how to use any pre trained systems models (offine uses) guide me which models to download with which command line commands? i run python with windows command lines only . i dont use jupyter or anything like that ... which transformer to use (offline) how to use the pretrained systems (offline only)



Step-by-Step Guide for Your Request:
To accomplish the tasks outlined, including reading PDF files, analyzing sentences semantically, simplifying them, and generating a structured text file, follow these steps:

1. Setup and Installation
Install Required Libraries
Run the following commands in your terminal:

bash

pip install transformers sentencepiece torch pdfplumber nltk
Download Pretrained Models for Offline Use
Choose a model for sentence simplification:

T5: t5-small or t5-large
BART: facebook/bart-large
To download the models for offline use:

python

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "t5-small"  # or "facebook/bart-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Save model locally
tokenizer.save_pretrained("./models/" + model_name)
model.save_pretrained("./models/" + model_name)
Once downloaded, these files can be used offline by loading them from the saved directory.

2. Extract Text from PDFs
Use pdfplumber to extract text:

python

import pdfplumber

def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text
3. Semantic Analysis and Simplification
Tokenize Sentences
Use nltk for sentence tokenization:

python

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

def tokenize_sentences(text):
    return sent_tokenize(text)
Simplify Sentences
Leverage T5 or BART models for simplification:

python

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def simplify_sentence(sentence, tokenizer, model):
    inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)
    outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
4. Comprehensibility and Complexity Metric
For calculating comprehensibility, a proxy metric can be based on sentence length and unique words:

python

def calculate_comprehensibility(sentence):
    words = sentence.split()
    unique_words = set(words)
    return len(words) / len(unique_words)  # Lower values suggest simpler sentences
5. Dump Sentences to File
Write Sentences in Required Format
python

def write_sentences_with_levels(sentences, simplified_sentences, output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        for i, (orig, simplified) in enumerate(zip(sentences, simplified_sentences), start=1):
            f.write(f"{i}.0 {orig} [Complexity: {calculate_comprehensibility(orig):.2f}]\n")
            for j, part in enumerate(simplified, start=1):
                f.write(f"{i}.{j} {part} [Complexity: {calculate_comprehensibility(part):.2f}]\n")
6. Tkinter Integration
Use Tkinter to select files and run the process:

python

import tkinter as tk
from tkinter import filedialog

def select_pdf_and_process():
    pdf_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf")])
    if pdf_path:
        text = extract_text_from_pdf(pdf_path)
        sentences = tokenize_sentences(text)
        simplified_sentences = [simplify_sentence(s, tokenizer, model).split(". ") for s in sentences]
        write_sentences_with_levels(sentences, simplified_sentences, "output.txt")
        print("Simplified sentences saved to output.txt")

root = tk.Tk()
root.title("Simplify PDF Sentences")
btn = tk.Button(root, text="Select PDF and Process", command=select_pdf_and_process)
btn.pack(pady=20)
root.mainloop()
Summary of Outputs
File Format:

css

1.0 Original sentence [Complexity: 3.25]
1.1 Simplified part 1 [Complexity: 2.45]
1.2 Simplified part 2 [Complexity: 1.78]
...
Output Location:
output.txt in the current working directory.

Notes on Offline Model Usage
Offline Model Loading:
python

tokenizer = AutoTokenizer.from_pretrained("./models/t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("./models/t5-small")
Ensure Sufficient Memory: T5 and BART models require moderate system resources. If working on large files, split them into smaller chunks.

C:\Users\Sanjoy Nath>pip install transformers sentencepiece torch pdfplumber nltk
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.44.2)
Collecting sentencepiece
  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)
Requirement already satisfied: torch in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (2.4.1)
Requirement already satisfied: pdfplumber in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.11.4)
Requirement already satisfied: nltk in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (3.9.1)
Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)
Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)
Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)
Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)
Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)
Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)
Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.19.1)
Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)
Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (4.12.2)
Requirement already satisfied: sympy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (1.13.2)
Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.3)
Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.1.4)
Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (2024.9.0)
Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (75.1.0)
Requirement already satisfied: pdfminer.six==20231228 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (20231228)
Requirement already satisfied: Pillow>=9.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (10.4.0)
Requirement already satisfied: pypdfium2>=4.18.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (4.30.0)
Requirement already satisfied: charset-normalizer>=2.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)
Requirement already satisfied: cryptography>=36.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)
Requirement already satisfied: click in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (1.4.2)
Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch) (2.1.5)
Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy->torch) (1.3.0)
Requirement already satisfied: cffi>=1.12 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)
Requirement already satisfied: pycparser in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)
Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 992.0/992.0 kB 1.9 MB/s eta 0:00:00
Installing collected packages: sentencepiece
Successfully installed sentencepiece-0.2.0


Below is a complete Python program with a main function and a Tkinter interface to accomplish your requirements. This includes reading a PDF or TXT file, performing semantic analysis and sentence simplification using offline models (T5 or BART), and dumping the structured output as described.

Full Code Implementation
python
Copy code
import tkinter as tk
from tkinter import filedialog, messagebox
import pdfplumber
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Function to extract text from a TXT file
def extract_text_from_txt(txt_path):
    with open(txt_path, "r", encoding="utf-8") as file:
        return file.read()

# Function to tokenize text into sentences
def tokenize_sentences(text):
    return sent_tokenize(text)

# Function to simplify a single sentence using the pretrained model
def simplify_sentence(sentence, tokenizer, model):
    inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)
    outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)
    simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return simplified.split(". ")  # Split into parts

# Function to calculate a proxy comprehensibility score
def calculate_comprehensibility(sentence):
    words = sentence.split()
    unique_words = set(words)
    return len(words) / len(unique_words) if unique_words else 0

# Function to process the file and dump results
def process_file(input_file, file_type, output_file, tokenizer, model):
    # Extract text based on file type
    text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)
    
    # Tokenize text into sentences
    sentences = tokenize_sentences(text)
    
    # Simplify sentences and calculate comprehensibility
    with open(output_file, "w", encoding="utf-8") as f:
        for i, sentence in enumerate(sentences, start=1):
            # Write original sentence
            complexity = calculate_comprehensibility(sentence)
            f.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")
            
            # Simplify and write simplified parts
            simplified_parts = simplify_sentence(sentence, tokenizer, model)
            for j, part in enumerate(simplified_parts, start=1):
                complexity_part = calculate_comprehensibility(part)
                f.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")

# Main function to handle Tkinter interface and processing
def main():
    # Load pretrained models (offline)
    model_name = "t5-small"  # Change to "facebook/bart-large" if needed
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Tkinter GUI
    def select_file():
        file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])
        if not file_path:
            return
        file_type = "pdf" if file_path.endswith(".pdf") else "txt"
        output_file = filedialog.asksaveasfilename(defaultextension=".txt", filetypes=[("Text Files", "*.txt")])
        if not output_file:
            return
        try:
            process_file(file_path, file_type, output_file, tokenizer, model)
            messagebox.showinfo("Success", f"Processing complete! Output saved to {output_file}")
        except Exception as e:
            messagebox.showerror("Error", f"An error occurred: {e}")

    root = tk.Tk()
    root.title("Semantic Analysis and Sentence Simplification")

    tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)
    tk.Button(root, text="Select File", command=select_file).pack(pady=20)
    tk.Button(root, text="Exit", command=root.quit).pack(pady=10)

    root.mainloop()

if __name__ == "__main__":
    main()
How It Works
Dependencies Installed:

Transformers: pip install transformers
Sentencepiece: pip install sentencepiece
Torch: pip install torch
PDFPlumber: pip install pdfplumber
NLTK: pip install nltk
Pretrained Models:

The script downloads the t5-small or facebook/bart-large models for offline use. Ensure sufficient disk space.
Process:

Select a file (PDF or TXT) via the GUI.
Extract text from the selected file.
Tokenize the text into sentences.
Simplify each sentence into granular components.
Write the original and simplified sentences with comprehensibility scores to a new file.
Output Format:

plaintext

1.0 Original sentence [Complexity: 3.25]
1.1 Simplified part 1 [Complexity: 2.10]
1.2 Simplified part 2 [Complexity: 1.85]
...
Output File:

User specifies the output file location during the save dialog.
Key Notes
Performance: The program may take time for large files. Ensure your system has adequate resources.
Offline Model: The downloaded models are stored locally and used offline.
Customizability: Change model_name to use a different pretrained model, such as facebook/bart-large.
C:\Users\Sanjoy Nath>pip install transformers
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.44.2)
Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)
Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)
Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)
Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)
Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)
Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)
Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.19.1)
Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)
Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)



C:\Users\Sanjoy Nath>pip install sentencepiece
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: sentencepiece in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.2.0)

C:\Users\Sanjoy Nath>pip install sentencepiece
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: sentencepiece in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.2.0)

C:\Users\Sanjoy Nath>pip install torch
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (2.4.1)
Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (4.12.2)
Requirement already satisfied: sympy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (1.13.2)
Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.3)
Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.1.4)
Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (2024.9.0)
Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (75.1.0)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch) (2.1.5)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy->torch) (1.3.0)

C:\Users\Sanjoy Nath>pip install pdfplumber
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pdfplumber in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.11.4)
Requirement already satisfied: pdfminer.six==20231228 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (20231228)
Requirement already satisfied: Pillow>=9.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (10.4.0)
Requirement already satisfied: pypdfium2>=4.18.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (4.30.0)
Requirement already satisfied: charset-normalizer>=2.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)
Requirement already satisfied: cryptography>=36.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)
Requirement already satisfied: cffi>=1.12 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)
Requirement already satisfied: pycparser in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)

C:\Users\Sanjoy Nath>pip install nltk
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: nltk in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (3.9.1)
Requirement already satisfied: click in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (1.4.2)
Requirement already satisfied: regex>=2021.8.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (2024.9.11)
Requirement already satisfied: tqdm in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (4.66.5)
Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from click->nltk) (0.4.6)



yes it did these for first time run... will it necessary to do every time running? how long will it take? i need the progress bar like things for the outputs D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS>PY SENTENCES_SIMPLIFIERS.PY

tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.32k/2.32k [00:00<?, ?B/s]

C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\huggingface_hub\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Sanjoy Nath\.cache\huggingface\hub\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.

To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development

warnings.warn(message)

spiece.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 1.53MB/s]

tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.39M/1.39M [00:00<00:00, 2.53MB/s]

config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.21k/1.21k [00:00<?, ?B/s]

model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 242M/242M [01:45<00:00, 2.29MB/s]

generation_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:00<?, ?B/s] yes i have changed the code at the output file point... keep the structure like that and show progress report also import tkinter as tk

from tkinter import filedialog, messagebox

import pdfplumber

from nltk.tokenize import sent_tokenize

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

import os


# Function to extract text from a PDF

def extract_text_from_pdf(pdf_path):

with pdfplumber.open(pdf_path) as pdf:

text = ""

for page in pdf.pages:

text += page.extract_text() + "\n"

return text


# Function to extract text from a TXT file

def extract_text_from_txt(txt_path):

with open(txt_path, "r", encoding="utf-8") as file:

return file.read()


# Function to tokenize text into sentences

def tokenize_sentences(text):

return sent_tokenize(text)


# Function to simplify a single sentence using the pretrained model

def simplify_sentence(sentence, tokenizer, model):

inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)

outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)

simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)

return simplified.split(". ") # Split into parts


# Function to calculate a proxy comprehensibility score

def calculate_comprehensibility(sentence):

words = sentence.split()

unique_words = set(words)

return len(words) / len(unique_words) if unique_words else 0


# Function to process the file and dump results

def process_file(input_file, file_type, output_file, tokenizer, model):

# Extract text based on file type

text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)


# Tokenize text into sentences

sentences = tokenize_sentences(text)


# Simplify sentences and calculate comprehensibility

with open(output_file, "w", encoding="utf-8") as f:

for i, sentence in enumerate(sentences, start=1):

# Write original sentence

complexity = calculate_comprehensibility(sentence)

f.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")


# Simplify and write simplified parts

simplified_parts = simplify_sentence(sentence, tokenizer, model)

for j, part in enumerate(simplified_parts, start=1):

complexity_part = calculate_comprehensibility(part)

f.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")


# Main function to handle Tkinter interface and processing

def main():

# Load pretrained models (offline)

model_name = "t5-small" # Change to "facebook/bart-large" if needed

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)


# Tkinter GUI

def select_file():

file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])

if not file_path:

return

file_type = "pdf" if file_path.endswith(".pdf") else "txt"

output_file =file_path +"_SIMPLIFIED_sENTENCES.TXT"### filedialog.asksaveasfilename(defaultextension=".txt", filetypes=[("Text Files", "*.txt")])

if not output_file:

return

try:

process_file(file_path, file_type, output_file, tokenizer, model)

messagebox.showinfo("Success", f"Processing complete! Output saved to {output_file}")

except Exception as e:

messagebox.showerror("Error", f"An error occurred: {e}")


root = tk.Tk()

root.title("Semantic Analysis and Sentence Simplification")


tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)

tk.Button(root, text="Select File", command=select_file).pack(pady=20)

tk.Button(root, text="Exit", command=root.quit).pack(pady=10)


root.mainloop()


if __name__ == "__main__":

main()

Enhanced Implementation with Progress Bar
Here is an enhanced version of your script, including a progress bar to show real-time progress during the processing. The progress bar will update as each sentence is processed.

Updated Code
python

import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import pdfplumber
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Function to extract text from a TXT file
def extract_text_from_txt(txt_path):
    with open(txt_path, "r", encoding="utf-8") as file:
        return file.read()

# Function to tokenize text into sentences
def tokenize_sentences(text):
    return sent_tokenize(text)

# Function to simplify a single sentence using the pretrained model
def simplify_sentence(sentence, tokenizer, model):
    inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)
    outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)
    simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return simplified.split(". ")  # Split into parts

# Function to calculate a proxy comprehensibility score
def calculate_comprehensibility(sentence):
    words = sentence.split()
    unique_words = set(words)
    return len(words) / len(unique_words) if unique_words else 0

# Function to process the file and dump results with a progress bar
def process_file_with_progress(input_file, file_type, output_file, tokenizer, model, progress_bar, progress_label):
    # Extract text based on file type
    text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)
    
    # Tokenize text into sentences
    sentences = tokenize_sentences(text)
    total_sentences = len(sentences)

    # Simplify sentences and calculate comprehensibility
    with open(output_file, "w", encoding="utf-8") as f:
        for i, sentence in enumerate(sentences, start=1):
            # Update progress bar and label
            progress_bar["value"] = (i / total_sentences) * 100
            progress_label["text"] = f"Processing sentence {i}/{total_sentences}"
            progress_bar.update()

            # Write original sentence
            complexity = calculate_comprehensibility(sentence)
            f.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")
            
            # Simplify and write simplified parts
            simplified_parts = simplify_sentence(sentence, tokenizer, model)
            for j, part in enumerate(simplified_parts, start=1):
                complexity_part = calculate_comprehensibility(part)
                f.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")

# Main function to handle Tkinter interface and processing
def main():
    # Load pretrained models (offline)
    model_name = "t5-small"  # Change to "facebook/bart-large" if needed
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Tkinter GUI
    def select_file():
        file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])
        if not file_path:
            return
        file_type = "pdf" if file_path.endswith(".pdf") else "txt"
        output_file = file_path + "_SIMPLIFIED_SENTENCES.TXT"
        if not output_file:
            return
        try:
            progress_label["text"] = "Initializing..."
            progress_bar["value"] = 0
            process_file_with_progress(file_path, file_type, output_file, tokenizer, model, progress_bar, progress_label)
            progress_label["text"] = "Processing complete!"
            messagebox.showinfo("Success", f"Processing complete! Output saved to {output_file}")
        except Exception as e:
            progress_label["text"] = "Error occurred."
            messagebox.showerror("Error", f"An error occurred: {e}")

    # Create Tkinter GUI
    root = tk.Tk()
    root.title("Semantic Analysis and Sentence Simplification")

    tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)
    tk.Button(root, text="Select File", command=select_file).pack(pady=20)

    progress_label = tk.Label(root, text="")
    progress_label.pack(pady=5)

    progress_bar = ttk.Progressbar(root, orient="horizontal", length=300, mode="determinate")
    progress_bar.pack(pady=5)

    tk.Button(root, text="Exit", command=root.quit).pack(pady=10)

    root.mainloop()

if __name__ == "__main__":
    main()
Features Added
Progress Bar:

Shows the percentage of completion during processing.
Updated for each sentence processed.
Progress Label:

Displays the current sentence being processed, e.g., Processing sentence 3/100.
Output File Naming:

Automatically appends _SIMPLIFIED_SENTENCES.TXT to the input file's name.
Instructions
Save the code as sentences_simplifiers.py.
Run the script:
bash

python sentences_simplifiers.py
Select a PDF or TXT file using the GUI.
Monitor progress in the progress bar and label.
The output will be saved as a text file in the same directory as the input file.
Performance Notes
Model Download:
The models and tokenizers will download only the first time. Subsequent runs will use the cached versions.
Processing Time:
Processing time depends on file size, sentence count, and system resources.
Progress bar gives real-time feedback.
This enhanced script ensures usability and a smooth user experience. 





rewrite this with 2 output files ... one output is as it is coming and the another output file is to put the sorted order smallest to largest complexity in the output data and also find the keywords in the document , list the keywords with the complexity. List the key phrases and with complexity of the key phrases import tkinter as tk

from tkinter import filedialog, messagebox, ttk

import pdfplumber

from nltk.tokenize import sent_tokenize

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

import os


# Function to extract text from a PDF

def extract_text_from_pdf(pdf_path):

with pdfplumber.open(pdf_path) as pdf:

text = ""

for page in pdf.pages:

text += page.extract_text() + "\n"

return text


# Function to extract text from a TXT file

def extract_text_from_txt(txt_path):

with open(txt_path, "r", encoding="utf-8") as file:

return file.read()


# Function to tokenize text into sentences

def tokenize_sentences(text):

return sent_tokenize(text)


# Function to simplify a single sentence using the pretrained model

def simplify_sentence(sentence, tokenizer, model):

inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)

outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)

simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)

return simplified.split(". ") # Split into parts


# Function to calculate a proxy comprehensibility score

def calculate_comprehensibility(sentence):

words = sentence.split()

unique_words = set(words)

return len(words) / len(unique_words) if unique_words else 0


# Function to process the file and dump results with a progress bar

def process_file_with_progress(input_file, file_type, output_file, tokenizer, model, progress_bar, progress_label):

# Extract text based on file type

text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)


# Tokenize text into sentences

sentences = tokenize_sentences(text)

total_sentences = len(sentences)


# Simplify sentences and calculate comprehensibility

with open(output_file, "w", encoding="utf-8") as f:

for i, sentence in enumerate(sentences, start=1):

# Update progress bar and label

progress_bar["value"] = (i / total_sentences) * 100

progress_label["text"] = f"Processing sentence {i}/{total_sentences}"

progress_bar.update()


# Write original sentence

complexity = calculate_comprehensibility(sentence)

f.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")


# Simplify and write simplified parts

simplified_parts = simplify_sentence(sentence, tokenizer, model)

for j, part in enumerate(simplified_parts, start=1):

complexity_part = calculate_comprehensibility(part)

f.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")


# Main function to handle Tkinter interface and processing

def main():

# Load pretrained models (offline)

model_name = "t5-small" # Change to "facebook/bart-large" if needed

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)


# Tkinter GUI

def select_file():

file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])

if not file_path:

return

file_type = "pdf" if file_path.endswith(".pdf") else "txt"

output_file = file_path + "_SIMPLIFIED_SENTENCES.TXT"

if not output_file:

return

try:

progress_label["text"] = "Initializing..."

progress_bar["value"] = 0

process_file_with_progress(file_path, file_type, output_file, tokenizer, model, progress_bar, progress_label)

progress_label["text"] = "Processing complete!"

messagebox.showinfo("Success", f"Processing complete! Output saved to {output_file}")

except Exception as e:

progress_label["text"] = "Error occurred."

messagebox.showerror("Error", f"An error occurred: {e}")


# Create Tkinter GUI

root = tk.Tk()

root.title("Semantic Analysis and Sentence Simplification")


tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)

tk.Button(root, text="Select File", command=select_file).pack(pady=20)


progress_label = tk.Label(root, text="")

progress_label.pack(pady=5)


progress_bar = ttk.Progressbar(root, orient="horizontal", length=300, mode="determinate")

progress_bar.pack(pady=5)


tk.Button(root, text="Exit", command=root.quit).pack(pady=10)


root.mainloop()


if __name__ == "__main__":

main()



Below is the updated Python code that generates two output files:

First Output File: Contains the original and simplified sentences in order of appearance.
Second Output File: Contains sentences sorted by complexity (smallest to largest) with their keywords and key phrases along with their complexities.
Updated Code
python

import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import pdfplumber
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from collections import Counter
import os

# Function to extract text from a PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Function to extract text from a TXT file
def extract_text_from_txt(txt_path):
    with open(txt_path, "r", encoding="utf-8") as file:
        return file.read()

# Function to tokenize text into sentences
def tokenize_sentences(text):
    return sent_tokenize(text)

# Function to simplify a single sentence using the pretrained model
def simplify_sentence(sentence, tokenizer, model):
    inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)
    outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)
    simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return simplified.split(". ")  # Split into parts

# Function to calculate a proxy comprehensibility score
def calculate_comprehensibility(sentence):
    words = sentence.split()
    unique_words = set(words)
    return len(words) / len(unique_words) if unique_words else 0

# Function to extract keywords and key phrases
def extract_keywords_and_phrases(text):
    stop_words = set(stopwords.words('english'))
    words = word_tokenize(text)
    keywords = [word for word in words if word.isalnum() and word.lower() not in stop_words]
    key_phrases = [" ".join(phrase) for phrase in zip(words, words[1:]) if all(w.isalnum() for w in phrase)]
    return Counter(keywords), Counter(key_phrases)

# Function to process the file and dump results
def process_file_with_progress(input_file, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label):
    # Extract text based on file type
    text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)
    
    # Tokenize text into sentences
    sentences = tokenize_sentences(text)
    total_sentences = len(sentences)

    # Data structure to hold sentence complexities and simplified parts
    sentence_data = []

    # Process each sentence
    with open(output_file1, "w", encoding="utf-8") as f1:
        for i, sentence in enumerate(sentences, start=1):
            # Update progress bar and label
            progress_bar["value"] = (i / total_sentences) * 100
            progress_label["text"] = f"Processing sentence {i}/{total_sentences}"
            progress_bar.update()

            # Calculate complexity
            complexity = calculate_comprehensibility(sentence)

            # Simplify sentence
            simplified_parts = simplify_sentence(sentence, tokenizer, model)

            # Write to first output file
            f1.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")
            for j, part in enumerate(simplified_parts, start=1):
                complexity_part = calculate_comprehensibility(part)
                f1.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")
            
            # Collect data for sorting
            sentence_data.append((sentence.strip(), complexity, simplified_parts))

    # Sort by complexity
    sorted_data = sorted(sentence_data, key=lambda x: x[1])

    # Extract keywords and key phrases
    all_text = " ".join([s[0] for s in sorted_data])  # Combine all sentences
    keywords, key_phrases = extract_keywords_and_phrases(all_text)

    # Write to second output file
    with open(output_file2, "w", encoding="utf-8") as f2:
        for sentence, complexity, simplified_parts in sorted_data:
            f2.write(f"Sentence: {sentence} [Complexity: {complexity:.2f}]\n")
            for part in simplified_parts:
                f2.write(f"  Simplified: {part.strip()}\n")
        f2.write("\n--- Keywords and Their Complexity ---\n")
        for keyword, count in keywords.most_common():
            f2.write(f"{keyword}: {count}\n")
        f2.write("\n--- Key Phrases and Their Complexity ---\n")
        for phrase, count in key_phrases.most_common():
            f2.write(f"{phrase}: {count}\n")

# Main function to handle Tkinter interface and processing
def main():
    # Load pretrained models (offline)
    model_name = "t5-small"  # Change to "facebook/bart-large" if needed
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    # Tkinter GUI
    def select_file():
        file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])
        if not file_path:
            return
        file_type = "pdf" if file_path.endswith(".pdf") else "txt"
        output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"
        output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"
        if not output_file1 or not output_file2:
            return
        try:
            progress_label["text"] = "Initializing..."
            progress_bar["value"] = 0
            process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)
            progress_label["text"] = "Processing complete!"
            messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")
        except Exception as e:
            progress_label["text"] = "Error occurred."
            messagebox.showerror("Error", f"An error occurred: {e}")

    # Create Tkinter GUI
    root = tk.Tk()
    root.title("Semantic Analysis and Sentence Simplification")

    tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)
    tk.Button(root, text="Select File", command=select_file).pack(pady=20)

    progress_label = tk.Label(root, text="")
    progress_label.pack(pady=5)

    progress_bar = ttk.Progressbar(root, orient="horizontal", length=300, mode="determinate")
    progress_bar.pack(pady=5)

    tk.Button(root, text="Exit", command=root.quit).pack(pady=10)

    root.mainloop()

if __name__ == "__main__":
    main()
Key Features:
Two Output Files:

_SIMPLIFIED_SENTENCES.TXT: Original and simplified sentences in the order of appearance.
_SORTED_COMPLEXITY.TXT: Sentences sorted by complexity with keywords and key phrases listed.
Keywords and Key Phrases:

Extracted using nltk's word_tokenize and bigrams.
Progress Bar:

Shows progress during processing.
Sorting by Complexity:

Sentences are sorted from smallest to largest complexity in the second output.
The issue you're experiencing occurs because the messagebox.showinfo with "Processing complete!" is being shown regardless of whether the files are fully processed or not. This could happen due to the following reasons:

The process_file_with_progress function completes execution, but the file writing or processing might not have been completed properly (e.g., due to exceptions or incomplete operations).
The message box for success is displayed immediately after calling the process_file_with_progress function, without verifying if everything succeeded.
Fix: Ensure Proper Completion Check
Here is the corrected code. Changes include:

Adding checks for errors and ensuring file writes are completed before showing the success message.
Moving the progress_label["text"] update properly to reflect real progress.
Using exception handling within the process_file_with_progress function to catch and handle partial failures.
Updated Code with Fix
python

def process_file_with_progress(input_file, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label):
    try:
        # Extract text based on file type
        text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)
        
        # Tokenize text into sentences
        sentences = tokenize_sentences(text)
        total_sentences = len(sentences)

        # Ensure there are sentences to process
        if total_sentences == 0:
            raise ValueError("No sentences found in the document.")

        # Data structure to hold sentence complexities and simplified parts
        sentence_data = []

        # Process each sentence
        with open(output_file1, "w", encoding="utf-8") as f1:
            for i, sentence in enumerate(sentences, start=1):
                # Update progress bar and label
                progress_bar["value"] = (i / total_sentences) * 100
                progress_label["text"] = f"Processing sentence {i}/{total_sentences}"
                progress_bar.update()

                # Calculate complexity
                complexity = calculate_comprehensibility(sentence)

                # Simplify sentence
                simplified_parts = simplify_sentence(sentence, tokenizer, model)

                # Write to first output file
                f1.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")
                for j, part in enumerate(simplified_parts, start=1):
                    complexity_part = calculate_comprehensibility(part)
                    f1.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")
                
                # Collect data for sorting
                sentence_data.append((sentence.strip(), complexity, simplified_parts))

        # Sort by complexity
        sorted_data = sorted(sentence_data, key=lambda x: x[1])

        # Extract keywords and key phrases
        all_text = " ".join([s[0] for s in sorted_data])  # Combine all sentences
        keywords, key_phrases = extract_keywords_and_phrases(all_text)

        # Write to second output file
        with open(output_file2, "w", encoding="utf-8") as f2:
            for sentence, complexity, simplified_parts in sorted_data:
                f2.write(f"Sentence: {sentence} [Complexity: {complexity:.2f}]\n")
                for part in simplified_parts:
                    f2.write(f"  Simplified: {part.strip()}\n")
            f2.write("\n--- Keywords and Their Complexity ---\n")
            for keyword, count in keywords.most_common():
                f2.write(f"{keyword}: {count}\n")
            f2.write("\n--- Key Phrases and Their Complexity ---\n")
            for phrase, count in key_phrases.most_common():
                f2.write(f"{phrase}: {count}\n")

        # Finalize progress
        progress_label["text"] = "Processing complete!"
        return True  # Indicate success

    except Exception as e:
        progress_label["text"] = f"Error: {e}"
        return False  # Indicate failure

# Update select_file to only show success if the processing is complete
def select_file():
    file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])
    if not file_path:
        return
    file_type = "pdf" if file_path.endswith(".pdf") else "txt"
    output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"
    output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"
    if not output_file1 or not output_file2:
        return
    try:
        progress_label["text"] = "Initializing..."
        progress_bar["value"] = 0
        success = process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)
        if success:
            messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")
        else:
            messagebox.showerror("Error", "Processing failed. Check progress and files.")
    except Exception as e:
        progress_label["text"] = "Error occurred."
        messagebox.showerror("Error", f"An error occurred: {e}")
Changes Made:
Completion Check:

The process_file_with_progress function now returns True if all operations complete successfully and False otherwise.
The select_file function checks this return value to decide whether to show the success message or an error message.
Error Handling:

Catches any exceptions during processing and provides appropriate feedback via the progress label and message boxes.
Empty Document Check:

Throws an error if no sentences are found in the selected file.
Progress Feedback:

The progress bar and label correctly reflect the processing status.
With this update, the completion message will only display when processing is truly complete. 





Other Pretrained Models for Simplification and Similar Tasks
There are many pretrained models for text simplification, summarization, keyword extraction, and sentence analysis. Below is a list of popular models categorized by tasks:

1. Text Simplification
T5 (Text-To-Text Transfer Transformer):

Models: t5-small, t5-base, t5-large.
Purpose: Text simplification, summarization, translation, and more.
Key Features: Converts tasks into a text-to-text framework.
BART (Bidirectional and Auto-Regressive Transformer):

Models: facebook/bart-large, facebook/bart-large-cnn.
Purpose: Summarization, text simplification, paraphrasing.
Key Features: Combines bidirectional encoding and autoregressive decoding.
PEGASUS (Pre-training with Extracted Gap Sentences):

Models: google/pegasus-xsum, google/pegasus-large.
Purpose: Specialized for summarization tasks but can simplify text by customizing prompts.
Key Features: Focuses on summarization with a unique pretraining strategy.
2. Keyword and Key Phrase Extraction
DistilBERT:

Models: distilbert-base-uncased, distilbert-base-cased.
Purpose: Text classification, keyword extraction.
Key Features: A lighter and faster version of BERT.
BERT (Bidirectional Encoder Representations from Transformers):

Models: bert-base-uncased, bert-large-cased.
Purpose: Text embedding, sentence similarity, keyword extraction.
Key Features: General-purpose and widely used.
Sentence Transformers:

Models: all-MiniLM-L6-v2, paraphrase-MiniLM-L6-v2.
Purpose: Keyword extraction, semantic search.
Key Features: Generates sentence embeddings optimized for similarity tasks.
3. Semantic Role Labeling & Contextual Understanding
RoBERTa (Robustly Optimized BERT):

Models: roberta-base, roberta-large.
Purpose: Context understanding, text embedding.
Key Features: Improves on BERT by training on more data with optimized hyperparameters.
XLNet:

Models: xlnet-base-cased, xlnet-large-cased.
Purpose: Text generation, contextual understanding.
Key Features: Addresses BERT's limitations with permutation-based language modeling.
4. Named Entity Recognition (NER)
SpaCy Pretrained Models:

Models: en_core_web_sm, en_core_web_md, en_core_web_lg.
Purpose: NER, keyword extraction, dependency parsing.
Key Features: Fast and efficient for production use.
HuggingFace Token Classification Models:

Models: dbmdz/bert-large-cased-finetuned-conll03-english.
Purpose: Named entity recognition tasks.
Key Features: Fine-tuned for token classification.
Common Use Cases in the Industry
1. Text Simplification
Use Case: Simplifying customer support articles, user manuals, or legal documents for readability.
Examples:
Chatbots and FAQ systems.
Reducing reading complexity in health information (e.g., for patients).
2. Summarization
Use Case: Condensing large texts into shorter summaries.
Examples:
News article summarization.
Meeting transcription summaries.
3. Keyword Extraction
Use Case: Extracting important keywords from a document for indexing or categorization.
Examples:
Document management systems.
SEO optimization tools.
4. Sentiment Analysis
Use Case: Understanding customer sentiments from reviews or social media.
Examples:
E-commerce product reviews.
Brand monitoring on social media.
5. Semantic Search
Use Case: Searching for relevant documents or answers based on semantic similarity.
Examples:
Enterprise knowledge bases.
Search engines like Google.
6. Named Entity Recognition (NER)
Use Case: Identifying entities like names, dates, and locations in text.
Examples:
Legal text analysis.
Healthcare systems for recognizing diseases, medications, etc.
7. Paraphrasing
Use Case: Generating alternative versions of a sentence.
Examples:
Content creation.
Avoiding plagiarism in academic work.
8. Contextual Understanding
Use Case: Understanding the meaning of ambiguous sentences in different contexts.
Examples:
Dialogue systems.
Sentiment and intent classification.
Transformers with NLTK and WordNet
WordNet, integrated with NLTK, complements transformers by offering lexical information for:

Synonyms and Antonyms:
Use WordNet to enrich or verify keywords.
Hypernym-Hyponym Relationships:
Identify hierarchical relationships between terms.
Keyword Context:
Filter out noisy keywords by verifying their relevance to the topic.
Keyword Grouping:
Group synonyms together for categorization.
Example Workflow in Industry
Document Analysis Pipeline
Input: Large documents, reviews, or social media posts.
Preprocessing:
Tokenization and stopword removal using NLTK.
Part-of-speech tagging and keyword extraction using WordNet and transformers.
Semantic Tasks:
Simplify sentences with T5 or BART.
Summarize the text using PEGASUS.
Extract keywords or phrases with BERT embeddings.
Output:
Simplified documents, summary reports, or extracted entities.
To install the necessary packages and run examples offline for text simplification, summarization, keyword extraction, and sentence analysis, follow these steps:

Step 1: Install Required Libraries
You need to install transformers (for pretrained models), torch (for deep learning tasks), and spaCy (for NLP tasks like NER). Below are the commands for installing them:

bash

pip install transformers torch spaCy
Additionally, install NLTK and WordNet for lexical information:

bash

pip install nltk
python -m nltk.downloader wordnet
Step 2: Download Pretrained Models
Use HuggingFace's transformers library to load pretrained models for text simplification, summarization, and other tasks.

Example: Text Simplification (T5)
You can use models like t5-small for text simplification. Here's an example:

python

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load model and tokenizer
model = T5ForConditionalGeneration.from_pretrained("t5-small")
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Example text
text = "The quick brown fox jumps over the lazy dog."

# Preprocess text
input_text = f"simplify: {text}"
inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

# Generate simplified text
outputs = model.generate(inputs["input_ids"])
simplified_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Simplified Text:", simplified_text)
Example: Summarization (BART)
Use BART for text summarization.

python

from transformers import BartForConditionalGeneration, BartTokenizer

# Load model and tokenizer
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

# Example text (longer text for summarization)
text = """
Artificial intelligence (AI) refers to the simulation of human intelligence in machines 
that are programmed to think like humans and mimic their actions. The term may also be applied 
to any machine that exhibits traits associated with a human mind such as learning and problem-solving.
"""

# Tokenize and summarize
inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True)
summary_ids = model.generate(inputs["input_ids"], max_length=50, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)

# Decode and print summary
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print("Summary:", summary)
Example: Keyword Extraction (DistilBERT)
You can use DistilBERT for extracting embeddings and finding keywords:

python

from transformers import DistilBertTokenizer, DistilBertModel
import torch

# Load model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

# Example text
text = "Hugging Face's transformers library is an essential tool for NLP tasks."

# Tokenize input text
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# Get the last hidden state (embeddings)
embeddings = outputs.last_hidden_state
print("Embeddings:", embeddings)
Example: Named Entity Recognition (NER) with SpaCy
You can use SpaCy to perform Named Entity Recognition (NER):

python

import spacy

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Example text
text = "Barack Obama was born in Hawaii on August 4, 1961."

# Process text with spaCy
doc = nlp(text)

# Extract named entities
for ent in doc.ents:
    print(ent.text, ent.label_)
Step 3: Running the Scripts Offline
Once the packages and models are installed, you can run these scripts offline without an internet connection, as the models are downloaded locally during the first run.

Example Workflow
Input: A document or text input.
Preprocessing: Tokenization, stopword removal (via NLTK).
Semantic Tasks:
Simplify text using T5.
Summarize text using BART.
Extract keywords using DistilBERT.
Perform NER using SpaCy.
Output: Simplified text, summarized content, extracted keywords, or recognized entities.
Additional Notes
T5 (Text Simplification): Focuses on generating a simplified version of the text.
BART (Summarization): Specializes in producing a concise summary.
DistilBERT (Keyword Extraction): Can be used for generating sentence embeddings for keyword extraction tasks.
SpaCy (NER): Detects entities like names, locations, etc.
You can mix and match these models to create a pipeline for various NLP tasks based on your needs.







C:\Users\Sanjoy Nath>pip install transformers torch spaCy

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.44.2)

Requirement already satisfied: torch in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (2.4.1)

Requirement already satisfied: spaCy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (3.7.6)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)

Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)

Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)

Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)

Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)

Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)

Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)

Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)

Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.19.1)

Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)

Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (4.12.2)

Requirement already satisfied: sympy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (1.13.2)

Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.3)

Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.1.4)

Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (2024.9.0)

Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (75.1.0)

Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (3.0.12)

Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (1.0.5)

Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (1.0.10)

Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (2.0.8)

Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (3.0.9)

Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (8.2.5)

Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (1.1.3)

Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (2.4.8)

Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (2.0.10)

Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (0.4.1)

Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (0.12.5)

Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (2.9.2)

Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from spaCy) (3.4.0)

Requirement already satisfied: language-data>=1.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.2.0)

Requirement already satisfied: annotated-types>=0.6.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)

Requirement already satisfied: pydantic-core==2.23.4 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.23.4)

Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.3.2)

Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)

Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)

Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)

Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from thinc<8.3.0,>=8.2.2->spaCy) (0.7.11)

Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from thinc<8.3.0,>=8.2.2->spaCy) (0.1.5)

Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)

Requirement already satisfied: click>=8.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.1.7)

Requirement already satisfied: shellingham>=1.3.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)

Requirement already satisfied: rich>=10.11.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (13.8.1)

Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.19.0)

Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.0.4)

Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch) (2.1.5)

Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy->torch) (1.3.0)

Requirement already satisfied: marisa-trie>=0.7.7 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.2.0)

Requirement already satisfied: markdown-it-py>=2.2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (3.0.0)

Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.18.0)

Requirement already satisfied: wrapt in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.16.0)

Requirement already satisfied: mdurl~=0.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)



To support the deep learning and machine learning functionalities in your code, particularly for text processing and simplification, you can use several readymade systems and tools that provide pre-trained models and frameworks. Below are some recommended tools and libraries, along with how to install and use them:

1. Hugging Face Transformers
Purpose: Hugging Face provides pre-trained models for text processing tasks such as text simplification, tokenization, summarization, etc. It's a widely used library in the deep learning community for NLP (Natural Language Processing).

Use Cases: Sentence simplification (e.g., using models like T5 or BART), keyword extraction, and tokenization.

Installation:

bash

pip install transformers
Usage: You are already using AutoTokenizer and AutoModelForSeq2SeqLM from Hugging Face in your script, which is great for tasks like sentence simplification. Here’s an example of loading and using a model:

python

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Example sentence simplification
def simplify_sentence(sentence):
    inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)
    outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)
    simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return simplified
2. NLTK (Natural Language Toolkit)
Purpose: NLTK is a popular Python library for working with human language data. It provides tools for tokenization, text processing, and linguistic analysis.

Use Cases: Sentence tokenization, word tokenization, stopword removal, and keyword extraction.

Installation:

bash

pip install nltk
Usage: You are already using NLTK for tokenization and stopword removal. For example:

python

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
words = word_tokenize(text)
filtered_words = [word for word in words if word.lower() not in stop_words]
3. spaCy
Purpose: spaCy is another robust NLP library that provides pre-trained models for text processing tasks such as named entity recognition (NER), part-of-speech tagging, and dependency parsing.

Use Cases: More advanced text processing tasks like entity recognition and syntactic parsing. You can also use it for text simplification by applying custom models.

Installation:

bash

pip install spacy
python -m spacy download en_core_web_sm
Usage:

python

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")
for ent in doc.ents:
    print(ent.text, ent.label_)
4. Gensim
Purpose: Gensim is a library used for topic modeling and document similarity analysis. It's useful for keyword extraction and processing large text datasets.

Use Cases: Topic modeling, keyword extraction, and semantic similarity.

Installation:

bash

pip install gensim
Usage: Gensim can be used for more advanced keyword extraction methods such as LDA (Latent Dirichlet Allocation). For example:

python

from gensim.summarization import keywords

text = "Your document text here."
print(keywords(text))
5. PyTorch or TensorFlow
Purpose: PyTorch and TensorFlow are two of the most popular deep learning frameworks. These are essential for training custom models or fine-tuning existing ones.

Use Cases: Fine-tuning models, building custom neural networks for specific tasks.

Installation:

bash

pip install torch   # for PyTorch
pip install tensorflow   # for TensorFlow
Usage: For fine-tuning, you would typically load a pre-trained model from Hugging Face, but if you want to create custom models, you can use PyTorch or TensorFlow to train models on your own data.

6. TextBlob
Purpose: TextBlob is a simpler NLP library that makes text processing tasks easy. It offers basic functionality for NLP, such as part-of-speech tagging, noun phrase extraction, and sentiment analysis.

Use Cases: Basic text processing, sentiment analysis, and translation.

Installation:

bash

pip install textblob
Usage:

python

from textblob import TextBlob

text = "I love programming!"
blob = TextBlob(text)
print(blob.sentiment)  # Outputs polarity and subjectivity
How to Install These Libraries
Use pip to install each library:

bash

pip install transformers nltk spacy gensim torch tensorflow textblob
For models in Hugging Face, spaCy, or other libraries, make sure to download the appropriate pre-trained models (e.g., spacy download en_core_web_sm for spaCy).

Summary
For your use case, you are already on the right track with Hugging Face's transformers for text simplification and NLTK for tokenization and keyword extraction. If you want to dive deeper into NLP tasks, I recommend integrating these systems:

Hugging Face for advanced models like T5, BART, etc.
spaCy for more advanced linguistic processing.
Gensim for topic modeling and keyword extraction.
PyTorch or TensorFlow for custom model training or fine-tuning.
By combining these tools, you can handle a wide range of deep learning and machine learning tasks for your text processing and simplification project.

D:\>pip install transformers

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.44.2)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)

Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)

Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)

Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)

Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)

Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)

Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)

Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)

Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.19.1)

Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)

Requirement already satisfied: fsspec>=2023.5.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)

Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)

Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)

Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.3.2)

Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)

Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)

Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)





D:\>pip install gensim

Defaulting to user installation because normal site-packages is not writeable

Collecting gensim

  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)

Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from gensim) (1.26.4)

Collecting scipy<1.14.0,>=1.7.0 (from gensim)

  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)

Requirement already satisfied: smart-open>=1.8.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from gensim) (7.0.4)

Requirement already satisfied: wrapt in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)

Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.0/24.0 MB 3.3 MB/s eta 0:00:00

Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.9/45.9 MB 3.3 MB/s eta 0:00:00

Installing collected packages: scipy, gensim

  Attempting uninstall: scipy

    Found existing installation: scipy 1.14.1

    Uninstalling scipy-1.14.1:

      Successfully uninstalled scipy-1.14.1

  WARNING: Failed to remove contents in a temporary directory 'C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\~cipy.libs'.

  You can safely remove it manually.

  WARNING: Failed to remove contents in a temporary directory 'C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\~cipy'.

  You can safely remove it manually.

Successfully installed gensim-4.3.3 scipy-1.13.1



It seems that during the installation of the gensim library, the scipy library was downgraded from version 1.14.1 to version 1.13.1 because gensim has a dependency on a specific range of versions of scipy, namely >=1.7.0 and <1.14.0. This is a common situation when one library specifies compatibility with a particular version or range of versions for its dependencies.

In this case, gensim requested scipy version 1.13.1, and the installer downgraded it to satisfy the version constraint.

If you require the latest version of scipy (like 1.14.1 or beyond), you can manually upgrade it again using the following command:

bash

pip install --upgrade scipy
This will update scipy back to the latest version. However, keep in mind that this might cause issues with gensim if it requires an older version of scipy, so be cautious when manually upgrading dependencies for packages with specific version requirements.



D:\>pip install --upgrade scipy

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: scipy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (1.13.1)

Collecting scipy

  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)

Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from scipy) (1.26.4)

Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)

Installing collected packages: scipy

  Attempting uninstall: scipy

    Found existing installation: scipy 1.13.1

    Uninstalling scipy-1.13.1:

      Successfully uninstalled scipy-1.13.1

ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.

gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.

Successfully installed scipy-1.14.1





how to check the versions of all installed packages? and how to know which package depends on which package??? please write commands for python and please write a script in python to log the exhautive report for all packages and the versions dependency cases also



You can use pip to check the versions of all installed packages and understand their dependencies. Here's how to do it:

1. Check Installed Packages and Their Versions
To list all installed packages and their versions, use the following command in your terminal or command prompt:

bash

pip list
This will give you a list of all installed packages along with their versions.

2. Check Dependencies for a Specific Package
To check which packages a particular package depends on, you can use the following command:

bash

pip show <package_name>
For example, to check the dependencies of gensim, run:

bash

pip show gensim
This will display detailed information about the package, including its dependencies.

3. Check Reverse Dependencies (which packages depend on a specific package)
To find out which installed packages depend on a particular package, you can use:

bash

pipdeptree
If you don't have pipdeptree installed, you can install it via:

bash

pip install pipdeptree
Then, run the following to see the entire dependency tree:

bash

pipdeptree
You can also query for a specific package:

bash
pipdeptree --reverse --packages <package_name>
4. Python Script to Generate a Comprehensive Report of All Installed Packages, Their Versions, and Dependencies
You can use the following Python script to generate a report of all installed packages, their versions, and their dependencies (both direct and reverse).

Python Script:
python

import subprocess
import pkg_resources

def get_installed_packages():
    # Get a list of installed packages
    installed_packages = {pkg.key: pkg.version for pkg in pkg_resources.working_set}
    return installed_packages

def get_package_dependencies(package_name):
    # Get the dependencies for a particular package using pip show
    result = subprocess.run(['pip', 'show', package_name], capture_output=True, text=True)
    dependencies = []
    if result.returncode == 0:
        for line in result.stdout.splitlines():
            if line.startswith('Requires'):
                dependencies = line.split(':')[1].strip().split(', ')
                break
    return dependencies

def get_reverse_dependencies(package_name):
    # Get reverse dependencies using pipdeptree
    result = subprocess.run(['pipdeptree', '--reverse', '--packages', package_name], capture_output=True, text=True)
    reverse_dependencies = []
    if result.returncode == 0:
        reverse_dependencies = result.stdout.strip().splitlines()
    return reverse_dependencies

def generate_report():
    installed_packages = get_installed_packages()
    report = []

    for package, version in installed_packages.items():
        dependencies = get_package_dependencies(package)
        reverse_dependencies = get_reverse_dependencies(package)
        
        report.append({
            'Package': package,
            'Version': version,
            'Dependencies': dependencies,
            'Reverse Dependencies': reverse_dependencies
        })

    return report

def write_report_to_file(report, filename="package_report.txt"):
    with open(filename, 'w') as file:
        for package_info in report:
            file.write(f"Package: {package_info['Package']}\n")
            file.write(f"Version: {package_info['Version']}\n")
            file.write(f"Dependencies: {', '.join(package_info['Dependencies']) if package_info['Dependencies'] else 'None'}\n")
            file.write(f"Reverse Dependencies: {', '.join(package_info['Reverse Dependencies']) if package_info['Reverse Dependencies'] else 'None'}\n")
            file.write("\n" + "-"*50 + "\n")

if __name__ == '__main__':
    report = generate_report()
    write_report_to_file(report)
    print("Report generated and saved to 'package_report.txt'.")
Explanation:
get_installed_packages: Uses pkg_resources to get a dictionary of installed packages and their versions.
get_package_dependencies: Uses pip show to find the dependencies for a specific package.
get_reverse_dependencies: Uses pipdeptree to find which packages depend on the given package.
generate_report: Collects the installed packages, their dependencies, and reverse dependencies, and prepares the data for the report.
write_report_to_file: Saves the report to a text file (package_report.txt).
How to Run:
Ensure you have pipdeptree installed:
bash

pip install pipdeptree
Run the script in your Python environment:
bash

python generate_package_report.py
This will generate a report named package_report.txt, which includes information on each package, its version, its dependencies, and reverse dependencies.

Example Output Format:
yaml

Package: gensim
Version: 4.3.3
Dependencies: numpy, scipy, smart-open
Reverse Dependencies: None

--------------------------------------------------

Package: scipy
Version: 1.13.1
Dependencies: numpy
Reverse Dependencies: gensim, other_package_name
This should give you a comprehensive overview of your Python environment and package dependencies.

D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\installed_pypackages_versions_dependency_reports.py:2: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html

  import pkg_resources

Traceback (most recent call last):

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\installed_pypackages_versions_dependency_reports.py", line 55, in <module>

    report = generate_report()

             ^^^^^^^^^^^^^^^^^

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\installed_pypackages_versions_dependency_reports.py", line 34, in generate_report

    reverse_dependencies = get_reverse_dependencies(package)

                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\installed_pypackages_versions_dependency_reports.py", line 22, in get_reverse_dependencies

    result = subprocess.run(['pipdeptree', '--reverse', '--packages', package_name], capture_output=True, text=True)

             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Program Files\Python312\Lib\subprocess.py", line 548, in run

    with Popen(*popenargs, **kwargs) as process:

         ^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Program Files\Python312\Lib\subprocess.py", line 1026, in __init__

    self._execute_child(args, executable, preexec_fn, close_fds,

  File "C:\Program Files\Python312\Lib\subprocess.py", line 1538, in _execute_child

    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,

                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

FileNotFoundError: [WinError 2] The system cannot find the file specified

It seems that you're encountering two issues:

Deprecation Warning: The pkg_resources module is deprecated in favor of other tools. This warning can be ignored for now, but for long-term compatibility, you may want to consider using alternatives like importlib.metadata or other modern approaches for working with installed packages.

FileNotFoundError: The error message FileNotFoundError: [WinError 2] The system cannot find the file specified indicates that the pipdeptree command is not found in your environment. This is likely because the pipdeptree package is not installed or is not properly recognized by your system.

Steps to Resolve:
Fix the pipdeptree Not Found Issue:

First, ensure that pipdeptree is installed. You can install it using pip:
bash

pip install pipdeptree
If it is already installed, make sure that your Python environment's Scripts directory is included in your system's PATH.
Alternative Method Without pipdeptree: If you don't want to use pipdeptree or can't get it to work, you can use a different approach to get reverse dependencies using a Python script. Here's how you can modify the script to skip pipdeptree and simply list dependencies using pip show for each installed package.

Updated Python Script Without pipdeptree:
python

import subprocess
import sys

def get_installed_packages():
    # Get a list of installed packages
    installed_packages = {}
    result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True)
    if result.returncode == 0:
        for line in result.stdout.splitlines():
            package, version = line.split('==')
            installed_packages[package] = version
    return installed_packages

def get_package_dependencies(package_name):
    # Get the dependencies for a particular package using pip show
    result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], capture_output=True, text=True)
    dependencies = []
    if result.returncode == 0:
        for line in result.stdout.splitlines():
            if line.startswith('Requires'):
                dependencies = line.split(':')[1].strip().split(', ')
                break
    return dependencies

def generate_report():
    installed_packages = get_installed_packages()
    report = []

    for package, version in installed_packages.items():
        dependencies = get_package_dependencies(package)
        report.append({
            'Package': package,
            'Version': version,
            'Dependencies': dependencies if dependencies else 'None'
        })

    return report

def write_report_to_file(report, filename="package_report.txt"):
    with open(filename, 'w') as file:
        for package_info in report:
            file.write(f"Package: {package_info['Package']}\n")
            file.write(f"Version: {package_info['Version']}\n")
            file.write(f"Dependencies: {', '.join(package_info['Dependencies']) if isinstance(package_info['Dependencies'], list) else package_info['Dependencies']}\n")
            file.write("\n" + "-"*50 + "\n")

if __name__ == '__main__':
    report = generate_report()
    write_report_to_file(report)
    print("Report generated and saved to 'package_report.txt'.")
Explanation:
get_installed_packages: Uses pip freeze to list all installed packages and their versions. This will replace the use of pkg_resources.
get_package_dependencies: Uses pip show to get the dependencies for each installed package.
Report Generation: It collects the package name, version, and dependencies and writes them into a text file.
No pipdeptree Needed: This script only relies on pip show and pip freeze, so you won't need pipdeptree.
How to Run:
Ensure that pip is properly installed in your environment and that your Scripts folder is in the PATH if you're having issues with command not found errors.
Run the updated script:
bash

python generate_package_report.py
This will generate a report (package_report.txt) that includes all installed packages, their versions, and their dependencies without requiring pipdeptree.





D:\>pip install pipdeptree

Defaulting to user installation because normal site-packages is not writeable

Collecting pipdeptree

  Downloading pipdeptree-2.24.0-py3-none-any.whl.metadata (16 kB)

Requirement already satisfied: packaging>=24.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pipdeptree) (24.1)

Requirement already satisfied: pip>=24.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pipdeptree) (24.3.1)

Downloading pipdeptree-2.24.0-py3-none-any.whl (32 kB)

Installing collected packages: pipdeptree

Successfully installed pipdeptree-2.24.0



D:\>pip install pipdeptree

Defaulting to user installation because normal site-packages is not writeable

Collecting pipdeptree

  Downloading pipdeptree-2.24.0-py3-none-any.whl.metadata (16 kB)

Requirement already satisfied: packaging>=24.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pipdeptree) (24.1)

Requirement already satisfied: pip>=24.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pipdeptree) (24.3.1)

Downloading pipdeptree-2.24.0-py3-none-any.whl (32 kB)

Installing collected packages: pipdeptree

Successfully installed pipdeptree-2.24.0



D:\>pip freeze
absl-py==2.1.0

altgraph==0.17.4

annotated-types==0.7.0

anyio==4.6.0

argon2-cffi==23.1.0

argon2-cffi-bindings==21.2.0

arrow==1.3.0

asttokens==2.4.1

async-lru==2.0.4

attrs==24.2.0

babel==2.16.0

beautifulsoup4==4.12.3

bleach==6.1.0

blis==0.7.11

bokeh==3.6.0

cachetools==5.5.0

cairocffi==1.7.1

CairoSVG==2.7.1

catalogue==2.0.10

certifi==2024.8.30

cffi==1.17.1

chardet==5.2.0

charset-normalizer==3.3.2

chex==0.1.86

click==8.1.7

cloudpathlib==0.19.0

cloudpickle==3.0.0

cloup==3.0.5

colorama==0.4.6

comm==0.2.2

confection==0.1.5

contourpy==1.3.0

cryptography==43.0.1

cssselect2==0.7.0

cycler==0.12.1

cymem==2.0.8

dask==2024.9.1

dask-expr==1.1.15

debugpy==1.8.6

decorator==5.1.1

defusedxml==0.7.1

distributed==2024.9.1

distro==1.9.0

dm-tree==0.1.8

docstring_parser==0.16

dxf==1.1.1

emoji==2.12.1

en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889

et_xmlfile==2.0.0

etils==1.9.4

executing==2.1.0

ezdxf==1.3.3

fastjsonschema==2.20.0

filelock==3.16.1

flax==0.9.0

fonttools==4.53.1

fqdn==1.5.1

fsspec==2024.9.0

gensim==4.3.3

glcontext==3.0.0

google-ai-generativelanguage==0.6.10

google-api-core==2.21.0

google-api-python-client==2.149.0

google-auth==2.35.0

google-auth-httplib2==0.2.0

google-generativeai==0.8.3

googleapis-common-protos==1.65.0

grpcio==1.67.0

grpcio-status==1.67.0

h11==0.14.0

h5py==3.12.1

httpcore==1.0.5

httplib2==0.22.0

httpx==0.27.2

huggingface-hub==0.25.0

humanize==4.10.0

idna==3.10

immutabledict==4.2.0

importlib_resources==6.4.5

ipykernel==6.29.5

ipython==8.27.0

isoduration==20.11.0

isosurfaces==0.1.2

jax==0.4.33

jaxlib==0.4.33

jedi==0.19.1

Jinja2==3.1.4

jiter==0.5.0

joblib==1.4.2

json5==0.9.25

jsonpointer==3.0.0

jsonschema==4.23.0

jsonschema-specifications==2023.12.1

jupyter-events==0.10.0

jupyter-lsp==2.2.5

jupyter_client==8.6.3

jupyter_core==5.7.2

jupyter_server==2.14.2

jupyter_server_terminals==0.5.3

jupyterlab==4.2.5

jupyterlab_pygments==0.3.0

jupyterlab_server==2.27.3

kagglehub==0.3.3

keras==3.6.0

keras-core==0.1.7

keras-cv==0.9.0

kiwisolver==1.4.7

langcodes==3.4.0

language_data==1.2.0

locket==1.0.0

lxml==5.3.0

lz4==4.3.3

manim==0.18.1

ManimPango==0.6.0

mapbox_earcut==1.0.2

marisa-trie==1.2.0

markdown-it-py==3.0.0

MarkupSafe==2.1.5

matplotlib==3.9.2

matplotlib-inline==0.1.7

mdurl==0.1.2

MIDIUtil==1.2.1

mido==1.3.3

mistune==3.0.2

ml_dtypes==0.5.0

moderngl==5.12.0

moderngl-window==3.0.3

mpmath==1.3.0

msgpack==1.1.0

murmurhash==1.0.10

mutagen==1.47.0

namex==0.0.8

nbclient==0.10.0

nbconvert==7.16.4

nbformat==5.10.4

nest-asyncio==1.6.0

networkx==3.3

nltk==3.9.1

notebook==7.2.2

notebook_shim==0.2.4

numpy==1.26.4

openai==1.50.2

opencv-python==4.10.0.84

openpyxl==3.1.5

opt-einsum==3.3.0

optax==0.2.3

optree==0.13.0

orbax-checkpoint==0.6.4

overrides==7.7.0

packaging==24.1

pandas==2.2.2

pandocfilters==1.5.1

parso==0.8.4

partd==1.4.2

pdf2image==1.17.0

pdfminer.six==20231228

pdfplumber==0.11.4

pefile==2023.2.7

pillow==10.4.0

pipdeptree==2.24.0

platformdirs==4.3.6

preshed==3.0.9

pretty_midi==0.2.10

prometheus_client==0.21.0

promise==2.3

prompt_toolkit==3.0.48

proto-plus==1.25.0

protobuf==5.28.1

psutil==6.0.0

pure_eval==0.2.3

py4j==0.10.9.7

pyarrow==17.0.0

pyasn1==0.6.1

pyasn1_modules==0.4.1

pycairo==1.27.0

pycparser==2.22

pydantic==2.9.2

pydantic_core==2.23.4

pydub==0.25.1

pyglet==2.0.20

PyGLM==2.7.3

Pygments==2.18.0

pyinstaller==6.11.1

pyinstaller-hooks-contrib==2024.10

PyMuPDF==1.24.10

PyMuPDFb==1.24.10

pyparsing==3.1.4

PyPDF2==3.0.1

pypdfium2==4.30.0

pyphen==0.17.0

pyspark==3.5.3

pytesseract==0.3.13

python-dateutil==2.9.0.post0

python-json-logger==2.0.7

PyTuning==0.7.3

pytz==2024.2

pywin32==306

pywin32-ctypes==0.2.3

pywinpty==2.0.13

PyYAML==6.0.2

pyzmq==26.2.0

referencing==0.35.1

regex==2024.9.11

reportlab==4.2.2

requests==2.32.3

rfc3339-validator==0.1.4

rfc3986-validator==0.1.1

rich==13.8.1

rpds-py==0.20.0

rsa==4.9

safetensors==0.4.5

scikit-learn==1.5.2

scipy==1.14.1

screeninfo==0.8.1

Send2Trash==1.8.3

sentencepiece==0.2.0

setuptools==75.1.0

shellingham==1.5.4

simple-parsing==0.1.6

six==1.16.0

skia-pathops==0.8.0.post2

smart-open==7.0.4

sniffio==1.3.1

sortedcontainers==2.4.0

soupsieve==2.6

spacy==3.7.6

spacy-legacy==3.0.12

spacy-loggers==1.0.5

spacy-stanfordnlp==0.1.3

srsly==2.4.8

srt==3.5.3

stack-data==0.6.3

stanfordnlp==0.2.0

stanza==1.9.2

svgelements==1.9.6

svglib==1.5.1

sympy==1.13.2

tblib==3.0.0

tensorflow-datasets==4.9.6

tensorflow-metadata==1.16.1

tensorstore==0.1.65

termcolor==2.5.0

terminado==0.18.1

textblob==0.18.0.post0

thinc==8.2.5

threadpoolctl==3.5.0

tinycss2==1.3.0

tokenizers==0.19.1

toml==0.10.2

toolz==0.12.1

torch==2.4.1

torchvision==0.19.1

tornado==6.4.1

tqdm==4.66.5

traitlets==5.14.3

transformers==4.44.2

typer==0.12.5

types-python-dateutil==2.9.0.20240906

typing_extensions==4.12.2

tzdata==2024.1

uri-template==1.3.0

uritemplate==4.1.1

urllib3==2.2.3

vaderSentiment==3.3.2

wasabi==1.1.3

watchdog==6.0.0

wcwidth==0.2.13

weasel==0.4.1

webcolors==24.8.0

webencodings==0.5.1

websocket-client==1.8.0

wordcloud==1.9.3

wrapt==1.16.0

xyzservices==2024.9.0

zict==3.0.0

zipp==3.20.2



D:\>

It looks like you're encountering two issues:

FileNotFoundError for pipdeptree: This error indicates that pipdeptree is either not installed or cannot be found by your Python environment.

ValueError in get_installed_packages: The error suggests that the line being processed does not contain the expected == symbol, which is usually found in the output of pip freeze. This can happen if pip freeze produces an empty line or if there are other unexpected lines.

Solutions
Install pipdeptree (if you want to keep using it):

If you want to use pipdeptree (which is used to get reverse dependencies), make sure it's installed:
bash

pip install pipdeptree
Handle Empty or Unexpected Lines in pip freeze Output: The ValueError suggests that pip freeze might be producing an empty line or an unexpected output. We can improve the script by adding error handling for such cases.

Here’s an updated version of the script that includes checks for empty lines and handles the pip freeze output more robustly.

Updated Script:
python
import subprocess
import sys

def get_installed_packages():
    # Get a list of installed packages
    installed_packages = {}
    result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True)
    if result.returncode == 0:
        for line in result.stdout.splitlines():
            # Skip empty lines and lines that don't follow the "package==version" format
            if '==' in line:
                package, version = line.split('==')
                installed_packages[package] = version
    return installed_packages

def get_package_dependencies(package_name):
    # Get the dependencies for a particular package using pip show
    result = subprocess.run([sys.executable, '-m', 'pip', 'show', package_name], capture_output=True, text=True)
    dependencies = []
    if result.returncode == 0:
        for line in result.stdout.splitlines():
            if line.startswith('Requires'):
                dependencies = line.split(':')[1].strip().split(', ')
                break
    return dependencies

def generate_report():
    installed_packages = get_installed_packages()
    report = []

    for package, version in installed_packages.items():
        dependencies = get_package_dependencies(package)
        report.append({
            'Package': package,
            'Version': version,
            'Dependencies': dependencies if dependencies else 'None'
        })

    return report

def write_report_to_file(report, filename="package_report.txt"):
    with open(filename, 'w') as file:
        for package_info in report:
            file.write(f"Package: {package_info['Package']}\n")
            file.write(f"Version: {package_info['Version']}\n")
            file.write(f"Dependencies: {', '.join(package_info['Dependencies']) if isinstance(package_info['Dependencies'], list) else package_info['Dependencies']}\n")
            file.write("\n" + "-"*50 + "\n")

if __name__ == '__main__':
    report = generate_report()
    write_report_to_file(report)
    print("Report generated and saved to 'package_report.txt'.")
Changes Made:
Skip Empty Lines: In get_installed_packages(), I added a condition to skip lines that don't contain ==, which can happen with empty lines or malformed output.

Handle Malformed Lines Gracefully: If a line doesn't contain ==, it will be ignored without causing an error.

How to Run:
Ensure pip is properly installed and your environment is correctly configured.
Run the updated script:
bash
python refined_installed_pypackages_versions_dependency_reports.py
Additional Debugging:
If the error persists or if you're still facing problems, please check the output of pip freeze by running the following command in your terminal:

bash

pip freeze
This will show you the exact format of the output, helping to pinpoint any irregularities that could cause issues in parsing the output.





D:\>pip install textblob

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: textblob in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.18.0.post0)

Requirement already satisfied: nltk>=3.8 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from textblob) (3.9.1)

Requirement already satisfied: click in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk>=3.8->textblob) (8.1.7)

Requirement already satisfied: joblib in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk>=3.8->textblob) (1.4.2)

Requirement already satisfied: regex>=2021.8.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk>=3.8->textblob) (2024.9.11)

Requirement already satisfied: tqdm in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk>=3.8->textblob) (4.66.5)

Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from click->nltk>=3.8->textblob) (0.4.6)



if i want to find all the graphics from the pages of the pdf files and report the coordinates for the engineering graphics on the page then what are the best pretrained models ??/Will these work???For your use case, you are already on the right track with Hugging Face's transformers for text simplification and NLTK for tokenization and keyword extraction. If you want to dive deeper into NLP tasks, I recommend integrating these systems:

Hugging Face for advanced models like T5, BART, etc.

spaCy for more advanced linguistic processing.

Gensim for topic modeling and keyword extraction.

PyTorch or TensorFlow for custom model training or fine-tuning.



To extract and report the coordinates of engineering graphics from PDF files, you need specialized tools and pretrained models that can process and identify graphical elements in the PDF pages. This is a combination of Computer Vision and Document Layout Analysis tasks. While Hugging Face's transformers and NLP models like T5, BART, etc., are excellent for text-based tasks, they aren't directly designed for image extraction or graphical element analysis.

For extracting graphical elements from PDFs and reporting coordinates, here are the steps and tools you can consider:

1. Pretrained Models for Graphics and Layout Extraction
For your use case, you will need to look at models and frameworks specialized in Document Layout Analysis and Computer Vision to extract graphical elements. Below are some of the best pretrained models and tools for this task:

Layout Analysis Models:
LayoutLM (Hugging Face): LayoutLM is a pretrained model specifically designed for document layout analysis. It works by combining textual and visual features from documents to understand the layout and context. It's great for tasks like extracting both text and graphical elements in documents like PDFs. The model can help locate and classify different sections, including text and images, in documents.

Detectron2 (by Facebook AI): Detectron2 is a highly flexible and powerful library for object detection tasks. You can fine-tune it for detecting engineering drawings, images, and other graphics in PDF pages if you have a custom dataset. It can return coordinates for detected objects, including graphs, images, or complex diagrams.

PDFMiner + OpenCV: PDFMiner is great for extracting text and other metadata from PDF files. For graphics, you can use OpenCV in conjunction with PDFMiner to detect and extract images or shapes from the pages. By combining PDF parsing with OpenCV, you can identify graphical elements like line diagrams, charts, or engineering drawings and then extract their coordinates.

OCR and Image Processing Models:
Tesseract OCR: Tesseract is a widely-used OCR tool that can extract text from images, including scanned engineering diagrams. While Tesseract itself won’t directly give you the coordinates of graphical elements, you can use it to convert images to text and then apply image processing techniques to find the coordinates of the graphics.

YOLO (You Only Look Once): If your engineering graphics involve specific objects or symbols (like engineering icons or components), you could fine-tune a YOLO model for object detection to identify these symbols and return their bounding box coordinates.

2. Approach to Extracting Graphics and Coordinates
Here is a potential workflow to detect engineering graphics and extract coordinates:

Extract the Images and Graphical Elements:

Use a PDF parsing tool like PyMuPDF (fitz) or PDFMiner to extract the pages of the PDF as images (rasterized PDFs). You can then process these images using computer vision models.
Object Detection with a Pretrained Model:

Apply an object detection model like YOLO, Detectron2, or Faster R-CNN on the extracted images. These models can help identify objects or drawings, and they will return bounding box coordinates (x, y, width, height) for the detected objects.
Image Preprocessing:

If you're using Tesseract or other OCR models, ensure that the images are cleaned and preprocessed (e.g., using OpenCV for binarization, edge detection, or contour finding) to improve the detection of graphical elements.
Text and Graphics Fusion:

If you're working with both text and graphics (e.g., if text labels or captions are present on diagrams), models like LayoutLM could be useful to link the extracted text with the relevant graphic coordinates on the page.
3. Pipeline Example
Here’s how you can structure the pipeline:

Step 1: Extract PDF page as image

Use libraries like PyMuPDF (fitz) to rasterize the PDF into images.
Example: pdf2image or PyMuPDF can extract the pages of a PDF and convert them into images.
Step 2: Detect graphics with a pretrained object detection model (Detectron2 or YOLO)

Fine-tune or use a pretrained model on images of engineering drawings.
The model will output the coordinates (bounding boxes) of detected graphics.
Step 3: Combine with OCR (optional)

Use Tesseract OCR to extract any textual data around the images if necessary.
Use LayoutLM if you need to capture relationships between text and images.
Step 4: Report the Coordinates

Once the model identifies the coordinates of the graphics, you can save or report the bounding boxes as per your requirement.
4. Libraries and Tools
PyMuPDF (fitz): For parsing and extracting images from PDF files.

python

import fitz  # PyMuPDF
doc = fitz.open("your_file.pdf")
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    pix = page.get_pixmap()  # Convert page to image
    pix.save(f"page_{page_num}.png")
Detectron2: A library by Facebook AI for object detection tasks, including custom model training for detecting engineering graphics.

bash

pip install detectron2
YOLOv5: Another popular object detection model, which you can fine-tune on your engineering drawing dataset.

bash

pip install torch torchvision
Tesseract OCR: For OCR-based text extraction if the diagrams contain labels or annotations.

bash

pip install pytesseract
5. Reporting the Coordinates
Once the graphics are detected, you can create a report in your desired format (e.g., JSON, CSV). Here's an example of how you could format the output:

json

{
    "page": 1,
    "graphics": [
        {
            "type": "line_graph",
            "coordinates": {
                "x": 150,
                "y": 200,
                "width": 300,
                "height": 400
            },
            "labels": ["Label1", "Label2"]
        },
        {
            "type": "diagram",
            "coordinates": {
                "x": 500,
                "y": 600,
                "width": 100,
                "height": 100
            },
            "labels": ["Component A", "Component B"]
        }
    ]
}
Conclusion
For your use case, you can combine LayoutLM for text and layout analysis, Detectron2 or YOLO for object detection (graphics extraction), and PyMuPDF or PDFMiner for PDF processing. This multi-step approach will help you detect engineering graphics and report their coordinates effectively.



D:\>pip install torch torchvision

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: torch in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (2.4.1)

Requirement already satisfied: torchvision in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.19.1)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.16.1)

Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (4.12.2)

Requirement already satisfied: sympy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (1.13.2)

Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.3)

Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.1.4)

Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (2024.9.0)

Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (75.1.0)

Requirement already satisfied: numpy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torchvision) (1.26.4)

Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torchvision) (10.4.0)

Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch) (2.1.5)

Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy->torch) (1.3.0)





D:\>pip install pytesseract

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: pytesseract in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.3.13)

Requirement already satisfied: packaging>=21.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pytesseract) (24.1)

Requirement already satisfied: Pillow>=8.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pytesseract) (10.4.0)



To detect graphics, extract text (OCR), and report the coordinates of graphical objects (such as SVGs or other vector graphics) from a PDF, you'll need a more comprehensive solution that combines PDF parsing, OCR, and image processing. Here's how you can approach this:

Steps for Extracting Text, Graphics, and Coordinates:
Extract Text: Use PyMuPDF to extract both text and graphical elements. For text, PyMuPDF can extract text and its position on the page.

Extract Images and Vector Graphics: For extracting vector graphics (like SVGs or lines), you'll need to use PDF parsing libraries like PyMuPDF or pdf2svg. PyMuPDF can help with rasterizing the PDF to extract images and vector graphics.

Perform OCR (if needed): If the PDF contains scanned images of text (like scanned engineering diagrams), you'll need to run OCR to extract any text. For this, you can use Tesseract OCR.

Detect Coordinates: You can detect the coordinates of elements using PDF parsing libraries (e.g., PyMuPDF for text extraction) or object detection models for more complex graphics.

Example Code for Extracting Text and Coordinates
Here’s an example of how you can extract both text and graphics from a PDF using PyMuPDF and Tesseract OCR for text in images.

1. Extracting Text and Coordinates with PyMuPDF
python
Copy code
import fitz  # PyMuPDF

# Open the PDF file
doc = fitz.open("AISC STEEL CHART rev 11 (Draft copy).pdf")

# Iterate over each page
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    
    # Extract text and coordinates
    text_instances = page.search_for('')  # You can put a keyword or leave empty for all text
    for inst in text_instances:
        x0, y0, x1, y1 = inst  # Coordinates of the text instance
        print(f"Text found at ({x0}, {y0}, {x1}, {y1}):")
        print(page.get_text('text', clip=inst))  # Get the actual text
    
    # Convert the page to image (pixmap)
    pix = page.get_pixmap()  # Convert page to image
    pix.save(f"page_{page_num}.png")  # Save as PNG
In the code above:

page.search_for('') searches for text, and you can replace the empty string with a keyword.
The coordinates (x0, y0, x1, y1) represent the bounding box of the detected text on the page.
pix = page.get_pixmap() rasterizes the page into an image, which you can save as PNG.
2. Perform OCR on Images with Tesseract
If your PDF contains scanned images or images with text, you can use Tesseract OCR to extract text from images.

python

import pytesseract
from PIL import Image

# Example: Read the image from the page
img = Image.open("page_0.png")  # This would be the saved page image
text = pytesseract.image_to_string(img)
print("OCR Text:", text)
Make sure to install Tesseract OCR:

bash

pip install pytesseract
You'll also need to install Tesseract itself on your system (from here).

3. Extracting Vector Graphics from PDFs
PDFs may contain vector graphics (like SVG or lines). PyMuPDF can help with extracting vector elements such as paths and images.

python

import fitz  # PyMuPDF

doc = fitz.open("AISC STEEL CHART rev 11 (Draft copy).pdf")

# Iterate through pages and extract vector graphics
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    
    # Extracting vector graphics elements (paths, shapes, etc.)
    shapes = page.get_drawings()  # This will get vector drawing elements (lines, shapes)
    
    for shape in shapes:
        print(f"Shape found: {shape}")
        # You can further process the coordinates, like the bounding box or path points
        
    # Rasterize the page to extract images if needed
    pix = page.get_pixmap()
    pix.save(f"page_{page_num}.png")
4. Handling SVGs in PDFs
If you're specifically looking for SVG (Scalable Vector Graphics) embedded in PDFs, PyMuPDF is not designed to directly extract SVGs, but you can use tools like pdf2svg to convert PDF vector content into SVG format:

bash

sudo apt install pdf2svg
pdf2svg input.pdf output.svg
This will convert the PDF vector content into SVG format, which you can then analyze or process.

5. Detecting and Reporting Coordinates
If you need to detect specific graphical objects (e.g., charts, engineering diagrams) and report their coordinates, you can use object detection models like YOLO or Faster R-CNN after converting the PDF to images. These models can detect objects and return bounding boxes with coordinates.

Example Using YOLO (for Object Detection):
Prepare your dataset (e.g., labeled diagrams).
Train YOLO on your dataset for detecting engineering graphics.
Run YOLO on the images extracted from the PDF to detect and get coordinates.
Final Summary
Text extraction: Use PyMuPDF for extracting both text and its coordinates from PDFs.
Image extraction: Convert PDF pages to images using PyMuPDF and then use Tesseract OCR to extract text from images.
Vector graphics: Use PyMuPDF to detect vector elements (paths, lines, etc.), and for SVGs, use tools like pdf2svg.
Object detection for graphics: You can fine-tune object detection models like YOLO or Detectron2 on your dataset to identify and extract coordinates of specific graphical objects.
This should help you get started with extracting text, graphics, and their coordinates from PDF files.

It seems that you're using Windows, where the sudo command (common in Linux and macOS) is not available. To install and use pdf2svg on Windows, you'll need to use a different approach. Here's how you can proceed:

1. Install pdf2svg on Windows
You can install pdf2svg on Windows by following these steps:

Download pdf2svg for Windows:

You can download precompiled binaries for pdf2svg from this GitHub repository.
Look for the Windows binaries in the releases section or build it yourself from source if needed.
Extract the files:

Once you download the .zip file, extract it to a folder.
Add the folder to the System Path:

To make pdf2svg easily accessible from the command line, add its folder to the Windows Path environment variable.
Steps:

Right-click on This PC (or My Computer) and select Properties.
Click on Advanced system settings on the left side.
In the System Properties window, click on the Environment Variables button.
In the System Variables section, scroll down and find the Path variable. Select it and click Edit.
Click New and add the folder path where you extracted pdf2svg.
Click OK to save and close all the dialog boxes.
Verify Installation:

Open a new Command Prompt window and type pdf2svg to verify that the tool is working correctly. You should see its usage instructions or version info if it's installed correctly.
2. Use pdf2svg on Windows
Once installed, you can convert PDF files to SVG format using the following command in the Command Prompt:

bash

pdf2svg input.pdf output.svg
This will convert your PDF file (input.pdf) into an SVG file (output.svg). You can now process the SVG file as needed.

Alternative: Using PyMuPDF for Vector Extraction
If pdf2svg is difficult to set up, PyMuPDF (the library you're already using) can extract vector elements from a PDF. Here's how you can extract graphics and their coordinates from a PDF using PyMuPDF:

python

import fitz  # PyMuPDF

# Open the PDF file
doc = fitz.open("your_file.pdf")

# Iterate through pages and extract vector graphics
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    
    # Extracting vector graphics elements (paths, shapes, etc.)
    shapes = page.get_drawings()  # This will get vector drawing elements (lines, shapes)
    
    for shape in shapes:
        print(f"Shape found: {shape}")
        # You can further process the coordinates, like the bounding box or path points
        
    # Convert the page to image to extract any rasterized elements if needed
    pix = page.get_pixmap()
    pix.save(f"page_{page_num}.png")
This code extracts vector elements (shapes, paths) and saves the page as an image for rasterized elements.

Conclusion
For Windows, you can either install pdf2svg using the method outlined above or use PyMuPDF to extract vector graphics.
If you prefer a pure Python solution, PyMuPDF is a good alternative to extract vector-based data from PDFs.


D:\>pdf2svg

'pdf2svg' is not recognized as an internal or external command,

operable program or batch file. what are these??/If you need to detect specific graphical objects (e.g., charts, engineering diagrams) and report their coordinates, you can use object detection models like YOLO or Faster R-CNN after converting the PDF to images. These models can detect objects and return bounding boxes with coordinates.

Example Using YOLO (for Object Detection):

Prepare your dataset (e.g., labeled diagrams).

Train YOLO on your dataset for detecting engineering graphics.

Run YOLO on the images extracted from the PDF to detect and get coordinates.



It seems that the pdf2svg command is still not recognized on your system, which could be due to it not being installed correctly or not added to your system's PATH.

1. Install pdf2svg on Windows
To install and use pdf2svg on Windows without using Linux-specific commands (like sudo), follow these steps:

Step 1: Download the Windows binary for pdf2svg:

You can download a Windows version of pdf2svg from this GitHub release page.
Look for a precompiled .zip file in the "Assets" section (e.g., pdf2svg-win64.zip).
Step 2: Extract the files:

Extract the .zip file to a directory (e.g., C:\pdf2svg).
Step 3: Add pdf2svg to your system's PATH:

Right-click on "This PC" (or "My Computer") and select Properties.
Select Advanced system settings on the left.
Click the Environment Variables button at the bottom.
In the System Variables section, find and select the Path variable, and then click Edit.
Add the path to the folder where pdf2svg is located (e.g., C:\pdf2svg).
Click OK to save the changes.
Step 4: Verify installation:

Open a new Command Prompt window and type pdf2svg. If correctly installed, you should see the pdf2svg usage details.
Alternative: Object Detection Using YOLO or Faster R-CNN
If you're looking to detect specific graphical objects (like charts, engineering diagrams, etc.) from PDF files, the workflow you're considering (using YOLO or Faster R-CNN after converting the PDF pages into images) is appropriate. Below is an overview of how you can approach this problem:

Step-by-Step Object Detection using YOLO or Faster R-CNN:
1. Extract images from PDF using PyMuPDF
First, extract the pages from the PDF as images (this can be done using PyMuPDF as you're already doing):

python

import fitz  # PyMuPDF

# Open the PDF file
doc = fitz.open("your_file.pdf")

# Convert each page to an image
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    pix = page.get_pixmap()  # Convert page to image
    pix.save(f"page_{page_num}.png")  # Save the image
2. Prepare Dataset for Object Detection
Labeling Data: To train an object detection model (like YOLO or Faster R-CNN), you'll need to label your dataset (e.g., the diagrams or charts within the PDF images) with bounding boxes. You can use tools like LabelImg or CVAT to annotate the diagrams and charts on the images.

Each annotation should consist of a bounding box around each graphical object (chart, diagram) with class labels.
Export the annotations in a format suitable for YOLO or Faster R-CNN (usually in text files with coordinates).
3. Train YOLO or Faster R-CNN
YOLO (You Only Look Once) is a popular and efficient object detection model. You can use a pre-trained model and fine-tune it on your dataset, or train it from scratch if you have a large dataset.

Here's a general outline of how to proceed:

Install YOLO dependencies: You can use libraries like Darknet or Ultralytics YOLOv5 for training YOLO models.
Prepare the dataset in the required format (images + annotations).
Train the model using the prepared dataset.
Evaluate the trained model on new images to detect the objects and report the coordinates.
Faster R-CNN is another powerful object detection model. It's slower but more accurate than YOLO for some tasks. You can use Detectron2 (a PyTorch-based library) for Faster R-CNN.

4. Run Object Detection on PDF Pages
Once you've trained your model (YOLO or Faster R-CNN), you can apply it to the images extracted from the PDF files:

python

from your_model import YOLO  # or Faster R-CNN model

# Load your trained model
model = YOLO.load('path_to_model')  # or model = load_detectron_model()

# Run detection on each page image
for page_num in range(len(doc)):
    image_path = f"page_{page_num}.png"
    results = model.detect(image_path)
    
    # Extract the bounding box coordinates
    for result in results:
        print(f"Object detected: {result['label']} - Coordinates: {result['bbox']}")
In this code, the detect() method of the trained model will return a list of detected objects, where each object contains its label and bounding box coordinates (usually in the form of [x_min, y_min, x_max, y_max]).

Conclusion:
If pdf2svg is not working for you, you can continue with PyMuPDF to extract PDF page images and process them.
Use YOLO or Faster R-CNN for detecting specific objects in the images (such as engineering diagrams) and report their coordinates.
You'll need a labeled dataset for training the detection model.
This approach will allow you to detect graphical elements in your PDF files and report their positions as bounding box coordinates.

If pdf2svg is costly or not suitable for your needs, you can explore several free and alternative methods for converting PDF files to SVG or image formats. Here's a list of options that you can use to extract graphics or diagrams from PDFs without using pdf2svg:

1. PyMuPDF (fitz)
PyMuPDF allows you to extract images from PDFs. You can save each page as an image (PNG, JPEG, etc.) and later apply object detection models on these images.

Example:

python

import fitz  # PyMuPDF

# Open the PDF file
doc = fitz.open("your_file.pdf")

# Convert each page to an image
for page_num in range(len(doc)):
    page = doc.load_page(page_num)
    pix = page.get_pixmap()  # Convert page to image
    pix.save(f"page_{page_num}.png")  # Save the image
Once you've extracted the images, you can proceed with object detection on them.

2. pdf2image (Free alternative to pdf2svg)
pdf2image is another library that allows you to convert PDF files to images (JPEG/PNG). It’s a great alternative to pdf2svg and doesn't require costly setup.

Installation:

bash

pip install pdf2image
Usage:

python

from pdf2image import convert_from_path

# Convert PDF to images
images = convert_from_path('your_file.pdf')

# Save each page as an image (PNG or JPEG)
for i, image in enumerate(images):
    image.save(f'page_{i}.png', 'PNG')
After converting the PDF pages into images, you can perform object detection to extract the coordinates of engineering graphics.

3. Inkscape (Free Software)
You can use Inkscape, a popular open-source vector graphics editor, to convert PDF files to SVG format. Inkscape can be run from the command line, making it scriptable for automated workflows.

Example Command:

bash

inkscape input.pdf --export-plain-svg=output.svg
Inkscape is free, but you need to install it manually. After installation, it can be used through the command line or integrated into your Python script.

4. Poppler (pdftohtml)
Poppler is a free software library for rendering PDF documents, and it provides the pdftohtml tool which can convert PDF to HTML (SVG is often used for graphics in HTML).

Installation (Linux / Windows): For Linux-based systems, you can install it with:

bash

sudo apt-get install poppler-utils
For Windows, you can download the binaries from Poppler for Windows.

Usage:

bash

pdftohtml -c -hidden -xml your_file.pdf output.html
This command will convert the PDF into an HTML file, with embedded SVG elements. You can then extract the coordinates of the graphics from the SVG using XML parsing libraries in Python.

5. Other Open-Source Alternatives for Object Detection
If your goal is to detect and extract graphical objects like charts or diagrams, here are some free tools that you can combine with image conversion:

OpenCV: For detecting shapes, charts, and lines within images.
Tesseract OCR: For extracting any textual information from the images after conversion (useful for diagrams with labels).
6. Combining PDF to Image Conversion and Object Detection
Once you convert the PDF pages into images (either via PyMuPDF, pdf2image, or other tools), you can use object detection models like YOLO, Faster R-CNN, or OpenCV to detect graphical objects.

Example with OpenCV and contour detection:
python

import cv2
import numpy as np

# Load the page image
image = cv2.imread("page_0.png")

# Convert to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Apply threshold to get binary image
_, threshold = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)

# Find contours (objects)
contours, _ = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Draw bounding boxes around detected objects
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

# Save the output
cv2.imwrite("output.png", image)
This script uses OpenCV to detect and outline contours (objects) within the image, which might represent the diagrams or charts from your PDF.

Summary
PyMuPDF (fitz) and pdf2image are good free alternatives for converting PDF to images, which you can later process with object detection models.
Inkscape and Poppler can help convert PDF to SVG if that's needed.
You can integrate OpenCV or object detection models (like YOLO) on top of these images to extract the coordinates of graphical objects.
These methods should allow you to extract and detect engineering graphics or charts from PDFs without the need for pdf2svg, and they are cost-effective alternatives.

























































use pymupdf to do all these import matplotlib.pyplot as plt

import networkx as nx

import pandas as pd

from collections import Counter, defaultdict

from nltk import pos_tag, word_tokenize, ngrams

from nltk.corpus import stopwords, wordnet

from nltk.stem import WordNetLemmatizer

from wordcloud import WordCloud

from tkinter import Tk, filedialog

from PyPDF2 import PdfWriter, PdfReader

from svglib.svglib import svg2rlg

from reportlab.graphics import renderPDF

import io

import string

import logging # Import for logging errors

from nltk.stem import PorterStemmer

# Initialize the Porter Stemmer for stemming

stemmer = PorterStemmer()

# Initialize NLP tools

lemmatizer = WordNetLemmatizer()

stop_words = set(stopwords.words('english'))

# Set up logging to log errors to a file named 'error.log'

logging.basicConfig(filename='error.log', level=logging.ERROR)

from PyPDF2 import PdfReader

def generate_text_dump_from_pdf(pdf_path):

try:

reader = PdfReader(pdf_path)

text = ""

for page in reader.pages:

text += page.extract_text()

return text

except Exception as e:

logging.error(f"Error reading PDF file: {e}")

return ""




# Generate pivot report for noun-to-noun relatedness

def generate_noun_to_noun_pivot_report(pos_tagged_words, relatedness, filename='noun_to_noun_pivot_report.csv'):

noun_to_noun_data = defaultdict(Counter)

for (word1, pos1), (word2, pos2) in zip(pos_tagged_words, pos_tagged_words[1:]):

if pos1.startswith('NN') and pos2.startswith('NN'): # Check for noun-to-noun relatedness

noun_to_noun_data[word1][word2] += relatedness[word1].get(word2, 0)

# Prepare data for the CSV file

rows = []

for noun1, connections in noun_to_noun_data.items():

for noun2, weight in connections.items():

rows.append([noun1, noun2, weight])

# Save noun-to-noun pivot report as CSV

pd.DataFrame(rows, columns=['Noun1', 'Noun2', 'Weight']).to_csv(filename, index=False)

# Generate pivot report for noun-to-verb relatedness

def generate_noun_to_verb_pivot_report(pos_tagged_words, relatedness, filename='noun_to_verb_pivot_report.csv'):

noun_to_verb_data = defaultdict(Counter)

for (word1, pos1), (word2, pos2) in zip(pos_tagged_words, pos_tagged_words[1:]):

if pos1.startswith('NN') and pos2.startswith('VB'): # Check for noun-to-verb relatedness

noun_to_verb_data[word1][word2] += relatedness[word1].get(word2, 0)

# Prepare data for the CSV file

rows = []

for noun, verbs in noun_to_verb_data.items():

for verb, weight in verbs.items():

rows.append([noun, verb, weight])

# Save noun-to-verb pivot report as CSV

pd.DataFrame(rows, columns=['Noun', 'Verb', 'Weight']).to_csv(filename, index=False)

# Preprocess the text: tokenize, stem, lemmatize, and remove stopwords/punctuation

def preprocess_text_with_stemming(text):

if text is None:

return [], []

words = word_tokenize(text.lower())

lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word not in string.punctuation]

stemmed_words = [stemmer.stem(word) for word in words if word not in stop_words and word not in string.punctuation]

return lemmatized_words, stemmed_words

# Calculate stemming frequencies

def calculate_stem_frequencies(words):

return Counter(words)

# Helper function to convert POS tag to WordNet format

def get_wordnet_pos(treebank_tag):

if treebank_tag.startswith('J'):

return wordnet.ADJ

elif treebank_tag.startswith('V'):

return wordnet.VERB

elif treebank_tag.startswith('N'):

return wordnet.NOUN

elif treebank_tag.startswith('R'):

return wordnet.ADV

return wordnet.NOUN # Default to noun

# Preprocess the text: tokenize, lemmatize, and remove stopwords/punctuation

def preprocess_text(text):

if text is None:

return []

words = word_tokenize(text.lower())

return [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word not in string.punctuation]

# Calculate word frequencies

def calculate_word_frequencies(words):

return Counter(words)

# Calculate n-grams frequencies

def calculate_ngrams(words, n=3):

return Counter(ngrams(words, n))

# Calculate word relatedness (co-occurrence) within a sliding window

def calculate_word_relatedness(words, window_size=30):

relatedness = defaultdict(Counter)

for i, word1 in enumerate(words):

for word2 in words[i+1:i+window_size]:

if word1 != word2:

relatedness[word1][word2] += 1

return relatedness

# Generate and save a word cloud as an SVG

def visualize_wordcloud(word_freqs, output_file='wordcloud.svg', title='Word Cloud'):

wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freqs)

plt.figure(figsize=(10, 5))

plt.imshow(wordcloud, interpolation='bilinear')

plt.axis("off")

plt.title(title, fontsize=16)

plt.savefig(output_file, format="svg")

plt.close()

# Export word relatedness data to a CSV file

def export_graph_data_to_csv(relatedness, filename='word_relatedness.csv'):

rows = [[word1, word2, weight] for word1, connections in relatedness.items() for word2, weight in connections.items()]

pd.DataFrame(rows, columns=['Word1', 'Word2', 'Weight']).to_csv(filename, index=False)

# Export POS tagged frequencies to a CSV file

def export_pos_frequencies_to_csv(pos_tags, filename='pos_frequencies.csv'):

pos_freqs = Counter(pos_tags)

pd.DataFrame.from_dict(pos_freqs, orient='index', columns=['Frequency']).sort_values(by='Frequency', ascending=False).to_csv(filename)

# Visualize a word relatedness graph as an SVG

def visualize_word_graph(relatedness, word_freqs, output_file='word_graph.svg'):

G = nx.Graph()

for word1, connections in relatedness.items():

for word2, weight in connections.items():

if word1 != word2 and weight > 1:

G.add_edge(word1, word2, weight=weight)

pos = nx.circular_layout(G)

edge_weights = [G[u][v]['weight'] for u, v in G.edges()]

plt.figure(figsize=(12, 12))

nx.draw_networkx_nodes(G, pos, node_size=[word_freqs.get(node, 1) * 300 for node in G.nodes()], node_color='skyblue', alpha=0.7)

nx.draw_networkx_labels(G, pos, font_size=12, font_color='black', font_weight='bold')

nx.draw_networkx_edges(G, pos, width=[w * 0.2 for w in edge_weights], edge_color='gray')

nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): G[u][v]['weight'] for u, v in G.edges()}, font_size=10, font_color='red')

plt.title("Word Relatedness Graph", fontsize=16)

plt.tight_layout()

plt.savefig(output_file, format="svg")

plt.close()

# Export phrase and POS tag relationships to CSV

def export_phrase_pos_relationships_to_csv(phrases, pos_tags, filename='phrase_pos_relationships.csv'):

data = []

phrase_pos_pairs = list(zip(phrases, pos_tags))

phrase_pos_counter = Counter(phrase_pos_pairs)

for (phrase, pos_tag), frequency in phrase_pos_counter.items():

data.append([phrase, pos_tag, frequency])

pd.DataFrame(data, columns=['Phrase', 'POS Tag', 'Frequency']).to_csv(filename, index=False)

# Split text into manageable chunks for analysis

def chunk_text(text, chunk_size=10000):

words = text.split()

for i in range(0, len(words), chunk_size):

yield ' '.join(words[i:i + chunk_size])

# Convert SVG to PDF

def convert_svg_to_pdf(svg_file, pdf_file):

try:

drawing = svg2rlg(svg_file)

renderPDF.drawToFile(drawing, pdf_file)

except Exception as e:

logging.error(f"Failed to convert SVG to PDF: {e}")

# # # # Generate pivot report with word and phrase breakups by POS tags

# # # def generate_pivot_report(pos_tagged_words, pos_tagged_phrases)

# Generate pivot report with word and phrase breakups by POS tags

def generate_pivot_report(pos_tagged_words, pos_tagged_phrases, filename='pivot_report.csv'):

pivot_data = defaultdict(lambda: {'Words': [], 'Phrases': [], 'Word Count': 0, 'Phrase Count': 0})


# Separate words and phrases by their POS tags

for word, pos in pos_tagged_words:

pivot_data[pos]['Words'].append(word)

pivot_data[pos]['Word Count'] += 1

for phrase, pos in pos_tagged_phrases:

pivot_data[pos]['Phrases'].append(phrase)

pivot_data[pos]['Phrase Count'] += 1


# Prepare data for the CSV file

pivot_rows = []

for pos, data in pivot_data.items():

pivot_rows.append({

'POS Tag': pos,

'Words': ', '.join(data['Words'][:10]), # Limit to 10 words for readability

'Phrases': ', '.join(data['Phrases'][:10]), # Limit to 10 phrases for readability

'Word Count': data['Word Count'],

'Phrase Count': data['Phrase Count']

})


# Save pivot report as CSV

pd.DataFrame(pivot_rows).to_csv(filename, index=False)

# Analyze text and create visual outputs

def analyze_text(text, pdf_path=None):

if not text:

logging.warning("No text to analyze.")

print("No text to analyze.")

return


try:

all_words = []

all_pos_tags = []

pos_tagged_words = []


for chunk in chunk_text(text):

words, _ = preprocess_text_with_stemming(chunk)

pos_tags = pos_tag(words)

all_words.extend(words)

all_pos_tags.extend([tag for _, tag in pos_tags])

pos_tagged_words.extend(pos_tags)


relatedness = calculate_word_relatedness(all_words, window_size=60)

export_graph_data_to_csv(relatedness)

export_pos_frequencies_to_csv(all_pos_tags)


# Generate separate noun-to-noun and noun-to-verb reports

generate_noun_to_noun_pivot_report(pos_tagged_words, relatedness)

generate_noun_to_verb_pivot_report(pos_tagged_words, relatedness)


# Create visualizations and generate general pivot report

visualize_word_graph(relatedness, calculate_word_frequencies(all_words))

generate_pivot_report(pos_tagged_words, [])


if pdf_path:

convert_svg_to_pdf('wordcloud.svg', pdf_path)


except Exception as e:

logging.error(f"Error during text analysis: {e}")

print("An error occurred during text analysis.")

# Main program function to load and analyze a text file

def main():

root = Tk()

root.withdraw()


try:

# Prompt user to select a text file

file_path = filedialog.askopenfilename(title="Select Text File",

filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])

if not file_path:

print("No file selected. Exiting.")

return


if file_path.endswith('.pdf'):

text = generate_text_dump_from_pdf(file_path) # Now this will work

else:

text = read_text_from_file(file_path)


analyze_text(text)


except Exception as e:

logging.error(f"Error in main program: {e}")

print("An error occurred.")

if __name__ == "__main__":

main() ans also use the pretrained (pytorch tensorflow ,bert huggingface models whatever is necessary to run offline sentence wise parse tree generator for every sentence and generate a pdf file where one blank page is added for each sentence with proper parse tree also with the nltk file chooser and automated output files with keyword detectors , key phrase detectors counters and complexities to log on each such new pdf generatedimport tkinter as tk

from tkinter import filedialog, messagebox, ttk

import pdfplumber

from nltk.tokenize import sent_tokenize, word_tokenize

from nltk.corpus import stopwords

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

from collections import Counter

import os


# Function to extract text from a PDF

def extract_text_from_pdf(pdf_path):

with pdfplumber.open(pdf_path) as pdf:

text = ""

for page in pdf.pages:

text += page.extract_text() + "\n"

return text


# Function to extract text from a TXT file

def extract_text_from_txt(txt_path):

with open(txt_path, "r", encoding="utf-8") as file:

return file.read()


# Function to tokenize text into sentences

def tokenize_sentences(text):

return sent_tokenize(text)


# Function to simplify a single sentence using the pretrained model

def simplify_sentence(sentence, tokenizer, model):

inputs = tokenizer("simplify: " + sentence, return_tensors="pt", truncation=True)

outputs = model.generate(inputs["input_ids"], max_length=512, num_beams=4, early_stopping=True)

simplified = tokenizer.decode(outputs[0], skip_special_tokens=True)

return simplified.split(". ") # Split into parts


# Function to calculate a proxy comprehensibility score

def calculate_comprehensibility(sentence):

words = sentence.split()

unique_words = set(words)

return len(words) / len(unique_words) if unique_words else 0


# Function to extract keywords and key phrases

def extract_keywords_and_phrases(text):

stop_words = set(stopwords.words('english'))

words = word_tokenize(text)

keywords = [word for word in words if word.isalnum() and word.lower() not in stop_words]

key_phrases = [" ".join(phrase) for phrase in zip(words, words[1:]) if all(w.isalnum() for w in phrase)]

return Counter(keywords), Counter(key_phrases)


def process_file_with_progress(input_file, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label):

try:

# Extract text based on file type

text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)


# Tokenize text into sentences

sentences = tokenize_sentences(text)

total_sentences = len(sentences)


# Ensure there are sentences to process

if total_sentences == 0:

raise ValueError("No sentences found in the document.")


# Data structure to hold sentence complexities and simplified parts

sentence_data = []


# Process each sentence

with open(output_file1, "w", encoding="utf-8") as f1:

for i, sentence in enumerate(sentences, start=1):

# Update progress bar and label

progress_bar["value"] = (i / total_sentences) * 100

progress_label["text"] = f"Processing sentence {i}/{total_sentences}"

progress_bar.update()


# Calculate complexity

complexity = calculate_comprehensibility(sentence)


# Simplify sentence

simplified_parts = simplify_sentence(sentence, tokenizer, model)


# Write to first output file

f1.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")

for j, part in enumerate(simplified_parts, start=1):

complexity_part = calculate_comprehensibility(part)

f1.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")


# Collect data for sorting

sentence_data.append((sentence.strip(), complexity, simplified_parts))


# Sort by complexity

sorted_data = sorted(sentence_data, key=lambda x: x[1])


# Extract keywords and key phrases

all_text = " ".join([s[0] for s in sorted_data]) # Combine all sentences

keywords, key_phrases = extract_keywords_and_phrases(all_text)


# Write to second output file

with open(output_file2, "w", encoding="utf-8") as f2:

for sentence, complexity, simplified_parts in sorted_data:

f2.write(f"Sentence: {sentence} [Complexity: {complexity:.2f}]\n")

for part in simplified_parts:

f2.write(f" Simplified: {part.strip()}\n")

f2.write("\n--- Keywords and Their Complexity ---\n")

for keyword, count in keywords.most_common():

f2.write(f"{keyword}: {count}\n")

f2.write("\n--- Key Phrases and Their Complexity ---\n")

for phrase, count in key_phrases.most_common():

f2.write(f"{phrase}: {count}\n")


# Finalize progress

progress_label["text"] = "Processing complete!"

return True # Indicate success


except Exception as e:

progress_label["text"] = f"Error: {e}"

return False # Indicate failure


# Update select_file to only show success if the processing is complete

def select_file():

file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])

if not file_path:

return

file_type = "pdf" if file_path.endswith(".pdf") else "txt"

output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"

output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"

if not output_file1 or not output_file2:

return

try:

progress_label["text"] = "Initializing..."

progress_bar["value"] = 0

success = process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)

if success:

messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")

else:

messagebox.showerror("Error", "Processing failed. Check progress and files.")

except Exception as e:

progress_label["text"] = "Error occurred."

messagebox.showerror("Error", f"An error occurred: {e}")


# # # def process_file_with_progress(input_file, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label):

# # # try:

# # # Extract text based on file type

# # # text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)


# # # Tokenize text into sentences

# # # sentences = tokenize_sentences(text)

# # # total_sentences = len(sentences)


# # # Ensure there are sentences to process

# # # if total_sentences == 0:

# # # raise ValueError("No sentences found in the document.")


# # # Data structure to hold sentence complexities and simplified parts

# # # sentence_data = []


# # # Process each sentence

# # # with open(output_file1, "w", encoding="utf-8") as f1:

# # # for i, sentence in enumerate(sentences, start=1):

# # # Update progress bar and label

# # # progress_bar["value"] = (i / total_sentences) * 100

# # # progress_label["text"] = f"Processing sentence {i}/{total_sentences}"

# # # progress_bar.update()


# # # Calculate complexity

# # # complexity = calculate_comprehensibility(sentence)


# # # Simplify sentence

# # # simplified_parts = simplify_sentence(sentence, tokenizer, model)


# # # Write to first output file

# # # f1.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")

# # # for j, part in enumerate(simplified_parts, start=1):

# # # complexity_part = calculate_comprehensibility(part)

# # # f1.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")


# # # Collect data for sorting

# # # sentence_data.append((sentence.strip(), complexity, simplified_parts))


# # # Sort by complexity

# # # sorted_data = sorted(sentence_data, key=lambda x: x[1])


# # # Extract keywords and key phrases

# # # all_text = " ".join([s[0] for s in sorted_data]) # Combine all sentences

# # # keywords, key_phrases = extract_keywords_and_phrases(all_text)


# # # Write to second output file

# # # with open(output_file2, "w", encoding="utf-8") as f2:

# # # for sentence, complexity, simplified_parts in sorted_data:

# # # f2.write(f"Sentence: {sentence} [Complexity: {complexity:.2f}]\n")

# # # for part in simplified_parts:

# # # f2.write(f" Simplified: {part.strip()}\n")

# # # f2.write("\n--- Keywords and Their Complexity ---\n")

# # # for keyword, count in keywords.most_common():

# # # f2.write(f"{keyword}: {count}\n")

# # # f2.write("\n--- Key Phrases and Their Complexity ---\n")

# # # for phrase, count in key_phrases.most_common():

# # # f2.write(f"{phrase}: {count}\n")


# # # Finalize progress

# # # progress_label["text"] = "Processing complete!"

# # # return True # Indicate success


# # # except Exception as e:

# # # progress_label["text"] = f"Error: {e}"

# # # return False # Indicate failure


# Update select_file to only show success if the processing is complete

def select_file():

file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])

if not file_path:

return

file_type = "pdf" if file_path.endswith(".pdf") else "txt"

output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"

output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"

if not output_file1 or not output_file2:

return

try:

progress_label["text"] = "Initializing..."

progress_bar["value"] = 0

success = process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)

if success:

messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")

else:

messagebox.showerror("Error", "Processing failed. Check progress and files.")

except Exception as e:

progress_label["text"] = "Error occurred."

messagebox.showerror("Error", f"An error occurred: {e}")




# # # # Function to process the file and dump results

# # # def process_file_with_progress(input_file, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label):

# # # # Extract text based on file type

# # # text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)


# # # # Tokenize text into sentences

# # # sentences = tokenize_sentences(text)

# # # total_sentences = len(sentences)


# # # # Data structure to hold sentence complexities and simplified parts

# # # sentence_data = []


# # # # Process each sentence

# # # with open(output_file1, "w", encoding="utf-8") as f1:

# # # for i, sentence in enumerate(sentences, start=1):

# # # # Update progress bar and label

# # # progress_bar["value"] = (i / total_sentences) * 100

# # # progress_label["text"] = f"Processing sentence {i}/{total_sentences}"

# # # progress_bar.update()


# # # # Calculate complexity

# # # complexity = calculate_comprehensibility(sentence)


# # # # Simplify sentence

# # # simplified_parts = simplify_sentence(sentence, tokenizer, model)


# # # # Write to first output file

# # # f1.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")

# # # for j, part in enumerate(simplified_parts, start=1):

# # # complexity_part = calculate_comprehensibility(part)

# # # f1.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")


# # # # Collect data for sorting

# # # sentence_data.append((sentence.strip(), complexity, simplified_parts))


# # # # Sort by complexity

# # # sorted_data = sorted(sentence_data, key=lambda x: x[1])


# # # # Extract keywords and key phrases

# # # all_text = " ".join([s[0] for s in sorted_data]) # Combine all sentences

# # # keywords, key_phrases = extract_keywords_and_phrases(all_text)


# # # # Write to second output file

# # # with open(output_file2, "w", encoding="utf-8") as f2:

# # # for sentence, complexity, simplified_parts in sorted_data:

# # # f2.write(f"Sentence: {sentence} [Complexity: {complexity:.2f}]\n")

# # # for part in simplified_parts:

# # # f2.write(f" Simplified: {part.strip()}\n")

# # # f2.write("\n--- Keywords and Their Complexity ---\n")

# # # for keyword, count in keywords.most_common():

# # # f2.write(f"{keyword}: {count}\n")

# # # f2.write("\n--- Key Phrases and Their Complexity ---\n")

# # # for phrase, count in key_phrases.most_common():

# # # f2.write(f"{phrase}: {count}\n")


# Main function to handle Tkinter interface and processing

def main():

# Load pretrained models (offline)

model_name = "t5-small" # Change to "facebook/bart-large" if needed

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)


# Tkinter GUI

def select_file():

file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])

if not file_path:

return

file_type = "pdf" if file_path.endswith(".pdf") else "txt"

output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"

output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"

if not output_file1 or not output_file2:

return

try:

progress_label["text"] = "Initializing..."

progress_bar["value"] = 0

process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)

progress_label["text"] = "Processing complete!"

messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")

except Exception as e:

progress_label["text"] = "Error occurred."

messagebox.showerror("Error", f"An error occurred: {e}")


# Create Tkinter GUI

root = tk.Tk()

root.title("Semantic Analysis and Sentence Simplification")


tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)

tk.Button(root, text="Select File", command=select_file).pack(pady=20)


progress_label = tk.Label(root, text="")

progress_label.pack(pady=5)


progress_bar = ttk.Progressbar(root, orient="horizontal", length=300, mode="determinate")

progress_bar.pack(pady=5)


tk.Button(root, text="Exit", command=root.quit).pack(pady=10)


root.mainloop()


if __name__ == "__main__":

main()





D:\>pip install pymupdf transformers torch nltk pdfplumber matplotlib

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: pymupdf in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (1.24.10)

Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.44.2)

Requirement already satisfied: torch in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (2.4.1)

Requirement already satisfied: nltk in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (3.9.1)

Requirement already satisfied: pdfplumber in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.11.4)

Requirement already satisfied: matplotlib in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (3.9.2)

Requirement already satisfied: PyMuPDFb==1.24.10 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pymupdf) (1.24.10)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)

Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)

Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)

Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)

Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)

Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)

Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)

Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)

Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.19.1)

Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)

Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (4.12.2)

Requirement already satisfied: sympy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (1.13.2)

Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.3)

Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.1.4)

Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (2024.9.0)

Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (75.1.0)

Requirement already satisfied: click in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (8.1.7)

Requirement already satisfied: joblib in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from nltk) (1.4.2)

Requirement already satisfied: pdfminer.six==20231228 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (20231228)

Requirement already satisfied: Pillow>=9.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (10.4.0)

Requirement already satisfied: pypdfium2>=4.18.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfplumber) (4.30.0)

Requirement already satisfied: charset-normalizer>=2.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)

Requirement already satisfied: cryptography>=36.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)

Requirement already satisfied: contourpy>=1.0.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from matplotlib) (1.3.0)

Requirement already satisfied: cycler>=0.10 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from matplotlib) (0.12.1)

Requirement already satisfied: fonttools>=4.22.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from matplotlib) (4.53.1)

Requirement already satisfied: kiwisolver>=1.3.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from matplotlib) (1.4.7)

Requirement already satisfied: pyparsing>=2.3.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from matplotlib) (3.1.4)

Requirement already satisfied: python-dateutil>=2.7 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from matplotlib) (2.9.0.post0)

Requirement already satisfied: six>=1.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)

Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)

Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch) (2.1.5)

Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)

Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)

Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)

Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy->torch) (1.3.0)

Requirement already satisfied: cffi>=1.12 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)

Requirement already satisfied: pycparser in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)



To implement the text analysis and sentence-wise parse tree generation, as well as PDF generation with each sentence and its corresponding parse tree, you can use pymupdf (for handling PDFs) and the Hugging Face pre-trained models for natural language processing tasks, including parsing. Below is a step-by-step approach that builds upon your existing code and uses pymupdf, transformers (Hugging Face), and nltk.

Install the necessary packages: Ensure you have the following dependencies:

bash

pip install pymupdf transformers torch nltk pdfplumber matplotlib
Extract text from PDF and TXT: We already have a function for extracting text from PDFs using pdfplumber, so we will use it as it fits your needs for text extraction.

Sentence Tokenization: We'll use nltk.sent_tokenize() for sentence splitting.

Parsing Sentences: For sentence parsing, we will use a pre-trained Hugging Face model, such as BERT or a parser model like spaCy or the transformers pipeline for parsing.

Generate a PDF with Sentence Parse Trees: We will utilize pymupdf (also known as fitz) to create a PDF file where each page will contain a sentence and its corresponding parse tree.

Here’s how you can implement this:

Code Example:
python

import fitz  # PyMuPDF
import nltk
from nltk.tokenize import sent_tokenize
from nltk import pos_tag, word_tokenize
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import pipeline
import pdfplumber
import os
from tkinter import Tk, filedialog, messagebox

# Initialize the HuggingFace tokenizer and model for parsing (you can use other models as well)
model_name = "bert-base-uncased"  # Placeholder, choose an appropriate model for parsing
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Use Hugging Face's parsing pipeline
parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        text = ""
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Function to extract text from TXT
def extract_text_from_txt(txt_path):
    with open(txt_path, "r", encoding="utf-8") as file:
        return file.read()

# Function to create a PDF file with sentences and parse trees
def create_pdf_with_parse_trees(sentences, output_pdf_path):
    doc = fitz.open()  # Create a new PDF document

    for sentence in sentences:
        page = doc.new_page()  # Add a new page for each sentence
        page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text

        # Get parse tree for the sentence (use your NLP model for parsing)
        parse_tree = generate_parse_tree(sentence)
        page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree

    doc.save(output_pdf_path)  # Save the PDF

# Function to generate a parse tree for a sentence (using HuggingFace model)
def generate_parse_tree(sentence):
    result = parse_pipeline(sentence)  # Generate parse tree or structure
    return result[0]['generated_text']  # Placeholder, adapt based on your model's output

# Main function to run the application
def main():
    # Initialize Tkinter dialog
    root = Tk()
    root.withdraw()

    # Prompt user to select a file (either PDF or TXT)
    file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])

    if not file_path:
        messagebox.showerror("Error", "No file selected. Exiting.")
        return

    # Extract text from the file
    if file_path.endswith('.pdf'):
        text = extract_text_from_pdf(file_path)
    else:
        text = extract_text_from_txt(file_path)

    # Tokenize the text into sentences
    sentences = sent_tokenize(text)

    # Create a PDF with parse trees for each sentence
    output_pdf_path = "output_with_parse_trees.pdf"
    create_pdf_with_parse_trees(sentences, output_pdf_path)

    messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")

if __name__ == "__main__":
    main()
Explanation:
Text Extraction:

For PDF files, we use pdfplumber to extract the text.
For TXT files, we simply read the content directly.
Sentence Tokenization:

Using nltk.sent_tokenize() to split the text into sentences.
Parse Tree Generation:

Using a Hugging Face model and tokenizer to generate the parse tree for each sentence. This is just a basic placeholder, so you'll need a specific model that outputs detailed parse structures or tree representations.
Creating a PDF:

The create_pdf_with_parse_trees() function creates a new PDF, adding a page for each sentence. The sentence is placed on the page, followed by its corresponding parse tree.
GUI Interaction:

The program uses tkinter to prompt the user to select a file and then processes the file accordingly.
Notes:
Model Choice: In the code, we are using a generic BERT model (bert-base-uncased), which is not ideal for parsing. You can substitute it with a model specifically trained for parsing or use a dedicated parsing library like spaCy or another Hugging Face model designed for parsing tasks.
Sentence Parsing: The parse_pipeline is currently set up for text-to-text generation. You would need a proper model or method that provides parse tree structures in a usable format. Alternatively, use a specialized parser like spaCy for better results.
This approach will generate a PDF with each sentence and its parse tree on a separate page. You can customize the output format as needed.

D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS>py trying___pdfparsetreesgenerators.py

tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48.0/48.0 [00:00<?, ?B/s]

C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\huggingface_hub\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Sanjoy Nath\.cache\huggingface\hub\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.

To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development

  warnings.warn(message)

config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<?, ?B/s]

vocab.txt: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 375kB/s]

tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 1.18MB/s]

C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884

  warnings.warn(

Traceback (most recent call last):

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\trying___pdfparsetreesgenerators.py", line 14, in <module>

    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\models\auto\auto_factory.py", line 567, in from_pretrained

    raise ValueError(

ValueError: Unrecognized configuration class <class 'transformers.models.bert.configuration_bert.BertConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.

Model type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.





The error message indicates that the model you're trying to load (bert-base-uncased) is incompatible with the AutoModelForSeq2SeqLM class. Specifically, AutoModelForSeq2SeqLM is intended for sequence-to-sequence models, such as BART, T5, or Marian, whereas BERT (bert-base-uncased) is a masked language model that doesn't support sequence-to-sequence tasks out-of-the-box.

To fix this error, you can either:

Use a different model architecture that is suitable for sequence-to-sequence tasks, such as T5, BART, or MBart.
Use a different class for loading BERT if you intend to use it for classification or masked language modeling, such as AutoModelForMaskedLM for BERT.
Here's an updated version of the code for loading a compatible model like T5:

python

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

model_name = "t5-small"  # Or any other seq2seq model

# Load the model
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
If you need to stick with BERT and your goal is something like masked language modeling, try:

python

from transformers import AutoModelForMaskedLM, AutoTokenizer

model_name = "bert-base-uncased"  # Use BERT for masked language modeling

# Load the model
model = AutoModelForMaskedLM.from_pretrained(model_name)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
Ensure you're using the right model class for the task you're working on.





Certainly! Based on the error you're encountering, I will rewrite the entire code assuming you're trying to use a T5 model for sequence-to-sequence tasks. This will allow you to work with the right model and avoid the error you encountered.

Here’s how you can modify the code:

python
Copy code
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Define the model name (T5-small for demonstration)
model_name = "t5-small"  # Or any other seq2seq model like "t5-base", "bart-large", etc.

# Load the model for sequence-to-sequence tasks
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Load the tokenizer for the model
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example usage: Encoding input and generating output
input_text = "Translate English to French: How are you?"
inputs = tokenizer(input_text, return_tensors="pt")

# Generate output using the model
outputs = model.generate(inputs["input_ids"])

# Decode the generated output
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Generated Output:", decoded_output)
Explanation of the code:
Model and Tokenizer Loading:

We load the t5-small model using AutoModelForSeq2SeqLM for sequence-to-sequence tasks like translation, summarization, etc.
The AutoTokenizer is used to tokenize input text into model-compatible format.
Input Text:

We define input_text for a sequence-to-sequence task, such as translation.
Encoding and Decoding:

The tokenizer encodes the input text into token IDs.
The model.generate() method is used to generate the output sequence based on the input.
Output:

Finally, the generated token IDs are decoded back into human-readable text using tokenizer.decode().
If you prefer using BERT for masked language modeling (instead of sequence-to-sequence tasks), here's how you can rewrite it:
python

from transformers import AutoModelForMaskedLM, AutoTokenizer

# Define the BERT model name
model_name = "bert-base-uncased"  # BERT for masked language modeling

# Load the BERT model
model = AutoModelForMaskedLM.from_pretrained(model_name)

# Load the tokenizer for BERT
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example usage: Encoding input for masked language modeling
input_text = "The quick brown fox jumped over the lazy [MASK]."
inputs = tokenizer(input_text, return_tensors="pt")

# Predict the masked word
outputs = model(**inputs)

# Get the predicted token IDs
predicted_token_ids = outputs.logits.argmax(dim=-1)

# Decode the predicted token back to a word
predicted_word = tokenizer.decode(predicted_token_ids[0, inputs.input_ids[0] == tokenizer.mask_token_id][0])

print("Predicted Masked Word:", predicted_word)
Key points:
For BERT (masked language modeling), the text contains a [MASK] token, and the model predicts the word for that masked token.
The BERT model is loaded with AutoModelForMaskedLM instead of AutoModelForSeq2SeqLM.
Let me know which task you're aiming for, and I can adjust the code further if needed!





The warning you're seeing suggests that the model's default parameter max_length is being used, which limits the generation length of the output. The recommendation is to use the max_new_tokens parameter to control the maximum length of generated text more explicitly.

This warning can be resolved by setting max_new_tokens to a desired value (e.g., 50, 100, etc.) when using the pipeline. You can modify the parse_pipeline call to include this parameter as shown below:

Updated Code Snippet:
python

# Use Hugging Face's parsing pipeline for text generation with max_new_tokens
parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

# Function to generate a parse tree for a sentence using HuggingFace model
def generate_parse_tree(sentence):
    # Generate parse tree or sentence transformation using a text2text model
    result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
    return result[0]['generated_text']  # Adapt based on your model's output
Explanation:
max_new_tokens=100: This parameter limits the number of tokens generated by the model. You can adjust the value based on your needs (e.g., 50, 100, 200).
Warning: By default, the model might not specify the length, and it falls back on max_length. It's generally better to specify max_new_tokens to control how much output the model generates.
Full Context for the Relevant Part of the Code:
python

# Use Hugging Face's parsing pipeline for text generation with max_new_tokens
parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

# Function to generate a parse tree for a sentence using HuggingFace model
def generate_parse_tree(sentence):
    # Generate parse tree or sentence transformation using a text2text model
    result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
    return result[0]['generated_text']  # Adapt based on your model's output
This should stop the warning from appearing and allow you to better control the length of the generated parse trees.





D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS>py pypdfsgraphcsextractions_fitz_with_pytorches.py



D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS>py trying___pretrained_pdfparsetreesgenerators.py

C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\generation\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.

  warnings.warn(

Traceback (most recent call last):

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\trying___pretrained_pdfparsetreesgenerators.py", line 80, in <module>

    # Tokenize the text into sentences

    ^^^^^^

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\trying___pretrained_pdfparsetreesgenerators.py", line 75, in main

    if file_path.endswith('.pdf'):

    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\trying___pretrained_pdfparsetreesgenerators.py", line 46, in create_pdf_with_parse_trees



  File "D:\SAANS_OFFLINES_GPT_SENTENCES_ANALYSERS\trying___pretrained_pdfparsetreesgenerators.py", line 34, in generate_parse_tree

    # # # result = parse_pipeline(sentence)

             ^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\pipelines\text2text_generation.py", line 167, in __call__

    result = super().__call__(*args, **kwargs)

             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\pipelines\base.py", line 1257, in __call__

    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\pipelines\base.py", line 1264, in run_single

    model_outputs = self.forward(model_inputs, **forward_params)

                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\pipelines\base.py", line 1164, in forward

    model_outputs = self._forward(model_inputs, **forward_params)

                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\pipelines\text2text_generation.py", line 191, in _forward

    output_ids = self.model.generate(**model_inputs, **generate_kwargs)

                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context

    return func(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\generation\utils.py", line 2024, in generate

    result = self._sample(

             ^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\generation\utils.py", line 2982, in _sample

    outputs = self(**model_inputs, return_dict=True)

              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl

    return self._call_impl(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl

    return forward_call(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\models\t5\modeling_t5.py", line 1739, in forward

    decoder_outputs = self.decoder(

                      ^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl

    return self._call_impl(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl

    return forward_call(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\models\t5\modeling_t5.py", line 1106, in forward

    layer_outputs = layer_module(

                    ^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl

    return self._call_impl(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl

    return forward_call(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\models\t5\modeling_t5.py", line 716, in forward

    cross_attention_outputs = self.layer[1](

                              ^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl

    return self._call_impl(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl

    return forward_call(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\transformers\models\t5\modeling_t5.py", line 626, in forward

    normed_hidden_states = self.layer_norm(hidden_states)

                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl

    return self._call_impl(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl

    return forward_call(*args, **kwargs)

           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It looks like there is an error occurring during the execution of the trying___pretrained_pdfparsetreesgenerators.py script. From the stack trace, it seems to be related to the transformers library, specifically in the T5 model from Hugging Face. The error is propagating from within the model's forward pass, possibly due to an issue with input data or configuration.

Here are a few things you can check and try to resolve this issue:

1. Check the Input Format
Ensure that the input you are passing into the model is in the correct format. The error may arise if the data passed to the model is not tokenized properly or doesn't match the expected input format for the T5 model.

You can add some debug prints to verify the structure of your input data:

python

print(f"Input to the model: {sentence}")
2. Verify Model and Tokenizer Compatibility
Ensure that the model and tokenizer you are using are compatible with each other. Sometimes a mismatch between the model and tokenizer versions can lead to such errors. If you haven't already, try explicitly defining the model and tokenizer as follows:

python

from transformers import T5Tokenizer, T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("t5-small")  # Or your specific model
tokenizer = T5Tokenizer.from_pretrained("t5-small")
3. Check Memory Usage
The issue may also occur due to insufficient GPU/CPU memory. If you are running this on a system with limited resources, try using smaller models or moving the execution to a machine with more resources.

You can also try running it on CPU by using the following:

python

model.to('cpu')
4. Set max_length or max_new_tokens
Since the warning suggests using max_new_tokens, ensure that you're specifying the maximum number of tokens to be generated explicitly. This can help avoid unexpected issues related to default values.

Update your call to the generate method with the appropriate parameters:

python

result = parse_pipeline(sentence, max_new_tokens=100)  # Set to a suitable number
5. Check for Specific Error in the Stack Trace
If there is any part of the error traceback indicating a specific function or line of code that's causing the problem, focus on that to understand the root cause. In this case, the issue seems to originate from the forward pass of the T5 model.

6. Update Your Libraries
If you're running an older version of the transformers or torch library, it could cause issues with compatibility. Try updating the libraries to the latest versions:

bash

pip install --upgrade transformers torch
7. Use CPU for Debugging
If the issue is related to GPU resources, you can try switching to CPU mode to check if that resolves the problem, especially for debugging:

python

model.to("cpu")
Conclusion
Try the above steps, and if the error persists, you can share more specific information about the model and the exact input data you're passing. This will help narrow down the potential cause.

.



D:\>pip install --upgrade transformers torch

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.44.2)

Collecting transformers

  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)

Requirement already satisfied: torch in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (2.4.1)

Collecting torch

  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)

Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)

Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)

Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)

Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)

Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)

Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)

Collecting tokenizers<0.22,>=0.21 (from transformers)

  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)

Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)

Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)

Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (4.12.2)

Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.3)

Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (3.1.4)

Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (2024.9.0)

Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch) (75.1.0)

Collecting sympy==1.13.1 (from torch)

  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)

Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy==1.13.1->torch) (1.3.0)

Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)

Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch) (2.1.5)

Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.3.2)

Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)

Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)

Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)

Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 3.3 MB/s eta 0:00:00

Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.0/203.0 MB 3.2 MB/s eta 0:00:00

Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 3.3 MB/s eta 0:00:00

Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 3.3 MB/s eta 0:00:00

Installing collected packages: sympy, torch, tokenizers, transformers

  Attempting uninstall: sympy

    Found existing installation: sympy 1.13.2

    Uninstalling sympy-1.13.2:

      Successfully uninstalled sympy-1.13.2

  WARNING: Failed to remove contents in a temporary directory 'C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\~ympy'.

  You can safely remove it manually.

  Attempting uninstall: torch

    Found existing installation: torch 2.4.1

    Uninstalling torch-2.4.1:

      Successfully uninstalled torch-2.4.1

  Attempting uninstall: tokenizers

    Found existing installation: tokenizers 0.19.1

    Uninstalling tokenizers-0.19.1:

      Successfully uninstalled tokenizers-0.19.1

  Attempting uninstall: transformers

    Found existing installation: transformers 4.44.2

    Uninstalling transformers-4.44.2:

      Successfully uninstalled transformers-4.44.2

ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.

torchvision 0.19.1 requires torch==2.4.1, but you have torch 2.5.1 which is incompatible.

Successfully installed sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 transformers-4.47.1





The issue you're encountering is due to a conflict between the version of torch you are trying to install and the version required by torchvision. Specifically, torchvision version 0.19.1 requires torch version 2.4.1, but you're trying to install torch version 2.5.1, which is causing the conflict.

To resolve this, you can either:

Downgrade torch to version 2.4.1 to match the version required by torchvision:

bash

pip install torch==2.4.1
This should align the versions and avoid the conflict.

Upgrade torchvision to a version that supports torch 2.5.1:

bash

pip install torchvision --upgrade
This will ensure that torchvision is compatible with the latest version of torch (2.5.1).

Make sure to check your project requirements to ensure that the versions of torch and torchvision are compatible with the other packages you're using.







D:\>pip install torch==2.4.1

Defaulting to user installation because normal site-packages is not writeable

Collecting torch==2.4.1

  Using cached torch-2.4.1-cp312-cp312-win_amd64.whl.metadata (27 kB)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (3.16.1)

Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (4.12.2)

Requirement already satisfied: sympy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (1.13.1)

Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (3.3)

Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (3.1.4)

Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (2024.9.0)

Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.4.1) (75.1.0)

Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch==2.4.1) (2.1.5)

Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy->torch==2.4.1) (1.3.0)

Using cached torch-2.4.1-cp312-cp312-win_amd64.whl (199.4 MB)

Installing collected packages: torch

  Attempting uninstall: torch

    Found existing installation: torch 2.5.1

    Uninstalling torch-2.5.1:

      Successfully uninstalled torch-2.5.1

Successfully installed torch-2.4.1

D:\>pip install torchvision --upgrade

Defaulting to user installation because normal site-packages is not writeable

Requirement already satisfied: torchvision in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.19.1)

Collecting torchvision

  Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)

Requirement already satisfied: numpy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torchvision) (1.26.4)

Collecting torch==2.5.1 (from torchvision)

  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)

Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torchvision) (10.4.0)

Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (3.16.1)

Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (4.12.2)

Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (3.3)

Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (3.1.4)

Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (2024.9.0)

Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (75.1.0)

Requirement already satisfied: sympy==1.13.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch==2.5.1->torchvision) (1.13.1)

Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)

Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.5)

Downloading torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 3.3 MB/s eta 0:00:00

Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)

Installing collected packages: torch, torchvision

  Attempting uninstall: torch

    Found existing installation: torch 2.4.1

    Uninstalling torch-2.4.1:

      Successfully uninstalled torch-2.4.1

  Attempting uninstall: torchvision

    Found existing installation: torchvision 0.19.1

    Uninstalling torchvision-0.19.1:

      Successfully uninstalled torchvision-0.19.1

Successfully installed torch-2.5.1 torchvision-0.20.1




D:\>pip install stanfordnlp
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: stanfordnlp in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (0.2.0)
Requirement already satisfied: numpy in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from stanfordnlp) (1.26.4)
Requirement already satisfied: protobuf in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from stanfordnlp) (5.28.1)
Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from stanfordnlp) (2.32.3)
Requirement already satisfied: torch>=1.0.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from stanfordnlp) (2.5.1)
Requirement already satisfied: tqdm in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from stanfordnlp) (4.66.5)
Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (3.16.1)
Requirement already satisfied: typing-extensions>=4.8.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (4.12.2)
Requirement already satisfied: networkx in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (3.3)
Requirement already satisfied: jinja2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (3.1.4)
Requirement already satisfied: fsspec in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (2024.9.0)
Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (75.1.0)
Requirement already satisfied: sympy==1.13.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from torch>=1.0.0->stanfordnlp) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from sympy==1.13.1->torch>=1.0.0->stanfordnlp) (1.3.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->stanfordnlp) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->stanfordnlp) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->stanfordnlp) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->stanfordnlp) (2024.8.30)
Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm->stanfordnlp) (0.4.6)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.1.5)

D:\>pip install transformers
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (4.47.1)
Requirement already satisfied: filelock in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.25.0)
Requirement already satisfied: numpy>=1.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (1.26.4)
Requirement already satisfied: packaging>=20.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (24.1)
Requirement already satisfied: pyyaml>=5.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2024.9.11)
Requirement already satisfied: requests in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.21.0)
Requirement already satisfied: safetensors>=0.4.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from transformers) (4.66.5)
Requirement already satisfied: fsspec>=2023.5.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)
Requirement already satisfied: colorama in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from tqdm>=4.27->transformers) (0.4.6)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from requests->transformers) (2024.8.30)


D:\>pip install allennlp
Defaulting to user installation because normal site-packages is not writeable
Collecting allennlp
  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)
INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.
  Downloading allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)
  Downloading allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)
  Downloading allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)
  Downloading allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)
  Downloading allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)
  Downloading allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)
INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.
  Downloading allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Downloading allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)
  Downloading allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)
  Downloading allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)
  Downloading allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)
  Downloading allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)
  Downloading allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)
  Downloading allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)
Requirement already satisfied: torch>=1.2.0 in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from allennlp) (2.5.1)
Requirement already satisfied: overrides in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from allennlp) (7.7.0)
Requirement already satisfied: nltk in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (from allennlp) (3.9.1)
Collecting spacy<2.2,>=2.1.0 (from allennlp)
  Downloading spacy-2.1.9.tar.gz (30.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30.7/30.7 MB 3.2 MB/s eta 0:00:00
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [51 lines of output]
      Collecting setuptools
        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)
      Collecting wheel<0.33.0,>0.32.0
        Downloading wheel-0.32.3-py2.py3-none-any.whl.metadata (2.1 kB)
      Collecting Cython
        Downloading Cython-3.0.11-cp312-cp312-win_amd64.whl.metadata (3.2 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Downloading cymem-2.0.10-cp312-cp312-win_amd64.whl.metadata (8.6 kB)
      Collecting preshed<2.1.0,>=2.0.1
        Downloading preshed-2.0.1.tar.gz (113 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'error'
        error: subprocess-exited-with-error

        Getting requirements to build wheel did not run successfully.
        exit code: 1

        [20 lines of output]
        Traceback (most recent call last):
          File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 353, in <module>
            main()
          File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 335, in main
            json_out['return_val'] = hook(**hook_input['kwargs'])
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          File "C:\Users\Sanjoy Nath\AppData\Roaming\Python\Python312\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 118, in get_requires_for_build_wheel
            return hook(config_settings)
                   ^^^^^^^^^^^^^^^^^^^^^
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-txua6yff\overlay\Lib\site-packages\setuptools\build_meta.py", line 334, in get_requires_for_build_wheel
            return self._get_build_requires(config_settings, requirements=[])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-txua6yff\overlay\Lib\site-packages\setuptools\build_meta.py", line 304, in _get_build_requires
            self.run_setup()
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-txua6yff\overlay\Lib\site-packages\setuptools\build_meta.py", line 522, in run_setup
            super().run_setup(setup_script=setup_script)
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-txua6yff\overlay\Lib\site-packages\setuptools\build_meta.py", line 320, in run_setup
            exec(code, locals())
          File "<string>", line 9, in <module>
        ImportError: cannot import name 'msvccompiler' from 'distutils' (C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-txua6yff\overlay\Lib\site-packages\setuptools\_distutils\__init__.py). Did you mean: 'ccompiler'?
        [end of output]

        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error

      Getting requirements to build wheel did not run successfully.
      exit code: 1

      See above for output.

      note: This error originates from a subprocess, and is likely not a problem with pip.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

D:\>

It seems like there was an error while trying to install AllenNLP due to issues related to building dependencies, particularly with the msvccompiler from distutils. This error is often encountered on Windows when certain C++ build tools are not properly installed or configured.

Here are some steps you can try to resolve the issue:

1. Install Visual C++ Build Tools
This error can often be fixed by installing the Visual C++ Build Tools, which are required for compiling C++ extensions. You can download them from here.

Once installed, try the pip install command again.

2. Ensure Python Development Tools are Installed
You may also need the necessary development tools, including distutils, setuptools, and wheel. You can ensure they are up to date with:

bash

pip install --upgrade setuptools wheel
3. Install Specific Version of SpaCy
AllenNLP has a dependency on SpaCy version 2.1.x. Since you're seeing errors while installing SpaCy, try installing a compatible version manually:

bash

pip install spacy==2.1.9
4. Check for Python 3.12 Compatibility
It's possible that some dependencies are not fully compatible with Python 3.12 yet. Consider using a different Python version (e.g., Python 3.8 or 3.9). You can create a new virtual environment with an older Python version for compatibility:

bash

python -m venv myenv
myenv\Scripts\activate
pip install allennlp
5. Use Pre-built Wheels (If Available)
If you continue facing build issues, you can try to install pre-built wheels for dependencies like SpaCy, which might bypass the need for compilation:

Download .whl files from https://www.lfd.uci.edu/~gohlke/pythonlibs/ and install using pip:
bash

pip install <path_to_wheel_file>
6. Check Log Output and Search for Solutions
Review the error messages carefully for additional clues, particularly when they refer to specific Python or system configurations. Sometimes, specific configurations or missing packages are identified.


D:\>pip install --upgrade setuptools wheel
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: setuptools in c:\users\sanjoy nath\appdata\roaming\python\python312\site-packages (75.1.0)
Collecting setuptools
  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)
Collecting wheel
  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)
Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)
Downloading wheel-0.45.1-py3-none-any.whl (72 kB)
Installing collected packages: wheel, setuptools
  Attempting uninstall: setuptools
    Found existing installation: setuptools 75.1.0
    Uninstalling setuptools-75.1.0:
      Successfully uninstalled setuptools-75.1.0
Successfully installed setuptools-75.6.0 wheel-0.45.1



D:\>python -m venv myenv

D:\>myenv\Scripts\activate

(myenv) D:\>pip install allennlp
Collecting allennlp
  Using cached allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)
INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.
  Using cached allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)
  Using cached allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)
  Using cached allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)
  Using cached allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)
  Using cached allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)
  Using cached allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)
INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.
  Using cached allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)
  Using cached allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)
  Using cached allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)
  Using cached allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)
  Using cached allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)
  Using cached allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)
  Using cached allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)
Collecting torch>=1.2.0 (from allennlp)
  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)
Collecting overrides (from allennlp)
  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)
Collecting nltk (from allennlp)
  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting spacy<2.2,>=2.1.0 (from allennlp)
  Using cached spacy-2.1.9.tar.gz (30.7 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [51 lines of output]
      Collecting setuptools
        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)
      Collecting wheel<0.33.0,>0.32.0
        Using cached wheel-0.32.3-py2.py3-none-any.whl.metadata (2.1 kB)
      Collecting Cython
        Using cached Cython-3.0.11-cp312-cp312-win_amd64.whl.metadata (3.2 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.10-cp312-cp312-win_amd64.whl.metadata (8.6 kB)
      Collecting preshed<2.1.0,>=2.0.1
        Using cached preshed-2.0.1.tar.gz (113 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'error'
        error: subprocess-exited-with-error

        Getting requirements to build wheel did not run successfully.
        exit code: 1

        [20 lines of output]
        Traceback (most recent call last):
          File "D:\myenv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 353, in <module>
            main()
          File "D:\myenv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 335, in main
            json_out['return_val'] = hook(**hook_input['kwargs'])
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          File "D:\myenv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 118, in get_requires_for_build_wheel
            return hook(config_settings)
                   ^^^^^^^^^^^^^^^^^^^^^
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 334, in get_requires_for_build_wheel
            return self._get_build_requires(config_settings, requirements=[])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 304, in _get_build_requires
            self.run_setup()
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 522, in run_setup
            super().run_setup(setup_script=setup_script)
          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 320, in run_setup
            exec(code, locals())
          File "<string>", line 9, in <module>
        ImportError: cannot import name 'msvccompiler' from 'distutils' (C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\_distutils\__init__.py). Did you mean: 'ccompiler'?
        [end of output]

        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error

      Getting requirements to build wheel did not run successfully.
      exit code: 1

      See above for output.

      note: This error originates from a subprocess, and is likely not a problem with pip.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.

[notice] A new release of pip is available: 24.2 -> 24.3.1
[notice] To update, run: python.exe -m pip install --upgrade pip
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.


It looks like you are looking for a solution that includes a progress bar and feedback for the user while processing files. I'll explain how the progress bar functionality works in your provided code and ensure that everything is correctly implemented.

Key Parts of the Code
Progress Bar Implementation: The progress bar is updated during the processing of sentences, which is managed by the process_file_with_progress() function. The value of the progress bar is updated based on how many sentences have been processed in comparison to the total number of sentences.

Progress Label: The progress_label is updated with the current progress, such as which sentence is being processed and the percentage of completion.

Updated process_file_with_progress() and select_file() Implementation
I’ll make sure that the progress bar and the labels are updated as expected during the file processing. Below is the corrected and enhanced code:

python

def process_file_with_progress(input_file, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label):
    try:
        # Extract text based on file type
        text = extract_text_from_pdf(input_file) if file_type == "pdf" else extract_text_from_txt(input_file)
        
        # Tokenize text into sentences
        sentences = tokenize_sentences(text)
        total_sentences = len(sentences)

        # Ensure there are sentences to process
        if total_sentences == 0:
            raise ValueError("No sentences found in the document.")

        # Data structure to hold sentence complexities and simplified parts
        sentence_data = []

        # Process each sentence
        with open(output_file1, "w", encoding="utf-8") as f1:
            for i, sentence in enumerate(sentences, start=1):
                # Update progress bar and label
                progress_bar["value"] = (i / total_sentences) * 100
                progress_label["text"] = f"Processing sentence {i}/{total_sentences}"
                progress_bar.update()

                # Calculate complexity
                complexity = calculate_comprehensibility(sentence)

                # Simplify sentence
                simplified_parts = simplify_sentence(sentence, tokenizer, model)

                # Write to first output file
                f1.write(f"{i}.0 {sentence.strip()} [Complexity: {complexity:.2f}]\n")
                for j, part in enumerate(simplified_parts, start=1):
                    complexity_part = calculate_comprehensibility(part)
                    f1.write(f"{i}.{j} {part.strip()} [Complexity: {complexity_part:.2f}]\n")
                
                # Collect data for sorting
                sentence_data.append((sentence.strip(), complexity, simplified_parts))

        # Sort by complexity
        sorted_data = sorted(sentence_data, key=lambda x: x[1])

        # Extract keywords and key phrases
        all_text = " ".join([s[0] for s in sorted_data])  # Combine all sentences
        keywords, key_phrases = extract_keywords_and_phrases(all_text)

        # Write to second output file
        with open(output_file2, "w", encoding="utf-8") as f2:
            for sentence, complexity, simplified_parts in sorted_data:
                f2.write(f"Sentence: {sentence} [Complexity: {complexity:.2f}]\n")
                for part in simplified_parts:
                    f2.write(f"  Simplified: {part.strip()}\n")
            f2.write("\n--- Keywords and Their Complexity ---\n")
            for keyword, count in keywords.most_common():
                f2.write(f"{keyword}: {count}\n")
            f2.write("\n--- Key Phrases and Their Complexity ---\n")
            for phrase, count in key_phrases.most_common():
                f2.write(f"{phrase}: {count}\n")

        # Finalize progress
        progress_label["text"] = "Processing complete!"
        return True  # Indicate success

    except Exception as e:
        progress_label["text"] = f"Error: {e}"
        return False  # Indicate failure

def select_file():
    file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])
    if not file_path:
        return
    file_type = "pdf" if file_path.endswith(".pdf") else "txt"
    output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"
    output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"
    if not output_file1 or not output_file2:
        return
    try:
        progress_label["text"] = "Initializing..."
        progress_bar["value"] = 0
        success = process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)
        if success:
            messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")
        else:
            messagebox.showerror("Error", "Processing failed. Check progress and files.")
    except Exception as e:
        progress_label["text"] = "Error occurred."
        messagebox.showerror("Error", f"An error occurred: {e}")
Key Improvements:
Progress Bar: The progress bar (progress_bar) is updated dynamically as sentences are processed.
Progress Label: The label (progress_label) shows which sentence is being processed and the current progress percentage.
File Processing Logic: The logic for reading files, tokenizing sentences, simplifying them, calculating complexity, and writing to output files remains intact.
Error Handling: In case of errors, a message box shows the error, and the progress bar and label display appropriate messages.
What to Check:
Ensure that the tokenizer and model are correctly initialized and passed into the process_file_with_progress() function.
The progress_bar should be a ttk.Progressbar widget and the progress_label should be a Label widget in your GUI.
With these adjustments, you should have a working solution with the progress bar and labels updating as the file is processed.

It looks like you've correctly included the progress bar from tqdm for tracking the sentence processing, but it seems like it wasn't being updated in your create_pdf_with_parse_trees function. In your code, the progress_bar.update(1) line is inside the loop that processes each sentence, but since progress_bar is declared globally, there might be issues with scope, initialization, or timing of the updates.

To resolve this, you should ensure that:

The progress bar is properly initialized before starting the process.
It's updated in a thread-safe manner.
It's closed properly when done.
Here is the corrected version of your code with a few refinements:

python

import fitz  # PyMuPDF
import nltk
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import pipeline
import pdfplumber
import os
from tkinter import Tk, filedialog, messagebox
import torch
from tqdm import tqdm  # For the progress bar
import logging

# Set up logging for better error tracking
logging.basicConfig(filename="process_log.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")

# Initialize the HuggingFace tokenizer and model for text generation (T5 for sequence-to-sequence tasks)
model_name = "t5-small"  # Change this to any seq2seq model like "t5-base", "t5-large"
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    logging.info("Model and tokenizer loaded successfully.")
except Exception as e:
    logging.error(f"Error loading model and tokenizer: {e}")
    raise

# Ensure model is on the appropriate device (CPU or CUDA for GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Use Hugging Face's parsing pipeline for text generation
parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=device.index if device.type == 'cuda' else -1)

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            for page in pdf.pages:
                text += page.extract_text() + "\n"
        logging.info(f"Text extracted from PDF: {pdf_path}")
        return text
    except Exception as e:
        logging.error(f"Error extracting text from PDF {pdf_path}: {e}")
        raise

# Function to extract text from TXT file
def extract_text_from_txt(txt_path):
    try:
        with open(txt_path, "r", encoding="utf-8") as file:
            text = file.read()
        logging.info(f"Text extracted from TXT: {txt_path}")
        return text
    except Exception as e:
        logging.error(f"Error extracting text from TXT {txt_path}: {e}")
        raise

# Function to generate a parse tree for a sentence using HuggingFace model
def generate_parse_tree(sentence):
    try:
        result = parse_pipeline(sentence, max_new_tokens=100)  # Set a limit for generated tokens
        return result[0]['generated_text']
    except Exception as e:
        logging.error(f"Error generating parse tree for sentence: {sentence} - {e}")
        raise

# Function to create a PDF with sentences and parse trees
def create_pdf_with_parse_trees(sentences, output_pdf_path):
    try:
        doc = fitz.open()  # Create a new PDF document

        for idx, sentence in enumerate(sentences):
            page = doc.new_page()  # Add a new page for each sentence
            page.insert_text((72, 72), sentence, fontsize=12)  # Insert the sentence text

            # Get parse tree for the sentence (use your NLP model for parsing)
            parse_tree = generate_parse_tree(sentence)
            page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10)  # Insert the parse tree

            # Update progress bar
            progress_bar.update(1)  # Update progress bar here

        doc.save(output_pdf_path)  # Save the PDF
        logging.info(f"PDF created successfully at: {output_pdf_path}")
    except Exception as e:
        logging.error(f"Error creating PDF with parse trees: {e}")
        raise

# Main function to run the application
def main():
    try:
        # Initialize Tkinter dialog
        root = Tk()
        root.withdraw()

        # Prompt user to select a file (either PDF or TXT)
        file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])

        if not file_path:
            messagebox.showerror("Error", "No file selected. Exiting.")
            logging.warning("No file selected by the user.")
            return

        # Extract text from the file
        if file_path.endswith('.pdf'):
            text = extract_text_from_pdf(file_path)
        else:
            text = extract_text_from_txt(file_path)

        # Tokenize the text into sentences
        sentences = sent_tokenize(text)
        total_sentences = len(sentences)
        
        # Set up the progress bar (correctly placed before processing begins)
        global progress_bar
        progress_bar = tqdm(total=total_sentences, desc="Processing Sentences", unit="sentence")

        # Create a PDF with parse trees for each sentence
        output_pdf_path = "output_with_parse_trees.pdf"
        create_pdf_with_parse_trees(sentences, output_pdf_path)

        progress_bar.close()  # Close the progress bar once done
        messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")
        logging.info(f"PDF generated successfully: {output_pdf_path}")

    except Exception as e:
        logging.error(f"An error occurred in the main function: {e}")
        messagebox.showerror("Error", f"An error occurred: {e}")

if __name__ == "__main__":
    main()
Changes made:
Progress bar initialization and updates:

The progress_bar is initialized correctly at the start of the main function before the create_pdf_with_parse_trees function is called.
The progress bar is updated inside the loop that processes each sentence in create_pdf_with_parse_trees.
Global variable:

The progress_bar is declared as a global variable so that it can be accessed and updated inside the function create_pdf_with_parse_trees.
With this update, the progress bar should now be visible during processing, updating as each sentence is processed.

i need the system to auto calculate necessary sequence length and generate the svg images parse trees for each page (fit to page) rewrite this import fitz # PyMuPDF

import nltk

from nltk.tokenize import sent_tokenize

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

from transformers import pipeline

import pdfplumber

import os

from tkinter import Tk, filedialog, messagebox

import torch

from tqdm import tqdm # For the progress bar

import logging


# Set up logging for better error tracking

logging.basicConfig(filename="process_log.log", level=logging.INFO,

format="%(asctime)s - %(levelname)s - %(message)s")


# Initialize the HuggingFace tokenizer and model for text generation (T5 for sequence-to-sequence tasks)

model_name = "t5-small" # Change this to any seq2seq model like "t5-base", "t5-large"

try:

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

logging.info("Model and tokenizer loaded successfully.")

except Exception as e:

logging.error(f"Error loading model and tokenizer: {e}")

raise


# Ensure model is on the appropriate device (CPU or CUDA for GPU)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)


# Use Hugging Face's parsing pipeline for text generation

parse_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, device=device.index if device.type == 'cuda' else -1)


# Function to extract text from PDF

def extract_text_from_pdf(pdf_path):

try:

with pdfplumber.open(pdf_path) as pdf:

text = ""

for page in pdf.pages:

text += page.extract_text() + "\n"

logging.info(f"Text extracted from PDF: {pdf_path}")

return text

except Exception as e:

logging.error(f"Error extracting text from PDF {pdf_path}: {e}")

raise


# Function to extract text from TXT file

def extract_text_from_txt(txt_path):

try:

with open(txt_path, "r", encoding="utf-8") as file:

text = file.read()

logging.info(f"Text extracted from TXT: {txt_path}")

return text

except Exception as e:

logging.error(f"Error extracting text from TXT {txt_path}: {e}")

raise


# Function to generate a parse tree for a sentence using HuggingFace model

def generate_parse_tree(sentence):

try:

result = parse_pipeline(sentence, max_new_tokens=100) # Set a limit for generated tokens

return result[0]['generated_text']

except Exception as e:

logging.error(f"Error generating parse tree for sentence: {sentence} - {e}")

raise


# Function to create a PDF with sentences and parse trees

def create_pdf_with_parse_trees(sentences, output_pdf_path):

try:

doc = fitz.open() # Create a new PDF document


for idx, sentence in enumerate(sentences):

page = doc.new_page() # Add a new page for each sentence

page.insert_text((72, 72), sentence, fontsize=12) # Insert the sentence text


# Get parse tree for the sentence (use your NLP model for parsing)

parse_tree = generate_parse_tree(sentence)

page.insert_text((72, 150), f"Parse Tree:\n{parse_tree}", fontsize=10) # Insert the parse tree


# Update progress bar

progress_bar.update(1) # Update progress bar here


doc.save(output_pdf_path) # Save the PDF

logging.info(f"PDF created successfully at: {output_pdf_path}")

except Exception as e:

logging.error(f"Error creating PDF with parse trees: {e}")

raise


# Main function to run the application

def main():

try:

# Initialize Tkinter dialog

root = Tk()

root.withdraw()


# Prompt user to select a file (either PDF or TXT)

file_path = filedialog.askopenfilename(title="Select a file", filetypes=[("Text Files", "*.txt"), ("PDF Files", "*.pdf")])


if not file_path:

messagebox.showerror("Error", "No file selected. Exiting.")

logging.warning("No file selected by the user.")

return


# Extract text from the file

if file_path.endswith('.pdf'):

text = extract_text_from_pdf(file_path)

else:

text = extract_text_from_txt(file_path)


# Tokenize the text into sentences

sentences = sent_tokenize(text)

total_sentences = len(sentences)


# Set up the progress bar (correctly placed before processing begins)

global progress_bar

progress_bar = tqdm(total=total_sentences, desc="Processing Sentences", unit="sentence")


# Create a PDF with parse trees for each sentence

output_pdf_path = "output_with_parse_trees.pdf"

create_pdf_with_parse_trees(sentences, output_pdf_path)


progress_bar.close() # Close the progress bar once done

messagebox.showinfo("Success", f"PDF generated: {output_pdf_path}")

logging.info(f"PDF generated successfully: {output_pdf_path}")


except Exception as e:

logging.error(f"An error occurred in the main function: {e}")

messagebox.showerror("Error", f"An error occurred: {e}")


if __name__ == "__main__":

main()...... ue the tkinter progressbars like this styles

# Tkinter GUI

def select_file():

file_path = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf"), ("Text Files", "*.txt")])

if not file_path:

return

file_type = "pdf" if file_path.endswith(".pdf") else "txt"

output_file1 = file_path + "_SIMPLIFIED_SENTENCES.TXT"

output_file2 = file_path + "_SORTED_COMPLEXITY.TXT"

if not output_file1 or not output_file2:

return

try:

progress_label["text"] = "Initializing..."

progress_bar["value"] = 0

process_file_with_progress(file_path, file_type, output_file1, output_file2, tokenizer, model, progress_bar, progress_label)

progress_label["text"] = "Processing complete!"

messagebox.showinfo("Success", f"Processing complete!\nOutputs:\n1. {output_file1}\n2. {output_file2}")

except Exception as e:

progress_label["text"] = "Error occurred."

messagebox.showerror("Error", f"An error occurred: {e}")


# Create Tkinter GUI

root = tk.Tk()

root.title("Semantic Analysis and Sentence Simplification")


tk.Label(root, text="Select a PDF or TXT file to process").pack(pady=10)

tk.Button(root, text="Select File", command=select_file).pack(pady=20)


progress_label = tk.Label(root, text="")

progress_label.pack(pady=5)


progress_bar = ttk.Progressbar(root, orient="horizontal", length=300, mode="determinate")

progress_bar.pack(pady=5)


tk.Button(root, text="Exit", command=root.quit).pack(pady=10)


root.mainloop()



D:\>python -m venv myenv



D:\>myenv\Scripts\activate



(myenv) D:\>pip install allennlp

Collecting allennlp

  Using cached allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)

INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.

  Using cached allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)

  Using cached allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)

  Using cached allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)

  Using cached allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)

  Using cached allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)

  Using cached allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)

INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.

  Using cached allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)

INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.

  Using cached allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)

  Using cached allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)

  Using cached allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)

  Using cached allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)

  Using cached allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)

  Using cached allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)

  Using cached allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)

Collecting torch>=1.2.0 (from allennlp)

  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)

Collecting overrides (from allennlp)

  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)

Collecting nltk (from allennlp)

  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)

Collecting spacy<2.2,>=2.1.0 (from allennlp)

  Using cached spacy-2.1.9.tar.gz (30.7 MB)

  Installing build dependencies ... error

  error: subprocess-exited-with-error



  × pip subprocess to install build dependencies did not run successfully.

  │ exit code: 1

  ╰─> [51 lines of output]

      Collecting setuptools

        Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)

      Collecting wheel<0.33.0,>0.32.0

        Using cached wheel-0.32.3-py2.py3-none-any.whl.metadata (2.1 kB)

      Collecting Cython

        Using cached Cython-3.0.11-cp312-cp312-win_amd64.whl.metadata (3.2 kB)

      Collecting cymem<2.1.0,>=2.0.2

        Using cached cymem-2.0.10-cp312-cp312-win_amd64.whl.metadata (8.6 kB)

      Collecting preshed<2.1.0,>=2.0.1

        Using cached preshed-2.0.1.tar.gz (113 kB)

        Installing build dependencies: started

        Installing build dependencies: finished with status 'done'

        Getting requirements to build wheel: started

        Getting requirements to build wheel: finished with status 'error'

        error: subprocess-exited-with-error



        Getting requirements to build wheel did not run successfully.

        exit code: 1



        [20 lines of output]

        Traceback (most recent call last):

          File "D:\myenv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 353, in <module>

            main()

          File "D:\myenv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 335, in main

            json_out['return_val'] = hook(**hook_input['kwargs'])

                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          File "D:\myenv\Lib\site-packages\pip\_vendor\pyproject_hooks\_in_process\_in_process.py", line 118, in get_requires_for_build_wheel

            return hook(config_settings)

                   ^^^^^^^^^^^^^^^^^^^^^

          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 334, in get_requires_for_build_wheel

            return self._get_build_requires(config_settings, requirements=[])

                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 304, in _get_build_requires

            self.run_setup()

          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 522, in run_setup

            super().run_setup(setup_script=setup_script)

          File "C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\build_meta.py", line 320, in run_setup

            exec(code, locals())

          File "<string>", line 9, in <module>

        ImportError: cannot import name 'msvccompiler' from 'distutils' (C:\Users\Sanjoy Nath\AppData\Local\Temp\pip-build-env-n8im6pww\overlay\Lib\site-packages\setuptools\_distutils\__init__.py). Did you mean: 'ccompiler'?

        [end of output]



        note: This error originates from a subprocess, and is likely not a problem with pip.

      error: subprocess-exited-with-error



      Getting requirements to build wheel did not run successfully.

      exit code: 1



      See above for output.



      note: This error originates from a subprocess, and is likely not a problem with pip.

      [end of output]



  note: This error originates from a subprocess, and is likely not a problem with pip.



[notice] A new release of pip is available: 24.2 -> 24.3.1

[notice] To update, run: python.exe -m pip install --upgrade pip

error: subprocess-exited-with-error



× pip subprocess to install build dependencies did not run successfully.

│ exit code: 1

╰─> See above for output.



note: This error originates from a subprocess, and is likely not a problem with pip.



(myenv) D:\>pip install graphviz nltk transformers pdfplumber fitz tqdm

Collecting graphviz

  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)

Collecting nltk

  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)

Collecting transformers

  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)

Collecting pdfplumber

  Using cached pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)

Collecting fitz

  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)

Collecting tqdm

  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)

Collecting click (from nltk)

  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)

Collecting joblib (from nltk)

  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)

Collecting regex>=2021.8.3 (from nltk)

  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)

Collecting filelock (from transformers)

  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)

Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)

  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)

Collecting numpy>=1.17 (from transformers)

  Downloading numpy-2.2.0-cp312-cp312-win_amd64.whl.metadata (60 kB)

Collecting packaging>=20.0 (from transformers)

  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)

Collecting pyyaml>=5.1 (from transformers)

  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)

Collecting requests (from transformers)

  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)

Collecting tokenizers<0.22,>=0.21 (from transformers)

  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)

Collecting safetensors>=0.4.1 (from transformers)

  Using cached safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)

Collecting pdfminer.six==20231228 (from pdfplumber)

  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)

Collecting Pillow>=9.1 (from pdfplumber)

  Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)

Collecting pypdfium2>=4.18.0 (from pdfplumber)

  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)

Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20231228->pdfplumber)

  Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl.metadata (34 kB)

Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)

  Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl.metadata (5.7 kB)

Collecting configobj (from fitz)

  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)

Collecting configparser (from fitz)

  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)

Collecting httplib2 (from fitz)

  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)

Collecting nibabel (from fitz)

  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)

Collecting nipype (from fitz)

  Downloading nipype-1.9.2-py3-none-any.whl.metadata (6.8 kB)

Collecting pandas (from fitz)

  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)

Collecting pyxnat (from fitz)

  Downloading pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)

Collecting scipy (from fitz)

  Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)

Collecting colorama (from tqdm)

  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)

Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)

  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)

Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.24.0->transformers)

  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)

Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2->fitz)

  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)

Collecting networkx>=2.5 (from nipype->fitz)

  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)

Collecting prov>=1.5.2 (from nipype->fitz)

  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)

Collecting pydot>=1.2.3 (from nipype->fitz)

  Downloading pydot-3.0.3-py3-none-any.whl.metadata (10 kB)

Collecting python-dateutil>=2.2 (from nipype->fitz)

  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)

Collecting rdflib>=5.0.0 (from nipype->fitz)

  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)

Collecting simplejson>=3.8.0 (from nipype->fitz)

  Downloading simplejson-3.19.3-cp312-cp312-win_amd64.whl.metadata (3.2 kB)

Collecting traits>=6.2 (from nipype->fitz)

  Downloading traits-6.4.3-cp312-cp312-win_amd64.whl.metadata (5.2 kB)

Collecting acres (from nipype->fitz)

  Downloading acres-0.2.0-py3-none-any.whl.metadata (5.5 kB)

Collecting etelemetry>=0.3.1 (from nipype->fitz)

  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)

Collecting looseversion!=1.2 (from nipype->fitz)

  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)

Collecting puremagic (from nipype->fitz)

  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)

Collecting pytz>=2020.1 (from pandas->fitz)

  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)

Collecting tzdata>=2022.7 (from pandas->fitz)

  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)

Collecting lxml>=4.3 (from pyxnat->fitz)

  Using cached lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)

Collecting pathlib>=1.0 (from pyxnat->fitz)

  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)

Collecting idna<4,>=2.5 (from requests->transformers)

  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)

Collecting urllib3<3,>=1.21.1 (from requests->transformers)

  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)

Collecting certifi>=2017.4.17 (from requests->transformers)

  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)

Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)

  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)

Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz)

  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)

Collecting rdflib>=5.0.0 (from nipype->fitz)

  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)

Collecting six>=1.5 (from python-dateutil>=2.2->nipype->fitz)

  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)

Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)

  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)

Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)

  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)

Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)

Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)

Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)

Using cached pdfplumber-0.11.4-py3-none-any.whl (59 kB)

Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)

Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)

Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)

Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)

Downloading numpy-2.2.0-cp312-cp312-win_amd64.whl (12.6 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 3.3 MB/s eta 0:00:00

Downloading packaging-24.2-py3-none-any.whl (65 kB)

Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 3.4 MB/s eta 0:00:00

Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 3.3 MB/s eta 0:00:00

Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)

Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)

Using cached safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)

Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)

Using cached click-8.1.7-py3-none-any.whl (97 kB)

Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)

Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)

Downloading configparser-7.1.0-py3-none-any.whl (17 kB)

Using cached filelock-3.16.1-py3-none-any.whl (16 kB)

Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)

Using cached joblib-1.4.2-py3-none-any.whl (301 kB)

Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 3.3 MB/s eta 0:00:00

Downloading nipype-1.9.2-py3-none-any.whl (3.2 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 3.3 MB/s eta 0:00:00

Using cached scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)

Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 3.3 MB/s eta 0:00:00

Downloading pyxnat-1.6.2-py3-none-any.whl (95 kB)

Using cached requests-2.32.3-py3-none-any.whl (64 kB)

Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)

Downloading charset_normalizer-3.4.0-cp312-cp312-win_amd64.whl (102 kB)

Downloading cryptography-44.0.0-cp39-abi3-win_amd64.whl (3.2 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 3.0 MB/s eta 0:00:00

Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)

Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)

Using cached idna-3.10-py3-none-any.whl (70 kB)

Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)

Using cached lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)

Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 3.3 MB/s eta 0:00:00

Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)

Downloading prov-2.0.1-py3-none-any.whl (421 kB)

Downloading pydot-3.0.3-py3-none-any.whl (35 kB)

Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)

Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)

Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)

Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 528.1/528.1 kB 2.9 MB/s eta 0:00:00

Downloading simplejson-3.19.3-cp312-cp312-win_amd64.whl (75 kB)

Downloading traits-6.4.3-cp312-cp312-win_amd64.whl (5.0 MB)

   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 3.3 MB/s eta 0:00:00

Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)

Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)

Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)

Downloading acres-0.2.0-py3-none-any.whl (10 kB)

Downloading puremagic-1.28-py3-none-any.whl (43 kB)

Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)

Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)

Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)

Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)

Using cached pycparser-2.22-py3-none-any.whl (117 kB)

Installing collected packages: pytz, puremagic, pathlib, looseversion, urllib3, tzdata, typing-extensions, traits, six, simplejson, safetensors, regex, pyyaml, pypdfium2, pyparsing, pycparser, Pillow, packaging, numpy, networkx, lxml, joblib, idna, graphviz, fsspec, filelock, configparser, configobj, colorama, ci-info, charset-normalizer, certifi, acres, tqdm, scipy, requests, python-dateutil, pydot, nibabel, isodate, httplib2, click, cffi, rdflib, pyxnat, pandas, nltk, huggingface-hub, etelemetry, cryptography, tokenizers, prov, pdfminer.six, transformers, pdfplumber, nipype, fitz

Successfully installed Pillow-11.0.0 acres-0.2.0 certifi-2024.12.14 cffi-1.17.1 charset-normalizer-3.4.0 ci-info-0.3.0 click-8.1.7 colorama-0.4.6 configobj-5.0.9 configparser-7.1.0 cryptography-44.0.0 etelemetry-0.3.1 filelock-3.16.1 fitz-0.0.1.dev2 fsspec-2024.12.0 graphviz-0.20.3 httplib2-0.22.0 huggingface-hub-0.27.0 idna-3.10 isodate-0.6.1 joblib-1.4.2 looseversion-1.3.0 lxml-5.3.0 networkx-3.4.2 nibabel-5.3.2 nipype-1.9.2 nltk-3.9.1 numpy-2.2.0 packaging-24.2 pandas-2.2.3 pathlib-1.0.1 pdfminer.six-20231228 pdfplumber-0.11.4 prov-2.0.1 puremagic-1.28 pycparser-2.22 pydot-3.0.3 pyparsing-3.2.0 pypdfium2-4.30.1 python-dateutil-2.9.0.post0 pytz-2024.2 pyxnat-1.6.2 pyyaml-6.0.2 rdflib-6.3.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scipy-1.14.1 simplejson-3.19.3 six-1.17.0 tokenizers-0.21.0 tqdm-4.67.1 traits-6.4.3 transformers-4.47.1 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3



[notice] A new release of pip is available: 24.2 -> 24.3.1

[notice] To update, run: python.exe -m pip install --upgrade pip



(myenv) D:\>

 

















